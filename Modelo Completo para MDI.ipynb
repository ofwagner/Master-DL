{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como anexo a la práctica, voy a intentar construir el modelo completo de red neuronal. Usando variables sintéticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x199146898d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as randint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from keras.optimizers import Nadam\n",
    "from keras.layers import Dropout\n",
    "#from imblearn.under_sampling import NearMiss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=4))\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "datos_path = \"./\"\n",
    "clientes_file = \"Clientes_train.csv\"\n",
    "zonas_file = \"Zonas.csv\"\n",
    "\n",
    "clientes = pd.read_csv(os.path.join(datos_path, clientes_file), sep='\\t')\n",
    "zonas = pd.read_csv(os.path.join(datos_path, zonas_file), sep='\\t')\n",
    "\n",
    "datos = pd.merge(clientes, zonas, on=\"ID_Zona\", how=\"inner\")\n",
    "\n",
    "\n",
    "\n",
    "clientes_file_2 = \"Clientes_test.csv\"\n",
    "\n",
    "\n",
    "clientes2 = pd.read_csv(os.path.join(datos_path, clientes_file_2), sep='\\t')\n",
    "zonas = pd.read_csv(os.path.join(datos_path, zonas_file), sep='\\t')\n",
    "\n",
    "datos2 = pd.merge(clientes2, zonas, on=\"ID_Zona\", how=\"inner\")\n",
    "\n",
    "#targets2 = datos2[\"Seguro_Vivienda\"]\n",
    "#variables2 = datos2.drop([\"Seguro_Vivienda\"], axis=1, inplace=False)\n",
    "\n",
    "\n",
    "\n",
    "X_test_2 = datos2._get_numeric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Cliente</th>\n",
       "      <th>Fecha_Nacimiento</th>\n",
       "      <th>Fecha_Alta</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>ID_Zona</th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>...</th>\n",
       "      <th>Poblacion_Funcionario</th>\n",
       "      <th>Poblacion_Trabajador_Cualificado</th>\n",
       "      <th>Poblacion_Trabajador_No_Cualificado</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3088</td>\n",
       "      <td>29/03/1968</td>\n",
       "      <td>27/03/1989</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>617.55</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>21.01</td>\n",
       "      <td>27.13</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3441</td>\n",
       "      <td>01/05/1962</td>\n",
       "      <td>26/12/1984</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>21.01</td>\n",
       "      <td>27.13</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0412</td>\n",
       "      <td>19/01/1967</td>\n",
       "      <td>29/04/1987</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>21.01</td>\n",
       "      <td>27.13</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3565</td>\n",
       "      <td>20/04/1948</td>\n",
       "      <td>06/09/1969</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3315.54</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>21.01</td>\n",
       "      <td>27.13</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0315</td>\n",
       "      <td>28/07/1979</td>\n",
       "      <td>18/06/2001</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2561.45</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>21.01</td>\n",
       "      <td>27.13</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_Cliente Fecha_Nacimiento  Fecha_Alta    Sexo ID_Zona  Productos_Vida  \\\n",
       "0      C3088       29/03/1968  27/03/1989   Mujer   Z1143               0   \n",
       "1      C3441       01/05/1962  26/12/1984  Hombre   Z1143               0   \n",
       "2      C0412       19/01/1967  29/04/1987  Hombre   Z1143               0   \n",
       "3      C3565       20/04/1948  06/09/1969  Hombre   Z1143               0   \n",
       "4      C0315       28/07/1979  18/06/2001   Mujer   Z1143               0   \n",
       "\n",
       "   Productos_Vehiculos  Productos_Otros  Gasto_Vida  Gasto_Vehiculos  \\\n",
       "0                    1                0         0.0           617.55   \n",
       "1                    0                0         0.0             0.00   \n",
       "2                    0                1         0.0             0.00   \n",
       "3                    1                0         0.0          3315.54   \n",
       "4                    1                2         0.0          2561.45   \n",
       "\n",
       "           ...           Poblacion_Funcionario  \\\n",
       "0          ...                           28.17   \n",
       "1          ...                           28.17   \n",
       "2          ...                           28.17   \n",
       "3          ...                           28.17   \n",
       "4          ...                           28.17   \n",
       "\n",
       "   Poblacion_Trabajador_Cualificado  Poblacion_Trabajador_No_Cualificado  \\\n",
       "0                             21.01                                27.13   \n",
       "1                             21.01                                27.13   \n",
       "2                             21.01                                27.13   \n",
       "3                             21.01                                27.13   \n",
       "4                             21.01                                27.13   \n",
       "\n",
       "   Vivienda_Propiedad  Vivienda_Alquiler  Medico_Seguro_Privado  \\\n",
       "0               71.34              28.66                  32.77   \n",
       "1               71.34              28.66                  32.77   \n",
       "2               71.34              28.66                  32.77   \n",
       "3               71.34              28.66                  32.77   \n",
       "4               71.34              28.66                  32.77   \n",
       "\n",
       "   Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "0                    67.23                   2.23   \n",
       "1                    67.23                   2.23   \n",
       "2                    67.23                   2.23   \n",
       "3                    67.23                   2.23   \n",
       "4                    67.23                   2.23   \n",
       "\n",
       "   Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  \n",
       "0                           1.47                  96.3  \n",
       "1                           1.47                  96.3  \n",
       "2                           1.47                  96.3  \n",
       "3                           1.47                  96.3  \n",
       "4                           1.47                  96.3  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Cliente</th>\n",
       "      <th>Fecha_Nacimiento</th>\n",
       "      <th>Fecha_Alta</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>ID_Zona</th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>...</th>\n",
       "      <th>Poblacion_Funcionario</th>\n",
       "      <th>Poblacion_Trabajador_Cualificado</th>\n",
       "      <th>Poblacion_Trabajador_No_Cualificado</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C2172</td>\n",
       "      <td>05/10/1981</td>\n",
       "      <td>04/02/2005</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0403</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>735.14</td>\n",
       "      <td>2535.49</td>\n",
       "      <td>...</td>\n",
       "      <td>41.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.04</td>\n",
       "      <td>66.96</td>\n",
       "      <td>78.78</td>\n",
       "      <td>21.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3005</td>\n",
       "      <td>05/04/1974</td>\n",
       "      <td>26/11/1995</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>41.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.04</td>\n",
       "      <td>66.96</td>\n",
       "      <td>78.78</td>\n",
       "      <td>21.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1627</td>\n",
       "      <td>21/09/1983</td>\n",
       "      <td>27/12/2004</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3195.94</td>\n",
       "      <td>...</td>\n",
       "      <td>46.35</td>\n",
       "      <td>16.59</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>31.24</td>\n",
       "      <td>68.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3120</td>\n",
       "      <td>16/02/1986</td>\n",
       "      <td>24/09/2007</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0700</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4610.12</td>\n",
       "      <td>...</td>\n",
       "      <td>46.35</td>\n",
       "      <td>16.59</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>31.24</td>\n",
       "      <td>68.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0649</td>\n",
       "      <td>24/01/1945</td>\n",
       "      <td>02/12/1967</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>21.16</td>\n",
       "      <td>19.46</td>\n",
       "      <td>41.63</td>\n",
       "      <td>17.29</td>\n",
       "      <td>82.71</td>\n",
       "      <td>17.86</td>\n",
       "      <td>82.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>91.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_Cliente Fecha_Nacimiento  Fecha_Alta    Sexo ID_Zona  Productos_Vida  \\\n",
       "0      C2172       05/10/1981  04/02/2005  Hombre   Z0403               1   \n",
       "1      C3005       05/04/1974  26/11/1995  Hombre   Z0403               0   \n",
       "2      C1627       21/09/1983  27/12/2004   Mujer   Z0700               0   \n",
       "3      C3120       16/02/1986  24/09/2007   Mujer   Z0700               0   \n",
       "4      C0649       24/01/1945  02/12/1967  Hombre   Z1023               0   \n",
       "\n",
       "   Productos_Vehiculos  Productos_Otros  Gasto_Vida  Gasto_Vehiculos  \\\n",
       "0                    2                2      735.14          2535.49   \n",
       "1                    0                1        0.00             0.00   \n",
       "2                    1                1        0.00          3195.94   \n",
       "3                    3                0        0.00          4610.12   \n",
       "4                    0                0        0.00             0.00   \n",
       "\n",
       "           ...           Poblacion_Funcionario  \\\n",
       "0          ...                           41.42   \n",
       "1          ...                           41.42   \n",
       "2          ...                           46.35   \n",
       "3          ...                           46.35   \n",
       "4          ...                           21.16   \n",
       "\n",
       "   Poblacion_Trabajador_Cualificado  Poblacion_Trabajador_No_Cualificado  \\\n",
       "0                              0.00                                 0.00   \n",
       "1                              0.00                                 0.00   \n",
       "2                             16.59                                 1.30   \n",
       "3                             16.59                                 1.30   \n",
       "4                             19.46                                41.63   \n",
       "\n",
       "   Vivienda_Propiedad  Vivienda_Alquiler  Medico_Seguro_Privado  \\\n",
       "0               33.04              66.96                  78.78   \n",
       "1               33.04              66.96                  78.78   \n",
       "2                0.00             100.00                  31.24   \n",
       "3                0.00             100.00                  31.24   \n",
       "4               17.29              82.71                  17.86   \n",
       "\n",
       "   Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "0                    21.22                    0.0   \n",
       "1                    21.22                    0.0   \n",
       "2                    68.76                    0.0   \n",
       "3                    68.76                    0.0   \n",
       "4                    82.14                    0.0   \n",
       "\n",
       "   Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  \n",
       "0                            0.0                100.00  \n",
       "1                            0.0                100.00  \n",
       "2                            0.0                100.00  \n",
       "3                            0.0                100.00  \n",
       "4                            8.9                 91.11  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = datos[\"Seguro_Vivienda\"]\n",
    "variables = datos.drop([\"Seguro_Vivienda\"], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio de datos\n",
    "\n",
    "Hacemos un estudio de los datos. Empezamos por una inspección visual de una muestra de filas de la tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      2927\n",
       "unique        2\n",
       "top       False\n",
       "freq       2762\n",
       "Name: Seguro_Vivienda, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>Gasto_Otros</th>\n",
       "      <th>Tipo_Familia</th>\n",
       "      <th>Tipo_Pareja</th>\n",
       "      <th>Tipo_Soltero</th>\n",
       "      <th>Educacion_Superior</th>\n",
       "      <th>...</th>\n",
       "      <th>Poblacion_Funcionario</th>\n",
       "      <th>Poblacion_Trabajador_Cualificado</th>\n",
       "      <th>Poblacion_Trabajador_No_Cualificado</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "      <td>2927.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.080287</td>\n",
       "      <td>0.750940</td>\n",
       "      <td>1.066279</td>\n",
       "      <td>37.133915</td>\n",
       "      <td>1583.312689</td>\n",
       "      <td>273.471145</td>\n",
       "      <td>47.392026</td>\n",
       "      <td>34.010779</td>\n",
       "      <td>18.597458</td>\n",
       "      <td>14.136502</td>\n",
       "      <td>...</td>\n",
       "      <td>29.345565</td>\n",
       "      <td>22.355572</td>\n",
       "      <td>26.771343</td>\n",
       "      <td>54.007017</td>\n",
       "      <td>45.992983</td>\n",
       "      <td>27.863485</td>\n",
       "      <td>72.136515</td>\n",
       "      <td>1.449713</td>\n",
       "      <td>6.700150</td>\n",
       "      <td>91.850126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.365148</td>\n",
       "      <td>0.795887</td>\n",
       "      <td>1.001220</td>\n",
       "      <td>289.125502</td>\n",
       "      <td>2008.636888</td>\n",
       "      <td>741.281246</td>\n",
       "      <td>24.955844</td>\n",
       "      <td>20.001426</td>\n",
       "      <td>20.498031</td>\n",
       "      <td>18.661422</td>\n",
       "      <td>...</td>\n",
       "      <td>21.739949</td>\n",
       "      <td>20.322406</td>\n",
       "      <td>21.439939</td>\n",
       "      <td>37.375465</td>\n",
       "      <td>37.375465</td>\n",
       "      <td>24.390093</td>\n",
       "      <td>24.390093</td>\n",
       "      <td>5.362351</td>\n",
       "      <td>12.187754</td>\n",
       "      <td>14.078293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.410000</td>\n",
       "      <td>19.980000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15.230000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>9.420000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.175000</td>\n",
       "      <td>7.275000</td>\n",
       "      <td>56.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>649.610000</td>\n",
       "      <td>117.630000</td>\n",
       "      <td>48.180000</td>\n",
       "      <td>32.140000</td>\n",
       "      <td>14.350000</td>\n",
       "      <td>6.440000</td>\n",
       "      <td>...</td>\n",
       "      <td>27.480000</td>\n",
       "      <td>19.550000</td>\n",
       "      <td>22.220000</td>\n",
       "      <td>58.410000</td>\n",
       "      <td>41.590000</td>\n",
       "      <td>20.570000</td>\n",
       "      <td>79.430000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2948.910000</td>\n",
       "      <td>303.705000</td>\n",
       "      <td>65.450000</td>\n",
       "      <td>44.705000</td>\n",
       "      <td>30.530000</td>\n",
       "      <td>20.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>41.150000</td>\n",
       "      <td>33.700000</td>\n",
       "      <td>41.020000</td>\n",
       "      <td>91.825000</td>\n",
       "      <td>85.100000</td>\n",
       "      <td>43.470000</td>\n",
       "      <td>92.725000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.325000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4934.590000</td>\n",
       "      <td>16735.670000</td>\n",
       "      <td>17188.620000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.010000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Productos_Vida  Productos_Vehiculos  Productos_Otros   Gasto_Vida  \\\n",
       "count     2927.000000          2927.000000      2927.000000  2927.000000   \n",
       "mean         0.080287             0.750940         1.066279    37.133915   \n",
       "std          0.365148             0.795887         1.001220   289.125502   \n",
       "min          0.000000             0.000000         0.000000     0.000000   \n",
       "25%          0.000000             0.000000         0.000000     0.000000   \n",
       "50%          0.000000             1.000000         1.000000     0.000000   \n",
       "75%          0.000000             1.000000         2.000000     0.000000   \n",
       "max          4.000000            10.000000         7.000000  4934.590000   \n",
       "\n",
       "       Gasto_Vehiculos   Gasto_Otros  Tipo_Familia  Tipo_Pareja  Tipo_Soltero  \\\n",
       "count      2927.000000   2927.000000   2927.000000  2927.000000   2927.000000   \n",
       "mean       1583.312689    273.471145     47.392026    34.010779     18.597458   \n",
       "std        2008.636888    741.281246     24.955844    20.001426     20.498031   \n",
       "min           0.000000      0.000000      0.000000     0.000000      0.000000   \n",
       "25%           0.000000      0.000000     29.410000    19.980000      0.000000   \n",
       "50%         649.610000    117.630000     48.180000    32.140000     14.350000   \n",
       "75%        2948.910000    303.705000     65.450000    44.705000     30.530000   \n",
       "max       16735.670000  17188.620000    100.000000   100.000000    100.000000   \n",
       "\n",
       "       Educacion_Superior          ...           Poblacion_Funcionario  \\\n",
       "count         2927.000000          ...                     2927.000000   \n",
       "mean            14.136502          ...                       29.345565   \n",
       "std             18.661422          ...                       21.739949   \n",
       "min              0.000000          ...                        0.000000   \n",
       "25%              0.000000          ...                       15.230000   \n",
       "50%              6.440000          ...                       27.480000   \n",
       "75%             20.700000          ...                       41.150000   \n",
       "max            100.000000          ...                      100.000000   \n",
       "\n",
       "       Poblacion_Trabajador_Cualificado  Poblacion_Trabajador_No_Cualificado  \\\n",
       "count                       2927.000000                          2927.000000   \n",
       "mean                          22.355572                            26.771343   \n",
       "std                           20.322406                            21.439939   \n",
       "min                            0.000000                             0.000000   \n",
       "25%                            3.880000                             9.420000   \n",
       "50%                           19.550000                            22.220000   \n",
       "75%                           33.700000                            41.020000   \n",
       "max                          100.000000                           100.000000   \n",
       "\n",
       "       Vivienda_Propiedad  Vivienda_Alquiler  Medico_Seguro_Privado  \\\n",
       "count         2927.000000        2927.000000            2927.000000   \n",
       "mean            54.007017          45.992983              27.863485   \n",
       "std             37.375465          37.375465              24.390093   \n",
       "min              0.000000           0.000000               0.000000   \n",
       "25%             14.900000           8.175000               7.275000   \n",
       "50%             58.410000          41.590000              20.570000   \n",
       "75%             91.825000          85.100000              43.470000   \n",
       "max            100.000000         100.000000             100.000000   \n",
       "\n",
       "       Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "count              2927.000000            2927.000000   \n",
       "mean                 72.136515               1.449713   \n",
       "std                  24.390093               5.362351   \n",
       "min                   0.000000               0.000000   \n",
       "25%                  56.530000               0.000000   \n",
       "50%                  79.430000               0.000000   \n",
       "75%                  92.725000               0.000000   \n",
       "max                 100.000000             100.000000   \n",
       "\n",
       "       Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  \n",
       "count                    2927.000000           2927.000000  \n",
       "mean                        6.700150             91.850126  \n",
       "std                        12.187754             14.078293  \n",
       "min                         0.000000              0.000000  \n",
       "25%                         0.000000             90.120000  \n",
       "50%                         0.000000             99.990000  \n",
       "75%                         8.325000            100.000000  \n",
       "max                       100.000000            100.010000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(datos[\"Gasto_Vehiculos\"], bins = 10, range = (datos[\"Gasto_Vehiculos\"].min(), datos[\"Gasto_Vehiculos\"].max()))\n",
    "plt.title(\"Gasto Vehiculos\")\n",
    "plt.xlabel(\"Gasto Vehiculos\")\n",
    "plt.ylabel(\"Euros\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "variables.boxplot(column=\"Gasto_Vehiculos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = variables[\"Sexo\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Sexo\")\n",
    "plt.xlabel(\"Sexo\")\n",
    "plt.ylabel(\"Casos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1991dcae630>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = pd.crosstab([datos.Sexo], datos.Seguro_Vivienda)\n",
    "tmp.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creamos variables sintéticas para ámbas tablas: Edad (años) y Antiguedad (meses):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datos['Edad'] = ((pd.to_datetime(\"06/10/2018\", format='%d/%m/%Y') - pd.to_datetime(datos.Fecha_Nacimiento, format='%d/%m/%Y')) \n",
    "          /np.timedelta64(1, 'Y')).astype(int)\n",
    "\n",
    "datos['Antiguedad'] = ((pd.to_datetime(\"06/10/2018\", format='%d/%m/%Y') - pd.to_datetime(datos.Fecha_Alta, format='%d/%m/%Y')) \n",
    "          /np.timedelta64(1, 'M')).astype(int)\n",
    "\n",
    "datos['Sexo_2'] = 0\n",
    "datos['Sexo_2'][datos.Sexo=='Hombre'] =1\n",
    "\n",
    "\n",
    "datos2['Edad'] = ((pd.to_datetime(\"06/10/2018\", format='%d/%m/%Y') - pd.to_datetime(datos.Fecha_Nacimiento, format='%d/%m/%Y')) \n",
    "          /np.timedelta64(1, 'Y')).astype(int)\n",
    "\n",
    "datos2['Antiguedad'] = ((pd.to_datetime(\"06/10/2018\", format='%d/%m/%Y') - pd.to_datetime(datos.Fecha_Alta, format='%d/%m/%Y')) \n",
    "          /np.timedelta64(1, 'M')).astype(int)\n",
    "\n",
    "datos2['Sexo_2'] = 0\n",
    "datos2['Sexo_2'][datos2.Sexo=='Hombre'] =1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Cliente</th>\n",
       "      <th>Fecha_Nacimiento</th>\n",
       "      <th>Fecha_Alta</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>ID_Zona</th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>...</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Antiguedad</th>\n",
       "      <th>Sexo_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3088</td>\n",
       "      <td>29/03/1968</td>\n",
       "      <td>27/03/1989</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>617.55</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>50</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3441</td>\n",
       "      <td>01/05/1962</td>\n",
       "      <td>26/12/1984</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>56</td>\n",
       "      <td>405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0412</td>\n",
       "      <td>19/01/1967</td>\n",
       "      <td>29/04/1987</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>51</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3565</td>\n",
       "      <td>20/04/1948</td>\n",
       "      <td>06/09/1969</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3315.54</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>70</td>\n",
       "      <td>588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0315</td>\n",
       "      <td>28/07/1979</td>\n",
       "      <td>18/06/2001</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2561.45</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>39</td>\n",
       "      <td>207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_Cliente Fecha_Nacimiento  Fecha_Alta    Sexo ID_Zona  Productos_Vida  \\\n",
       "0      C3088       29/03/1968  27/03/1989   Mujer   Z1143               0   \n",
       "1      C3441       01/05/1962  26/12/1984  Hombre   Z1143               0   \n",
       "2      C0412       19/01/1967  29/04/1987  Hombre   Z1143               0   \n",
       "3      C3565       20/04/1948  06/09/1969  Hombre   Z1143               0   \n",
       "4      C0315       28/07/1979  18/06/2001   Mujer   Z1143               0   \n",
       "\n",
       "   Productos_Vehiculos  Productos_Otros  Gasto_Vida  Gasto_Vehiculos   ...    \\\n",
       "0                    1                0         0.0           617.55   ...     \n",
       "1                    0                0         0.0             0.00   ...     \n",
       "2                    0                1         0.0             0.00   ...     \n",
       "3                    1                0         0.0          3315.54   ...     \n",
       "4                    1                2         0.0          2561.45   ...     \n",
       "\n",
       "   Vivienda_Propiedad  Vivienda_Alquiler  Medico_Seguro_Privado  \\\n",
       "0               71.34              28.66                  32.77   \n",
       "1               71.34              28.66                  32.77   \n",
       "2               71.34              28.66                  32.77   \n",
       "3               71.34              28.66                  32.77   \n",
       "4               71.34              28.66                  32.77   \n",
       "\n",
       "   Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "0                    67.23                   2.23   \n",
       "1                    67.23                   2.23   \n",
       "2                    67.23                   2.23   \n",
       "3                    67.23                   2.23   \n",
       "4                    67.23                   2.23   \n",
       "\n",
       "   Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  Edad  Antiguedad  \\\n",
       "0                           1.47                  96.3    50         354   \n",
       "1                           1.47                  96.3    56         405   \n",
       "2                           1.47                  96.3    51         377   \n",
       "3                           1.47                  96.3    70         588   \n",
       "4                           1.47                  96.3    39         207   \n",
       "\n",
       "   Sexo_2  \n",
       "0       0  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Cliente</th>\n",
       "      <th>Fecha_Nacimiento</th>\n",
       "      <th>Fecha_Alta</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>ID_Zona</th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>...</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Antiguedad</th>\n",
       "      <th>Sexo_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C2172</td>\n",
       "      <td>05/10/1981</td>\n",
       "      <td>04/02/2005</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0403</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>735.14</td>\n",
       "      <td>2535.49</td>\n",
       "      <td>...</td>\n",
       "      <td>33.04</td>\n",
       "      <td>66.96</td>\n",
       "      <td>78.78</td>\n",
       "      <td>21.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50</td>\n",
       "      <td>354</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3005</td>\n",
       "      <td>05/04/1974</td>\n",
       "      <td>26/11/1995</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>33.04</td>\n",
       "      <td>66.96</td>\n",
       "      <td>78.78</td>\n",
       "      <td>21.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>56</td>\n",
       "      <td>405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1627</td>\n",
       "      <td>21/09/1983</td>\n",
       "      <td>27/12/2004</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3195.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>31.24</td>\n",
       "      <td>68.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>51</td>\n",
       "      <td>377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3120</td>\n",
       "      <td>16/02/1986</td>\n",
       "      <td>24/09/2007</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0700</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4610.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>31.24</td>\n",
       "      <td>68.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>70</td>\n",
       "      <td>588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0649</td>\n",
       "      <td>24/01/1945</td>\n",
       "      <td>02/12/1967</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>17.29</td>\n",
       "      <td>82.71</td>\n",
       "      <td>17.86</td>\n",
       "      <td>82.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>91.11</td>\n",
       "      <td>39</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_Cliente Fecha_Nacimiento  Fecha_Alta    Sexo ID_Zona  Productos_Vida  \\\n",
       "0      C2172       05/10/1981  04/02/2005  Hombre   Z0403               1   \n",
       "1      C3005       05/04/1974  26/11/1995  Hombre   Z0403               0   \n",
       "2      C1627       21/09/1983  27/12/2004   Mujer   Z0700               0   \n",
       "3      C3120       16/02/1986  24/09/2007   Mujer   Z0700               0   \n",
       "4      C0649       24/01/1945  02/12/1967  Hombre   Z1023               0   \n",
       "\n",
       "   Productos_Vehiculos  Productos_Otros  Gasto_Vida  Gasto_Vehiculos   ...    \\\n",
       "0                    2                2      735.14          2535.49   ...     \n",
       "1                    0                1        0.00             0.00   ...     \n",
       "2                    1                1        0.00          3195.94   ...     \n",
       "3                    3                0        0.00          4610.12   ...     \n",
       "4                    0                0        0.00             0.00   ...     \n",
       "\n",
       "   Vivienda_Propiedad  Vivienda_Alquiler  Medico_Seguro_Privado  \\\n",
       "0               33.04              66.96                  78.78   \n",
       "1               33.04              66.96                  78.78   \n",
       "2                0.00             100.00                  31.24   \n",
       "3                0.00             100.00                  31.24   \n",
       "4               17.29              82.71                  17.86   \n",
       "\n",
       "   Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "0                    21.22                    0.0   \n",
       "1                    21.22                    0.0   \n",
       "2                    68.76                    0.0   \n",
       "3                    68.76                    0.0   \n",
       "4                    82.14                    0.0   \n",
       "\n",
       "   Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  Edad  Antiguedad  \\\n",
       "0                            0.0                100.00    50         354   \n",
       "1                            0.0                100.00    56         405   \n",
       "2                            0.0                100.00    51         377   \n",
       "3                            0.0                100.00    70         588   \n",
       "4                            8.9                 91.11    39         207   \n",
       "\n",
       "   Sexo_2  \n",
       "0       1  \n",
       "1       1  \n",
       "2       0  \n",
       "3       0  \n",
       "4       1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos la tabla para entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Cliente</th>\n",
       "      <th>Fecha_Nacimiento</th>\n",
       "      <th>Fecha_Alta</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>ID_Zona</th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>...</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Antiguedad</th>\n",
       "      <th>Sexo_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C1947</td>\n",
       "      <td>10/09/1948</td>\n",
       "      <td>08/09/1973</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>492.21</td>\n",
       "      <td>3890.19</td>\n",
       "      <td>...</td>\n",
       "      <td>92.04</td>\n",
       "      <td>7.96</td>\n",
       "      <td>43.84</td>\n",
       "      <td>56.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.38</td>\n",
       "      <td>96.62</td>\n",
       "      <td>70</td>\n",
       "      <td>540</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>C2877</td>\n",
       "      <td>05/11/1966</td>\n",
       "      <td>19/11/1988</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0789</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6712.59</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.08</td>\n",
       "      <td>79.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>51</td>\n",
       "      <td>358</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>C1031</td>\n",
       "      <td>04/02/1980</td>\n",
       "      <td>20/01/2001</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0789</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6824.90</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.08</td>\n",
       "      <td>79.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>38</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>C1093</td>\n",
       "      <td>29/10/1948</td>\n",
       "      <td>18/03/1970</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0664</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5915.24</td>\n",
       "      <td>...</td>\n",
       "      <td>16.11</td>\n",
       "      <td>83.89</td>\n",
       "      <td>1.91</td>\n",
       "      <td>98.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.01</td>\n",
       "      <td>69</td>\n",
       "      <td>582</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>C3214</td>\n",
       "      <td>03/03/1948</td>\n",
       "      <td>02/09/1970</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1707</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6262.06</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>70.70</td>\n",
       "      <td>29.30</td>\n",
       "      <td>16.02</td>\n",
       "      <td>12.17</td>\n",
       "      <td>71.81</td>\n",
       "      <td>70</td>\n",
       "      <td>577</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>C2780</td>\n",
       "      <td>27/07/1952</td>\n",
       "      <td>13/10/1972</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0738</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6246.52</td>\n",
       "      <td>...</td>\n",
       "      <td>79.05</td>\n",
       "      <td>20.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>66</td>\n",
       "      <td>551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>C0013</td>\n",
       "      <td>29/11/1973</td>\n",
       "      <td>08/08/2000</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0314</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7616.57</td>\n",
       "      <td>...</td>\n",
       "      <td>5.65</td>\n",
       "      <td>94.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.53</td>\n",
       "      <td>93.47</td>\n",
       "      <td>44</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>C2650</td>\n",
       "      <td>17/04/1986</td>\n",
       "      <td>01/12/2006</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3529.81</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>8.78</td>\n",
       "      <td>91.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.59</td>\n",
       "      <td>90.41</td>\n",
       "      <td>32</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>C1412</td>\n",
       "      <td>06/04/1971</td>\n",
       "      <td>05/06/1991</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0522</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8725.95</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.26</td>\n",
       "      <td>22.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.57</td>\n",
       "      <td>78.43</td>\n",
       "      <td>47</td>\n",
       "      <td>328</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>C0799</td>\n",
       "      <td>16/12/1989</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0522</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5042.51</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.26</td>\n",
       "      <td>22.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.57</td>\n",
       "      <td>78.43</td>\n",
       "      <td>28</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>C2081</td>\n",
       "      <td>27/07/1945</td>\n",
       "      <td>07/08/1966</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0987</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>167.85</td>\n",
       "      <td>4596.58</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.52</td>\n",
       "      <td>58.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.91</td>\n",
       "      <td>92.09</td>\n",
       "      <td>73</td>\n",
       "      <td>625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>C3512</td>\n",
       "      <td>15/10/1960</td>\n",
       "      <td>24/01/1983</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0517</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4421.79</td>\n",
       "      <td>...</td>\n",
       "      <td>82.93</td>\n",
       "      <td>17.07</td>\n",
       "      <td>5.91</td>\n",
       "      <td>94.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.01</td>\n",
       "      <td>57</td>\n",
       "      <td>428</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>C1810</td>\n",
       "      <td>15/04/1982</td>\n",
       "      <td>09/04/2002</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1116</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4666.23</td>\n",
       "      <td>...</td>\n",
       "      <td>83.89</td>\n",
       "      <td>16.11</td>\n",
       "      <td>17.40</td>\n",
       "      <td>82.60</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>98.41</td>\n",
       "      <td>36</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>C1226</td>\n",
       "      <td>01/12/1989</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1171</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5126.79</td>\n",
       "      <td>...</td>\n",
       "      <td>41.07</td>\n",
       "      <td>58.93</td>\n",
       "      <td>17.98</td>\n",
       "      <td>82.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.05</td>\n",
       "      <td>97.95</td>\n",
       "      <td>28</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>C0336</td>\n",
       "      <td>28/12/1974</td>\n",
       "      <td>22/04/1997</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1171</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4721.17</td>\n",
       "      <td>...</td>\n",
       "      <td>41.07</td>\n",
       "      <td>58.93</td>\n",
       "      <td>17.98</td>\n",
       "      <td>82.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.05</td>\n",
       "      <td>97.95</td>\n",
       "      <td>43</td>\n",
       "      <td>257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>C0310</td>\n",
       "      <td>06/09/1952</td>\n",
       "      <td>17/03/1979</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0220</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7970.05</td>\n",
       "      <td>...</td>\n",
       "      <td>54.69</td>\n",
       "      <td>45.31</td>\n",
       "      <td>32.78</td>\n",
       "      <td>67.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.22</td>\n",
       "      <td>90.77</td>\n",
       "      <td>66</td>\n",
       "      <td>474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>C1055</td>\n",
       "      <td>11/10/1944</td>\n",
       "      <td>23/07/1965</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0447</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5852.51</td>\n",
       "      <td>...</td>\n",
       "      <td>26.23</td>\n",
       "      <td>73.77</td>\n",
       "      <td>69.75</td>\n",
       "      <td>30.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.42</td>\n",
       "      <td>94.59</td>\n",
       "      <td>73</td>\n",
       "      <td>638</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>C3117</td>\n",
       "      <td>14/10/1987</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0447</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>357.70</td>\n",
       "      <td>3759.02</td>\n",
       "      <td>...</td>\n",
       "      <td>26.23</td>\n",
       "      <td>73.77</td>\n",
       "      <td>69.75</td>\n",
       "      <td>30.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.42</td>\n",
       "      <td>94.59</td>\n",
       "      <td>30</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>C0963</td>\n",
       "      <td>17/03/1975</td>\n",
       "      <td>29/09/1996</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0849</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5010.69</td>\n",
       "      <td>...</td>\n",
       "      <td>79.87</td>\n",
       "      <td>20.13</td>\n",
       "      <td>43.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>43</td>\n",
       "      <td>264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>C0085</td>\n",
       "      <td>04/09/1955</td>\n",
       "      <td>03/05/1978</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0679</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3476.08</td>\n",
       "      <td>...</td>\n",
       "      <td>95.89</td>\n",
       "      <td>4.11</td>\n",
       "      <td>12.46</td>\n",
       "      <td>87.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>63</td>\n",
       "      <td>485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>C1298</td>\n",
       "      <td>02/11/1982</td>\n",
       "      <td>14/04/2005</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1167</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5381.28</td>\n",
       "      <td>...</td>\n",
       "      <td>46.77</td>\n",
       "      <td>53.23</td>\n",
       "      <td>5.14</td>\n",
       "      <td>94.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>35</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>C2124</td>\n",
       "      <td>28/01/1977</td>\n",
       "      <td>17/07/1997</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0444</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3272.01</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.73</td>\n",
       "      <td>20.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>41</td>\n",
       "      <td>254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>C0211</td>\n",
       "      <td>21/01/1950</td>\n",
       "      <td>08/12/1972</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1257</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>355.19</td>\n",
       "      <td>4479.41</td>\n",
       "      <td>...</td>\n",
       "      <td>60.87</td>\n",
       "      <td>39.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>68</td>\n",
       "      <td>549</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>C3672</td>\n",
       "      <td>31/10/1981</td>\n",
       "      <td>04/05/2002</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0290</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7207.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>36</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>C3633</td>\n",
       "      <td>22/10/1965</td>\n",
       "      <td>21/05/1986</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3397.36</td>\n",
       "      <td>...</td>\n",
       "      <td>48.18</td>\n",
       "      <td>51.82</td>\n",
       "      <td>14.38</td>\n",
       "      <td>85.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>98.01</td>\n",
       "      <td>52</td>\n",
       "      <td>388</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>C1296</td>\n",
       "      <td>04/06/1985</td>\n",
       "      <td>03/03/2007</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1210</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6715.75</td>\n",
       "      <td>...</td>\n",
       "      <td>97.01</td>\n",
       "      <td>2.99</td>\n",
       "      <td>25.69</td>\n",
       "      <td>74.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.16</td>\n",
       "      <td>91.85</td>\n",
       "      <td>33</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>C0934</td>\n",
       "      <td>05/06/1951</td>\n",
       "      <td>11/12/1972</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0648</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2244.72</td>\n",
       "      <td>...</td>\n",
       "      <td>42.51</td>\n",
       "      <td>57.49</td>\n",
       "      <td>33.80</td>\n",
       "      <td>66.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.74</td>\n",
       "      <td>94.26</td>\n",
       "      <td>67</td>\n",
       "      <td>549</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>C1673</td>\n",
       "      <td>11/02/1966</td>\n",
       "      <td>13/07/1986</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0723</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>93.77</td>\n",
       "      <td>7133.86</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.26</td>\n",
       "      <td>79.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>52</td>\n",
       "      <td>386</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>C3171</td>\n",
       "      <td>06/07/1984</td>\n",
       "      <td>13/02/2006</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0777</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3328.49</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32.87</td>\n",
       "      <td>67.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>34</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>C2304</td>\n",
       "      <td>19/05/1990</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1612</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1793.68</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.21</td>\n",
       "      <td>86.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.47</td>\n",
       "      <td>93.53</td>\n",
       "      <td>28</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>C3278</td>\n",
       "      <td>22/01/1961</td>\n",
       "      <td>21/11/1981</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1706</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9539.93</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.89</td>\n",
       "      <td>45.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>57</td>\n",
       "      <td>442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>C1100</td>\n",
       "      <td>02/06/1990</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1301</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8090.46</td>\n",
       "      <td>...</td>\n",
       "      <td>27.62</td>\n",
       "      <td>72.38</td>\n",
       "      <td>19.29</td>\n",
       "      <td>80.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.37</td>\n",
       "      <td>86.64</td>\n",
       "      <td>28</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>C2398</td>\n",
       "      <td>09/06/1987</td>\n",
       "      <td>08/06/2007</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0580</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6060.83</td>\n",
       "      <td>...</td>\n",
       "      <td>11.85</td>\n",
       "      <td>88.15</td>\n",
       "      <td>6.07</td>\n",
       "      <td>93.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.43</td>\n",
       "      <td>96.57</td>\n",
       "      <td>31</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>C2460</td>\n",
       "      <td>25/12/1962</td>\n",
       "      <td>18/03/1984</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1482</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4494.38</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>55</td>\n",
       "      <td>414</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>C2966</td>\n",
       "      <td>03/10/1948</td>\n",
       "      <td>15/10/1970</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1270</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3819.82</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.36</td>\n",
       "      <td>43.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>70</td>\n",
       "      <td>575</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>C1282</td>\n",
       "      <td>21/07/1979</td>\n",
       "      <td>18/03/2001</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1722</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5927.76</td>\n",
       "      <td>...</td>\n",
       "      <td>85.99</td>\n",
       "      <td>14.01</td>\n",
       "      <td>57.89</td>\n",
       "      <td>42.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.03</td>\n",
       "      <td>58.97</td>\n",
       "      <td>39</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>C2375</td>\n",
       "      <td>11/03/1982</td>\n",
       "      <td>27/10/2003</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3151.20</td>\n",
       "      <td>...</td>\n",
       "      <td>82.59</td>\n",
       "      <td>17.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>36</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>C1691</td>\n",
       "      <td>27/08/1960</td>\n",
       "      <td>09/03/1983</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1513</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5703.46</td>\n",
       "      <td>...</td>\n",
       "      <td>84.75</td>\n",
       "      <td>15.25</td>\n",
       "      <td>40.83</td>\n",
       "      <td>59.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>58</td>\n",
       "      <td>426</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>C2897</td>\n",
       "      <td>15/09/1989</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4137.85</td>\n",
       "      <td>...</td>\n",
       "      <td>84.75</td>\n",
       "      <td>15.25</td>\n",
       "      <td>40.83</td>\n",
       "      <td>59.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>29</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>C0076</td>\n",
       "      <td>16/12/1980</td>\n",
       "      <td>31/01/2002</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1120</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9391.84</td>\n",
       "      <td>...</td>\n",
       "      <td>15.32</td>\n",
       "      <td>84.68</td>\n",
       "      <td>27.99</td>\n",
       "      <td>72.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.67</td>\n",
       "      <td>93.34</td>\n",
       "      <td>37</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>C1895</td>\n",
       "      <td>25/10/1951</td>\n",
       "      <td>12/03/1972</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0311</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1909.58</td>\n",
       "      <td>...</td>\n",
       "      <td>45.34</td>\n",
       "      <td>54.66</td>\n",
       "      <td>29.69</td>\n",
       "      <td>70.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>66</td>\n",
       "      <td>558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>C0446</td>\n",
       "      <td>22/12/1965</td>\n",
       "      <td>04/01/1989</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1620</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5484.33</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>72.57</td>\n",
       "      <td>27.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>52</td>\n",
       "      <td>357</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>C2393</td>\n",
       "      <td>10/01/1985</td>\n",
       "      <td>28/12/2007</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2671.60</td>\n",
       "      <td>...</td>\n",
       "      <td>83.76</td>\n",
       "      <td>16.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>33</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>C3194</td>\n",
       "      <td>06/11/1949</td>\n",
       "      <td>24/10/1971</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1362.05</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.65</td>\n",
       "      <td>56.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.07</td>\n",
       "      <td>95.93</td>\n",
       "      <td>68</td>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>C1553</td>\n",
       "      <td>21/08/1945</td>\n",
       "      <td>03/08/1970</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0327</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7059.42</td>\n",
       "      <td>...</td>\n",
       "      <td>65.19</td>\n",
       "      <td>34.81</td>\n",
       "      <td>6.23</td>\n",
       "      <td>93.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>73</td>\n",
       "      <td>578</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>C1201</td>\n",
       "      <td>17/11/1970</td>\n",
       "      <td>16/05/1992</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0638</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7386.12</td>\n",
       "      <td>...</td>\n",
       "      <td>73.22</td>\n",
       "      <td>26.78</td>\n",
       "      <td>18.53</td>\n",
       "      <td>81.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>47</td>\n",
       "      <td>316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>C0596</td>\n",
       "      <td>11/04/1973</td>\n",
       "      <td>29/10/1995</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1169</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4374.25</td>\n",
       "      <td>6085.28</td>\n",
       "      <td>...</td>\n",
       "      <td>12.33</td>\n",
       "      <td>87.67</td>\n",
       "      <td>17.81</td>\n",
       "      <td>82.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.61</td>\n",
       "      <td>84.39</td>\n",
       "      <td>45</td>\n",
       "      <td>275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>C0439</td>\n",
       "      <td>13/05/1946</td>\n",
       "      <td>25/05/1967</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0806</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5678.40</td>\n",
       "      <td>...</td>\n",
       "      <td>98.05</td>\n",
       "      <td>1.95</td>\n",
       "      <td>14.67</td>\n",
       "      <td>85.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>72</td>\n",
       "      <td>616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>C3526</td>\n",
       "      <td>21/04/1971</td>\n",
       "      <td>14/10/1992</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1188</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1333.27</td>\n",
       "      <td>...</td>\n",
       "      <td>44.94</td>\n",
       "      <td>55.06</td>\n",
       "      <td>69.50</td>\n",
       "      <td>30.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.89</td>\n",
       "      <td>85.11</td>\n",
       "      <td>47</td>\n",
       "      <td>311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>C0102</td>\n",
       "      <td>16/09/1971</td>\n",
       "      <td>31/07/1992</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1479</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8181.41</td>\n",
       "      <td>...</td>\n",
       "      <td>28.30</td>\n",
       "      <td>71.70</td>\n",
       "      <td>45.07</td>\n",
       "      <td>54.93</td>\n",
       "      <td>18.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>81.78</td>\n",
       "      <td>47</td>\n",
       "      <td>314</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>C1699</td>\n",
       "      <td>10/03/1950</td>\n",
       "      <td>01/06/1974</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1540</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5356.77</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>44.36</td>\n",
       "      <td>55.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>68</td>\n",
       "      <td>532</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>C3644</td>\n",
       "      <td>08/06/1966</td>\n",
       "      <td>07/12/1988</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1589</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1875.75</td>\n",
       "      <td>...</td>\n",
       "      <td>45.06</td>\n",
       "      <td>54.94</td>\n",
       "      <td>79.62</td>\n",
       "      <td>20.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.27</td>\n",
       "      <td>58.73</td>\n",
       "      <td>52</td>\n",
       "      <td>357</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>C2707</td>\n",
       "      <td>06/05/1979</td>\n",
       "      <td>22/01/2001</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1471</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9215.10</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.42</td>\n",
       "      <td>79.58</td>\n",
       "      <td>5.94</td>\n",
       "      <td>22.52</td>\n",
       "      <td>71.54</td>\n",
       "      <td>39</td>\n",
       "      <td>212</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>C0417</td>\n",
       "      <td>24/05/1963</td>\n",
       "      <td>12/05/1983</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0616</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2582.68</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.58</td>\n",
       "      <td>58.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>99.99</td>\n",
       "      <td>55</td>\n",
       "      <td>424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>C3130</td>\n",
       "      <td>12/01/1945</td>\n",
       "      <td>26/10/1966</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0424</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6855.77</td>\n",
       "      <td>...</td>\n",
       "      <td>69.87</td>\n",
       "      <td>30.13</td>\n",
       "      <td>45.70</td>\n",
       "      <td>54.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>73</td>\n",
       "      <td>623</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>C1554</td>\n",
       "      <td>16/09/1949</td>\n",
       "      <td>14/04/1971</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0854</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7684.82</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.53</td>\n",
       "      <td>46.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>69</td>\n",
       "      <td>569</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2844</th>\n",
       "      <td>C1123</td>\n",
       "      <td>16/11/1953</td>\n",
       "      <td>13/06/1974</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0848</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4981.78</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71.88</td>\n",
       "      <td>28.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>44.90</td>\n",
       "      <td>55.10</td>\n",
       "      <td>64</td>\n",
       "      <td>531</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>C0547</td>\n",
       "      <td>05/01/1986</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0322</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6977.63</td>\n",
       "      <td>...</td>\n",
       "      <td>42.43</td>\n",
       "      <td>57.57</td>\n",
       "      <td>46.11</td>\n",
       "      <td>53.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.79</td>\n",
       "      <td>70.21</td>\n",
       "      <td>32</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>C3732</td>\n",
       "      <td>20/11/1965</td>\n",
       "      <td>30/12/1986</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8357.03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.04</td>\n",
       "      <td>98.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.26</td>\n",
       "      <td>98.74</td>\n",
       "      <td>52</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>C0959</td>\n",
       "      <td>13/12/1980</td>\n",
       "      <td>29/02/2004</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1424</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>181.56</td>\n",
       "      <td>1968.15</td>\n",
       "      <td>...</td>\n",
       "      <td>91.91</td>\n",
       "      <td>8.09</td>\n",
       "      <td>31.24</td>\n",
       "      <td>68.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>37</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1320 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID_Cliente Fecha_Nacimiento  Fecha_Alta    Sexo ID_Zona  Productos_Vida  \\\n",
       "11        C1947       10/09/1948  08/09/1973  Hombre   Z1201               1   \n",
       "34        C2877       05/11/1966  19/11/1988  Hombre   Z0789               0   \n",
       "41        C1031       04/02/1980  20/01/2001   Mujer   Z0789               0   \n",
       "60        C1093       29/10/1948  18/03/1970  Hombre   Z0664               0   \n",
       "69        C3214       03/03/1948  02/09/1970  Hombre   Z1707               0   \n",
       "84        C2780       27/07/1952  13/10/1972  Hombre   Z0738               0   \n",
       "109       C0013       29/11/1973  08/08/2000  Hombre   Z0314               0   \n",
       "138       C2650       17/04/1986  01/12/2006   Mujer   Z1112               0   \n",
       "153       C1412       06/04/1971  05/06/1991  Hombre   Z0522               0   \n",
       "157       C0799       16/12/1989  15/08/2009   Mujer   Z0522               0   \n",
       "172       C2081       27/07/1945  07/08/1966  Hombre   Z0987               1   \n",
       "216       C3512       15/10/1960  24/01/1983  Hombre   Z0517               0   \n",
       "239       C1810       15/04/1982  09/04/2002  Hombre   Z1116               0   \n",
       "263       C1226       01/12/1989  15/08/2009  Hombre   Z1171               0   \n",
       "270       C0336       28/12/1974  22/04/1997   Mujer   Z1171               0   \n",
       "293       C0310       06/09/1952  17/03/1979  Hombre   Z0220               0   \n",
       "296       C1055       11/10/1944  23/07/1965  Hombre   Z0447               0   \n",
       "298       C3117       14/10/1987  15/08/2009  Hombre   Z0447               2   \n",
       "306       C0963       17/03/1975  29/09/1996  Hombre   Z0849               0   \n",
       "379       C0085       04/09/1955  03/05/1978  Hombre   Z0679               0   \n",
       "395       C1298       02/11/1982  14/04/2005   Mujer   Z1167               0   \n",
       "425       C2124       28/01/1977  17/07/1997  Hombre   Z0444               0   \n",
       "458       C0211       21/01/1950  08/12/1972  Hombre   Z1257               1   \n",
       "490       C3672       31/10/1981  04/05/2002   Mujer   Z0290               0   \n",
       "498       C3633       22/10/1965  21/05/1986  Hombre   Z0255               0   \n",
       "500       C1296       04/06/1985  03/03/2007   Mujer   Z1210               0   \n",
       "511       C0934       05/06/1951  11/12/1972  Hombre   Z0648               0   \n",
       "513       C1673       11/02/1966  13/07/1986  Hombre   Z0723               1   \n",
       "522       C3171       06/07/1984  13/02/2006   Mujer   Z0777               0   \n",
       "601       C2304       19/05/1990  15/08/2009  Hombre   Z1612               0   \n",
       "...         ...              ...         ...     ...     ...             ...   \n",
       "2466      C3278       22/01/1961  21/11/1981   Mujer   Z1706               0   \n",
       "2469      C1100       02/06/1990  15/08/2009   Mujer   Z1301               0   \n",
       "2487      C2398       09/06/1987  08/06/2007   Mujer   Z0580               0   \n",
       "2502      C2460       25/12/1962  18/03/1984  Hombre   Z1482               0   \n",
       "2516      C2966       03/10/1948  15/10/1970  Hombre   Z1270               0   \n",
       "2531      C1282       21/07/1979  18/03/2001  Hombre   Z1722               0   \n",
       "2537      C2375       11/03/1982  27/10/2003   Mujer   Z0078               0   \n",
       "2549      C1691       27/08/1960  09/03/1983  Hombre   Z1513               0   \n",
       "2550      C2897       15/09/1989  15/08/2009   Mujer   Z1513               0   \n",
       "2556      C0076       16/12/1980  31/01/2002  Hombre   Z1120               0   \n",
       "2595      C1895       25/10/1951  12/03/1972  Hombre   Z0311               0   \n",
       "2605      C0446       22/12/1965  04/01/1989   Mujer   Z1620               0   \n",
       "2614      C2393       10/01/1985  28/12/2007  Hombre   Z0739               0   \n",
       "2653      C3194       06/11/1949  24/10/1971  Hombre   Z1615               0   \n",
       "2665      C1553       21/08/1945  03/08/1970  Hombre   Z0327               0   \n",
       "2684      C1201       17/11/1970  16/05/1992  Hombre   Z0638               0   \n",
       "2693      C0596       11/04/1973  29/10/1995  Hombre   Z1169               1   \n",
       "2730      C0439       13/05/1946  25/05/1967  Hombre   Z0806               0   \n",
       "2769      C3526       21/04/1971  14/10/1992   Mujer   Z1188               0   \n",
       "2770      C0102       16/09/1971  31/07/1992  Hombre   Z1479               0   \n",
       "2784      C1699       10/03/1950  01/06/1974   Mujer   Z1540               0   \n",
       "2803      C3644       08/06/1966  07/12/1988  Hombre   Z1589               0   \n",
       "2805      C2707       06/05/1979  22/01/2001  Hombre   Z1471               0   \n",
       "2809      C0417       24/05/1963  12/05/1983  Hombre   Z0616               0   \n",
       "2837      C3130       12/01/1945  26/10/1966  Hombre   Z0424               0   \n",
       "2842      C1554       16/09/1949  14/04/1971  Hombre   Z0854               0   \n",
       "2844      C1123       16/11/1953  13/06/1974  Hombre   Z0848               0   \n",
       "2855      C0547       05/01/1986  15/08/2009   Mujer   Z0322               0   \n",
       "2894      C3732       20/11/1965  30/12/1986  Hombre   Z0112               0   \n",
       "2925      C0959       13/12/1980  29/02/2004   Mujer   Z1424               1   \n",
       "\n",
       "      Productos_Vehiculos  Productos_Otros  Gasto_Vida  Gasto_Vehiculos  \\\n",
       "11                      0                2      492.21          3890.19   \n",
       "34                      1                3        0.00          6712.59   \n",
       "41                      1                2        0.00          6824.90   \n",
       "60                      1                2        0.00          5915.24   \n",
       "69                      1                2        0.00          6262.06   \n",
       "84                      1                2        0.00          6246.52   \n",
       "109                     1                1        0.00          7616.57   \n",
       "138                     1                0        0.00          3529.81   \n",
       "153                     3                2        0.00          8725.95   \n",
       "157                     1                1        0.00          5042.51   \n",
       "172                     1                2      167.85          4596.58   \n",
       "216                     1                0        0.00          4421.79   \n",
       "239                     1                2        0.00          4666.23   \n",
       "263                     1                2        0.00          5126.79   \n",
       "270                     1                2        0.00          4721.17   \n",
       "293                     1                1        0.00          7970.05   \n",
       "296                     3                2        0.00          5852.51   \n",
       "298                     1                2      357.70          3759.02   \n",
       "306                     2                2        0.00          5010.69   \n",
       "379                     2                1        0.00          3476.08   \n",
       "395                     1                2        0.00          5381.28   \n",
       "425                     2                1        0.00          3272.01   \n",
       "458                     0                2      355.19          4479.41   \n",
       "490                     1                2        0.00          7207.71   \n",
       "498                     0                2        0.00          3397.36   \n",
       "500                     3                2        0.00          6715.75   \n",
       "511                     0                2        0.00          2244.72   \n",
       "513                     1                2       93.77          7133.86   \n",
       "522                     0                1        0.00          3328.49   \n",
       "601                     0                0        0.00          1793.68   \n",
       "...                   ...              ...         ...              ...   \n",
       "2466                    2                2        0.00          9539.93   \n",
       "2469                    2                2        0.00          8090.46   \n",
       "2487                    1                2        0.00          6060.83   \n",
       "2502                    1                1        0.00          4494.38   \n",
       "2516                    2                2        0.00          3819.82   \n",
       "2531                    1                2        0.00          5927.76   \n",
       "2537                    0                0        0.00          3151.20   \n",
       "2549                    1                1        0.00          5703.46   \n",
       "2550                    0                0        0.00          4137.85   \n",
       "2556                    1                3        0.00          9391.84   \n",
       "2595                    0                2        0.00          1909.58   \n",
       "2605                    1                2        0.00          5484.33   \n",
       "2614                    0                2        0.00          2671.60   \n",
       "2653                    0                1        0.00          1362.05   \n",
       "2665                    1                0        0.00          7059.42   \n",
       "2684                    2                2        0.00          7386.12   \n",
       "2693                    1                1     4374.25          6085.28   \n",
       "2730                    3                0        0.00          5678.40   \n",
       "2769                    1                2        0.00          1333.27   \n",
       "2770                    1                2        0.00          8181.41   \n",
       "2784                    1                3        0.00          5356.77   \n",
       "2803                    0                2        0.00          1875.75   \n",
       "2805                    2                2        0.00          9215.10   \n",
       "2809                    0                2        0.00          2582.68   \n",
       "2837                    2                1        0.00          6855.77   \n",
       "2842                    2                2        0.00          7684.82   \n",
       "2844                    1                2        0.00          4981.78   \n",
       "2855                    1                2        0.00          6977.63   \n",
       "2894                    1                2        0.00          8357.03   \n",
       "2925                    0                2      181.56          1968.15   \n",
       "\n",
       "       ...    Vivienda_Propiedad  Vivienda_Alquiler  Medico_Seguro_Privado  \\\n",
       "11     ...                 92.04               7.96                  43.84   \n",
       "34     ...                100.00               0.00                  20.08   \n",
       "41     ...                100.00               0.00                  20.08   \n",
       "60     ...                 16.11              83.89                   1.91   \n",
       "69     ...                100.00               0.00                  70.70   \n",
       "84     ...                 79.05              20.95                   0.00   \n",
       "109    ...                  5.65              94.35                   0.00   \n",
       "138    ...                  0.00             100.00                   8.78   \n",
       "153    ...                100.00               0.00                  77.26   \n",
       "157    ...                100.00               0.00                  77.26   \n",
       "172    ...                100.00               0.00                  41.52   \n",
       "216    ...                 82.93              17.07                   5.91   \n",
       "239    ...                 83.89              16.11                  17.40   \n",
       "263    ...                 41.07              58.93                  17.98   \n",
       "270    ...                 41.07              58.93                  17.98   \n",
       "293    ...                 54.69              45.31                  32.78   \n",
       "296    ...                 26.23              73.77                  69.75   \n",
       "298    ...                 26.23              73.77                  69.75   \n",
       "306    ...                 79.87              20.13                  43.50   \n",
       "379    ...                 95.89               4.11                  12.46   \n",
       "395    ...                 46.77              53.23                   5.14   \n",
       "425    ...                100.00               0.00                  79.73   \n",
       "458    ...                 60.87              39.13                   0.00   \n",
       "490    ...                  0.00             100.00                   0.00   \n",
       "498    ...                 48.18              51.82                  14.38   \n",
       "500    ...                 97.01               2.99                  25.69   \n",
       "511    ...                 42.51              57.49                  33.80   \n",
       "513    ...                100.00               0.00                  20.26   \n",
       "522    ...                100.00               0.00                  32.87   \n",
       "601    ...                100.00               0.00                  13.21   \n",
       "...    ...                   ...                ...                    ...   \n",
       "2466   ...                100.00               0.00                  54.89   \n",
       "2469   ...                 27.62              72.38                  19.29   \n",
       "2487   ...                 11.85              88.15                   6.07   \n",
       "2502   ...                100.00               0.00                 100.00   \n",
       "2516   ...                100.00               0.00                  56.36   \n",
       "2531   ...                 85.99              14.01                  57.89   \n",
       "2537   ...                 82.59              17.41                   0.00   \n",
       "2549   ...                 84.75              15.25                  40.83   \n",
       "2550   ...                 84.75              15.25                  40.83   \n",
       "2556   ...                 15.32              84.68                  27.99   \n",
       "2595   ...                 45.34              54.66                  29.69   \n",
       "2605   ...                100.00               0.00                  72.57   \n",
       "2614   ...                 83.76              16.24                   0.00   \n",
       "2653   ...                100.00               0.00                  43.65   \n",
       "2665   ...                 65.19              34.81                   6.23   \n",
       "2684   ...                 73.22              26.78                  18.53   \n",
       "2693   ...                 12.33              87.67                  17.81   \n",
       "2730   ...                 98.05               1.95                  14.67   \n",
       "2769   ...                 44.94              55.06                  69.50   \n",
       "2770   ...                 28.30              71.70                  45.07   \n",
       "2784   ...                100.00               0.00                  44.36   \n",
       "2803   ...                 45.06              54.94                  79.62   \n",
       "2805   ...                100.00               0.00                  20.42   \n",
       "2809   ...                100.00               0.00                  41.58   \n",
       "2837   ...                 69.87              30.13                  45.70   \n",
       "2842   ...                100.00               0.00                  53.53   \n",
       "2844   ...                100.00               0.00                  71.88   \n",
       "2855   ...                 42.43              57.57                  46.11   \n",
       "2894   ...                  1.04              98.96                   0.00   \n",
       "2925   ...                 91.91               8.09                  31.24   \n",
       "\n",
       "      Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "11                      56.16                   0.00   \n",
       "34                      79.92                   0.00   \n",
       "41                      79.92                   0.00   \n",
       "60                      98.09                   0.00   \n",
       "69                      29.30                  16.02   \n",
       "84                     100.00                   0.00   \n",
       "109                    100.00                   0.00   \n",
       "138                     91.22                   0.00   \n",
       "153                     22.74                   0.00   \n",
       "157                     22.74                   0.00   \n",
       "172                     58.48                   0.00   \n",
       "216                     94.09                   0.00   \n",
       "239                     82.60                   1.59   \n",
       "263                     82.02                   0.00   \n",
       "270                     82.02                   0.00   \n",
       "293                     67.22                   0.00   \n",
       "296                     30.25                   0.00   \n",
       "298                     30.25                   0.00   \n",
       "306                     56.50                   0.00   \n",
       "379                     87.54                   0.00   \n",
       "395                     94.86                   0.00   \n",
       "425                     20.27                   0.00   \n",
       "458                    100.00                   0.00   \n",
       "490                    100.00                   0.00   \n",
       "498                     85.62                   0.00   \n",
       "500                     74.31                   0.00   \n",
       "511                     66.20                   0.00   \n",
       "513                     79.74                   0.00   \n",
       "522                     67.13                   0.00   \n",
       "601                     86.79                   0.00   \n",
       "...                       ...                    ...   \n",
       "2466                    45.11                   0.00   \n",
       "2469                    80.71                   0.00   \n",
       "2487                    93.93                   0.00   \n",
       "2502                     0.00                   0.00   \n",
       "2516                    43.64                   0.00   \n",
       "2531                    42.11                   0.00   \n",
       "2537                   100.00                   0.00   \n",
       "2549                    59.17                   0.00   \n",
       "2550                    59.17                   0.00   \n",
       "2556                    72.01                   0.00   \n",
       "2595                    70.31                   0.00   \n",
       "2605                    27.43                   0.00   \n",
       "2614                   100.00                   0.00   \n",
       "2653                    56.35                   0.00   \n",
       "2665                    93.77                   0.00   \n",
       "2684                    81.47                   0.00   \n",
       "2693                    82.19                   0.00   \n",
       "2730                    85.33                   0.00   \n",
       "2769                    30.50                   0.00   \n",
       "2770                    54.93                  18.22   \n",
       "2784                    55.64                   0.00   \n",
       "2803                    20.38                   0.00   \n",
       "2805                    79.58                   5.94   \n",
       "2809                    58.42                   0.00   \n",
       "2837                    54.30                   0.00   \n",
       "2842                    46.47                   0.00   \n",
       "2844                    28.12                   0.00   \n",
       "2855                    53.89                   0.00   \n",
       "2894                   100.00                   0.00   \n",
       "2925                    68.76                   0.00   \n",
       "\n",
       "      Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  Edad  Antiguedad  \\\n",
       "11                             3.38                 96.62    70         540   \n",
       "34                             0.00                100.00    51         358   \n",
       "41                             0.00                100.00    38         212   \n",
       "60                             0.00                100.01    69         582   \n",
       "69                            12.17                 71.81    70         577   \n",
       "84                             0.00                100.00    66         551   \n",
       "109                            6.53                 93.47    44         217   \n",
       "138                            9.59                 90.41    32         142   \n",
       "153                           21.57                 78.43    47         328   \n",
       "157                           21.57                 78.43    28         109   \n",
       "172                            7.91                 92.09    73         625   \n",
       "216                            0.00                100.01    57         428   \n",
       "239                            0.00                 98.41    36         197   \n",
       "263                            2.05                 97.95    28         109   \n",
       "270                            2.05                 97.95    43         257   \n",
       "293                            9.22                 90.77    66         474   \n",
       "296                            5.42                 94.59    73         638   \n",
       "298                            5.42                 94.59    30         109   \n",
       "306                            0.00                100.00    43         264   \n",
       "379                            0.00                100.00    63         485   \n",
       "395                            0.00                100.00    35         161   \n",
       "425                            0.00                100.00    41         254   \n",
       "458                            0.00                100.00    68         549   \n",
       "490                            0.00                100.00    36         197   \n",
       "498                            1.99                 98.01    52         388   \n",
       "500                            8.16                 91.85    33         139   \n",
       "511                            5.74                 94.26    67         549   \n",
       "513                            0.00                100.00    52         386   \n",
       "522                            0.00                100.00    34         151   \n",
       "601                            6.47                 93.53    28         109   \n",
       "...                             ...                   ...   ...         ...   \n",
       "2466                           0.00                100.00    57         442   \n",
       "2469                          13.37                 86.64    28         109   \n",
       "2487                           3.43                 96.57    31         135   \n",
       "2502                           0.00                100.00    55         414   \n",
       "2516                           0.00                100.00    70         575   \n",
       "2531                          41.03                 58.97    39         210   \n",
       "2537                           0.00                100.00    36         179   \n",
       "2549                           0.00                100.00    58         426   \n",
       "2550                           0.00                100.00    29         109   \n",
       "2556                           6.67                 93.34    37         200   \n",
       "2595                           0.00                100.00    66         558   \n",
       "2605                           0.00                100.00    52         357   \n",
       "2614                           0.00                100.00    33         129   \n",
       "2653                           4.07                 95.93    68         563   \n",
       "2665                           0.00                100.00    73         578   \n",
       "2684                           0.00                100.00    47         316   \n",
       "2693                          15.61                 84.39    45         275   \n",
       "2730                           0.00                100.00    72         616   \n",
       "2769                          14.89                 85.11    47         311   \n",
       "2770                           0.00                 81.78    47         314   \n",
       "2784                           0.00                100.00    68         532   \n",
       "2803                          41.27                 58.73    52         357   \n",
       "2805                          22.52                 71.54    39         212   \n",
       "2809                           0.00                 99.99    55         424   \n",
       "2837                           0.00                100.00    73         623   \n",
       "2842                           0.00                100.00    69         569   \n",
       "2844                          44.90                 55.10    64         531   \n",
       "2855                          29.79                 70.21    32         109   \n",
       "2894                           1.26                 98.74    52         381   \n",
       "2925                           0.00                100.00    37         175   \n",
       "\n",
       "      Sexo_2  \n",
       "11         1  \n",
       "34         1  \n",
       "41         0  \n",
       "60         1  \n",
       "69         1  \n",
       "84         1  \n",
       "109        1  \n",
       "138        0  \n",
       "153        1  \n",
       "157        0  \n",
       "172        1  \n",
       "216        1  \n",
       "239        1  \n",
       "263        1  \n",
       "270        0  \n",
       "293        1  \n",
       "296        1  \n",
       "298        1  \n",
       "306        1  \n",
       "379        1  \n",
       "395        0  \n",
       "425        1  \n",
       "458        1  \n",
       "490        0  \n",
       "498        1  \n",
       "500        0  \n",
       "511        1  \n",
       "513        1  \n",
       "522        0  \n",
       "601        1  \n",
       "...      ...  \n",
       "2466       0  \n",
       "2469       0  \n",
       "2487       0  \n",
       "2502       1  \n",
       "2516       1  \n",
       "2531       1  \n",
       "2537       0  \n",
       "2549       1  \n",
       "2550       0  \n",
       "2556       1  \n",
       "2595       1  \n",
       "2605       0  \n",
       "2614       1  \n",
       "2653       1  \n",
       "2665       1  \n",
       "2684       1  \n",
       "2693       1  \n",
       "2730       1  \n",
       "2769       0  \n",
       "2770       1  \n",
       "2784       0  \n",
       "2803       1  \n",
       "2805       1  \n",
       "2809       1  \n",
       "2837       1  \n",
       "2842       1  \n",
       "2844       1  \n",
       "2855       0  \n",
       "2894       1  \n",
       "2925       0  \n",
       "\n",
       "[1320 rows x 32 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_ext = datos\n",
    "\n",
    "for i in range(0,3):\n",
    "    datos_ext = datos_ext.append(datos_ext[datos_ext.Seguro_Vivienda==1])\n",
    "\n",
    "datos_ext[datos_ext.Seguro_Vivienda==1]\n",
    "#datos_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = datos[\"Seguro_Vivienda\"]\n",
    "variables = datos.drop([\"Seguro_Vivienda\", \"ID_Cliente\", \"Fecha_Nacimiento\", \"Fecha_Alta\", \"Sexo\", \"ID_Zona\"], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: Seguro_Vivienda, dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>Gasto_Otros</th>\n",
       "      <th>Tipo_Familia</th>\n",
       "      <th>Tipo_Pareja</th>\n",
       "      <th>Tipo_Soltero</th>\n",
       "      <th>Educacion_Superior</th>\n",
       "      <th>...</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Antiguedad</th>\n",
       "      <th>Sexo_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>617.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.1</td>\n",
       "      <td>18.27</td>\n",
       "      <td>6.63</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>50</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.1</td>\n",
       "      <td>18.27</td>\n",
       "      <td>6.63</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>56</td>\n",
       "      <td>405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>197.14</td>\n",
       "      <td>75.1</td>\n",
       "      <td>18.27</td>\n",
       "      <td>6.63</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>51</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3315.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.1</td>\n",
       "      <td>18.27</td>\n",
       "      <td>6.63</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>70</td>\n",
       "      <td>588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2561.45</td>\n",
       "      <td>176.94</td>\n",
       "      <td>75.1</td>\n",
       "      <td>18.27</td>\n",
       "      <td>6.63</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>39</td>\n",
       "      <td>207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Productos_Vida  Productos_Vehiculos  Productos_Otros  Gasto_Vida  \\\n",
       "0               0                    1                0         0.0   \n",
       "1               0                    0                0         0.0   \n",
       "2               0                    0                1         0.0   \n",
       "3               0                    1                0         0.0   \n",
       "4               0                    1                2         0.0   \n",
       "\n",
       "   Gasto_Vehiculos  Gasto_Otros  Tipo_Familia  Tipo_Pareja  Tipo_Soltero  \\\n",
       "0           617.55         0.00          75.1        18.27          6.63   \n",
       "1             0.00         0.00          75.1        18.27          6.63   \n",
       "2             0.00       197.14          75.1        18.27          6.63   \n",
       "3          3315.54         0.00          75.1        18.27          6.63   \n",
       "4          2561.45       176.94          75.1        18.27          6.63   \n",
       "\n",
       "   Educacion_Superior   ...    Vivienda_Propiedad  Vivienda_Alquiler  \\\n",
       "0                2.75   ...                 71.34              28.66   \n",
       "1                2.75   ...                 71.34              28.66   \n",
       "2                2.75   ...                 71.34              28.66   \n",
       "3                2.75   ...                 71.34              28.66   \n",
       "4                2.75   ...                 71.34              28.66   \n",
       "\n",
       "   Medico_Seguro_Privado  Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "0                  32.77                    67.23                   2.23   \n",
       "1                  32.77                    67.23                   2.23   \n",
       "2                  32.77                    67.23                   2.23   \n",
       "3                  32.77                    67.23                   2.23   \n",
       "4                  32.77                    67.23                   2.23   \n",
       "\n",
       "   Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  Edad  Antiguedad  \\\n",
       "0                           1.47                  96.3    50         354   \n",
       "1                           1.47                  96.3    56         405   \n",
       "2                           1.47                  96.3    51         377   \n",
       "3                           1.47                  96.3    70         588   \n",
       "4                           1.47                  96.3    39         207   \n",
       "\n",
       "   Sexo_2  \n",
       "0       0  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    variables, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = y_train*1\n",
    "y_test = y_test*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scale = StandardScaler()\n",
    "\n",
    "X_scale_train = scale.fit_transform(X_train)\n",
    "X_scale_test = scale.transform(X_test)\n",
    "\n",
    "#X_scale_train = X_scale_train.reshape((len(X_scale_train), np.prod(X_scale_train.shape[1:])))\n",
    "#X_scale_test = X_scale_test.reshape((len(X_scale_test), np.prod(X_scale_test.shape[1:])))\n",
    "\n",
    "#y_cat_train = y_train*1\n",
    "#y_cat_test = y_test*1\n",
    "\n",
    "#Necesitamos este paso para poder probar con dos neuronas en la última capa\n",
    "y_cat_train = to_categorical(y_train.astype(int))\n",
    "y_cat_test = to_categorical(y_test.astype(int))\n",
    "\n",
    "#y_cat_train[1:50]\n",
    "#y_train[1:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por lo que veo en las últimas versiones han quitado alguna métrica de Keras, pero en esta ruta están implementadas muchas:\n",
    "# https://github.com/GeekLiB/keras/blob/master/keras/metrics.py\n",
    "# tengo dudas si el recall es correcto, pero luego saldremos de dudas cuando veamos las curvas ROC en el siguiente notebook\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    '''Calculates the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    '''\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    '''Calculates the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    '''\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    '''Calculates the F score, the weighted harmonic mean of precision and recall.\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    '''\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "        \n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=0.01\n",
    "rate = 0.1\n",
    "x = Input(shape=(26,))\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(x)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(layer)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(layer)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(layer)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(layer)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(layer)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(layer)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "y = Dense(1, activation='sigmoid',kernel_initializer='he_uniform')(layer)\n",
    "mlp = Model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nadam = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "#callback = [EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto', baseline=None, restore_best_weights=False)]\n",
    "mlp.compile(optimizer='nadam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', recall, fbeta_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               2700      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 66,201\n",
      "Trainable params: 64,801\n",
      "Non-trainable params: 1,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2106 samples, validate on 235 samples\n",
      "Epoch 1/1000\n",
      "2106/2106 [==============================] - 3s 1ms/step - loss: 36.6073 - acc: 0.8528 - recall: 0.0894 - fbeta_score: 0.0590 - val_loss: 8.2091 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "2106/2106 [==============================] - 1s 349us/step - loss: 4.4359 - acc: 0.9164 - recall: 0.0190 - fbeta_score: 0.0174 - val_loss: 3.8069 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "2106/2106 [==============================] - 1s 373us/step - loss: 2.5935 - acc: 0.9269 - recall: 0.0142 - fbeta_score: 0.0122 - val_loss: 2.3580 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "2106/2106 [==============================] - 1s 339us/step - loss: 1.8681 - acc: 0.9302 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 1.4205 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "2106/2106 [==============================] - 1s 341us/step - loss: 1.8043 - acc: 0.9283 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 1.8447 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "2106/2106 [==============================] - 1s 342us/step - loss: 1.5231 - acc: 0.9340 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 1.3432 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "2106/2106 [==============================] - 1s 357us/step - loss: 1.4805 - acc: 0.9349 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 1.6702 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "2106/2106 [==============================] - 1s 363us/step - loss: 1.1850 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.9955 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "2106/2106 [==============================] - 1s 359us/step - loss: 1.1824 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 1.1119 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "2106/2106 [==============================] - 1s 372us/step - loss: 1.1856 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 1.0468 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "2106/2106 [==============================] - 1s 353us/step - loss: 1.0350 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.9392 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "2106/2106 [==============================] - 1s 369us/step - loss: 1.2735 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.9308 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 13/1000\n",
      "2106/2106 [==============================] - 1s 360us/step - loss: 1.0384 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 1.7703 - val_acc: 0.7319 - val_recall: 0.4681 - val_fbeta_score: 0.2652\n",
      "Epoch 14/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 1.4511 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 1.4209 - val_acc: 0.9447 - val_recall: 0.0213 - val_fbeta_score: 0.0284\n",
      "Epoch 15/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 1.0900 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 1.1661 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 16/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 1.0172 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.7108 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 17/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.9457 - acc: 0.9406 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.9125 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 18/1000\n",
      "2106/2106 [==============================] - 1s 397us/step - loss: 0.9794 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.7083 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 19/1000\n",
      "2106/2106 [==============================] - 1s 399us/step - loss: 1.0196 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.8260 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 20/1000\n",
      "2106/2106 [==============================] - 1s 427us/step - loss: 1.0929 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 1.0447 - val_acc: 0.9021 - val_recall: 0.4043 - val_fbeta_score: 0.3220\n",
      "Epoch 21/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 1.0546 - acc: 0.9383 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 1.0590 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 22/1000\n",
      "2106/2106 [==============================] - 1s 398us/step - loss: 1.0044 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 1.2992 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 23/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 1.0208 - acc: 0.9402 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 1.2223 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 24/1000\n",
      "2106/2106 [==============================] - 1s 400us/step - loss: 1.1007 - acc: 0.9411 - recall: 0.0028 - fbeta_score: 0.0028 - val_loss: 0.8588 - val_acc: 0.9489 - val_recall: 0.0638 - val_fbeta_score: 0.0709\n",
      "Epoch 25/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 1.0306 - acc: 0.9378 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.8870 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 26/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.9933 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 1.0701 - val_acc: 0.9532 - val_recall: 0.0851 - val_fbeta_score: 0.0993\n",
      "Epoch 27/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.9809 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.7184 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 28/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 1.0564 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.8605 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 29/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 1.3213 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.9256 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 30/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.9477 - acc: 0.9411 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.6594 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 31/1000\n",
      "2106/2106 [==============================] - 1s 443us/step - loss: 0.9484 - acc: 0.9430 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.9819 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 32/1000\n",
      "2106/2106 [==============================] - 1s 444us/step - loss: 0.9146 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.7015 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 33/1000\n",
      "2106/2106 [==============================] - 1s 449us/step - loss: 1.0115 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 1.2878 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 34/1000\n",
      "2106/2106 [==============================] - 1s 529us/step - loss: 0.9704 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.7136 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 35/1000\n",
      "2106/2106 [==============================] - 1s 578us/step - loss: 1.1949 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.8459 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 36/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 555us/step - loss: 0.9621 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 1.3703 - val_acc: 0.9234 - val_recall: 0.3617 - val_fbeta_score: 0.3191\n",
      "Epoch 37/1000\n",
      "2106/2106 [==============================] - 1s 526us/step - loss: 0.9121 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 1.7378 - val_acc: 0.7149 - val_recall: 0.4043 - val_fbeta_score: 0.2123\n",
      "Epoch 38/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.8436 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.8341 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 39/1000\n",
      "2106/2106 [==============================] - 1s 515us/step - loss: 0.7909 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.5711 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 40/1000\n",
      "2106/2106 [==============================] - 1s 577us/step - loss: 0.8042 - acc: 0.9397 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.6975 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 41/1000\n",
      "2106/2106 [==============================] - 1s 551us/step - loss: 0.7352 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.5832 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 42/1000\n",
      "2106/2106 [==============================] - 1s 581us/step - loss: 0.8743 - acc: 0.9392 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.6109 - val_acc: 0.9532 - val_recall: 0.0851 - val_fbeta_score: 0.0851\n",
      "Epoch 43/1000\n",
      "2106/2106 [==============================] - 1s 632us/step - loss: 0.9141 - acc: 0.9392 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 1.1711 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 44/1000\n",
      "2106/2106 [==============================] - 1s 588us/step - loss: 0.8865 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.6814 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 45/1000\n",
      "2106/2106 [==============================] - 1s 630us/step - loss: 0.8173 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.6610 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 46/1000\n",
      "2106/2106 [==============================] - 1s 569us/step - loss: 0.8253 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 1.0668 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 47/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.7548 - acc: 0.9406 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.8273 - val_acc: 0.9532 - val_recall: 0.3191 - val_fbeta_score: 0.3262\n",
      "Epoch 48/1000\n",
      "2106/2106 [==============================] - 1s 563us/step - loss: 0.7052 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.7238 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 49/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.7653 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5348 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 50/1000\n",
      "2106/2106 [==============================] - 1s 487us/step - loss: 0.7098 - acc: 0.9368 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.6088 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 51/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.8473 - acc: 0.9435 - recall: 0.0451 - fbeta_score: 0.0459 - val_loss: 0.8988 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 52/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.7828 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.7014 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 53/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.7924 - acc: 0.9397 - recall: 0.0111 - fbeta_score: 0.0119 - val_loss: 0.9646 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 54/1000\n",
      "2106/2106 [==============================] - 1s 386us/step - loss: 0.6818 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.5378 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 55/1000\n",
      "2106/2106 [==============================] - 1s 371us/step - loss: 0.6983 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 1.1846 - val_acc: 0.9532 - val_recall: 0.0851 - val_fbeta_score: 0.0851\n",
      "Epoch 56/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.8722 - acc: 0.9421 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.7045 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 57/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.7265 - acc: 0.9421 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.6619 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 58/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.7888 - acc: 0.9421 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.5722 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 59/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.7454 - acc: 0.9406 - recall: 0.0380 - fbeta_score: 0.0380 - val_loss: 0.5234 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 60/1000\n",
      "2106/2106 [==============================] - 1s 477us/step - loss: 0.6846 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4804 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 61/1000\n",
      "2106/2106 [==============================] - 1s 433us/step - loss: 0.6430 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.8479 - val_acc: 0.9234 - val_recall: 0.4468 - val_fbeta_score: 0.3957\n",
      "Epoch 62/1000\n",
      "2106/2106 [==============================] - 1s 389us/step - loss: 0.7088 - acc: 0.9383 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.6754 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 63/1000\n",
      "2106/2106 [==============================] - 1s 440us/step - loss: 0.7817 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.9906 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 64/1000\n",
      "2106/2106 [==============================] - 1s 385us/step - loss: 0.7301 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.5526 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 65/1000\n",
      "2106/2106 [==============================] - 1s 470us/step - loss: 0.6556 - acc: 0.9392 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.6149 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 66/1000\n",
      "2106/2106 [==============================] - 1s 511us/step - loss: 0.8161 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.6460 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 67/1000\n",
      "2106/2106 [==============================] - 1s 489us/step - loss: 0.6642 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.5061 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 68/1000\n",
      "2106/2106 [==============================] - 1s 477us/step - loss: 0.5846 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.5967 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 69/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.7165 - acc: 0.9421 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.6051 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 70/1000\n",
      "2106/2106 [==============================] - 1s 451us/step - loss: 0.7548 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.6295 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 71/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - ETA: 0s - loss: 0.6535 - acc: 0.9402 - recall: 0.0101 - fbeta_score: 0.01 - 1s 432us/step - loss: 0.6504 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.6738 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 72/1000\n",
      "2106/2106 [==============================] - 1s 431us/step - loss: 0.6477 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.5225 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 73/1000\n",
      "2106/2106 [==============================] - 1s 443us/step - loss: 0.6150 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.8485 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 74/1000\n",
      "2106/2106 [==============================] - 1s 473us/step - loss: 0.5942 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.5863 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 75/1000\n",
      "2106/2106 [==============================] - 1s 431us/step - loss: 0.6653 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.5778 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 76/1000\n",
      "2106/2106 [==============================] - 1s 446us/step - loss: 0.6892 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.8422 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 77/1000\n",
      "2106/2106 [==============================] - 1s 461us/step - loss: 0.6290 - acc: 0.9397 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.5326 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 78/1000\n",
      "2106/2106 [==============================] - 1s 454us/step - loss: 0.5669 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.5879 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 79/1000\n",
      "2106/2106 [==============================] - 1s 444us/step - loss: 0.6132 - acc: 0.9435 - recall: 0.0253 - fbeta_score: 0.0261 - val_loss: 0.5922 - val_acc: 0.9574 - val_recall: 0.1064 - val_fbeta_score: 0.1135\n",
      "Epoch 80/1000\n",
      "2106/2106 [==============================] - 1s 432us/step - loss: 0.6021 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.5142 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 81/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.5917 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.7718 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 82/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.5740 - acc: 0.9425 - recall: 0.0131 - fbeta_score: 0.0146 - val_loss: 0.6052 - val_acc: 0.9745 - val_recall: 0.3191 - val_fbeta_score: 0.3262\n",
      "Epoch 83/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.6071 - acc: 0.9406 - recall: 0.0546 - fbeta_score: 0.0586 - val_loss: 0.6870 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 84/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.6033 - acc: 0.9378 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.7778 - val_acc: 0.9489 - val_recall: 0.0638 - val_fbeta_score: 0.0709\n",
      "Epoch 85/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.6572 - acc: 0.9416 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.5996 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 86/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.6054 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.5508 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 87/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.5845 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5601 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 88/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.6026 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5271 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 89/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.5502 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.6049 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 90/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.5453 - acc: 0.9406 - recall: 0.0380 - fbeta_score: 0.0380 - val_loss: 0.5316 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 91/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.6080 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.5850 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 92/1000\n",
      "2106/2106 [==============================] - 1s 432us/step - loss: 0.6076 - acc: 0.9402 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.6052 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 93/1000\n",
      "2106/2106 [==============================] - 1s 436us/step - loss: 0.5350 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4825 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 94/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.5272 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.5958 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 95/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.5647 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.5848 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 96/1000\n",
      "2106/2106 [==============================] - 1s 427us/step - loss: 0.5613 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0190 - val_loss: 0.4705 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 97/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.5162 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4563 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 98/1000\n",
      "2106/2106 [==============================] - 1s 436us/step - loss: 0.5432 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4676 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 99/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.5261 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.7981 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 100/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.5778 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.6804 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 101/1000\n",
      "2106/2106 [==============================] - 1s 429us/step - loss: 0.5553 - acc: 0.9383 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.6477 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 102/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.5908 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.8773 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 103/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.5455 - acc: 0.9416 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.8274 - val_acc: 0.9702 - val_recall: 0.2340 - val_fbeta_score: 0.2411\n",
      "Epoch 104/1000\n",
      "2106/2106 [==============================] - 1s 432us/step - loss: 0.5937 - acc: 0.9402 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.6024 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 105/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.4921 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5693 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.5519 - acc: 0.9368 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5679 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 107/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.5716 - acc: 0.9411 - recall: 0.0285 - fbeta_score: 0.0317 - val_loss: 1.0336 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 108/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.6095 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4614 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 109/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.5336 - acc: 0.9368 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4014 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 110/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.5102 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5704 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 111/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.5300 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5645 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 112/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.5630 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.5558 - val_acc: 0.9447 - val_recall: 0.3191 - val_fbeta_score: 0.3262\n",
      "Epoch 113/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.4677 - acc: 0.9425 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4001 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 114/1000\n",
      "2106/2106 [==============================] - 1s 426us/step - loss: 0.5549 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.5147 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 115/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.5109 - acc: 0.9397 - recall: 0.0301 - fbeta_score: 0.0324 - val_loss: 0.4321 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 116/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.5071 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4887 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 117/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.5301 - acc: 0.9387 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.5457 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 118/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.5209 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4815 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 119/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.5200 - acc: 0.9387 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.5428 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 120/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.4783 - acc: 0.9416 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.4786 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 121/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.4719 - acc: 0.9416 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.6761 - val_acc: 0.9574 - val_recall: 0.1277 - val_fbeta_score: 0.1418\n",
      "Epoch 122/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.4688 - acc: 0.9435 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.4882 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 123/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.4685 - acc: 0.9411 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4305 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 124/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.4993 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.4080 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 125/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.4706 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3805 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 126/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.5233 - acc: 0.9416 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4525 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 127/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.4571 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4069 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 128/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.4521 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4544 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 129/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.5504 - acc: 0.9406 - recall: 0.0253 - fbeta_score: 0.0261 - val_loss: 0.4847 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 130/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.4923 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.6159 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 131/1000\n",
      "2106/2106 [==============================] - 1s 428us/step - loss: 0.4773 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4297 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 132/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.4760 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4664 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 133/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.4653 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4237 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 134/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.4396 - acc: 0.9387 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4994 - val_acc: 0.9489 - val_recall: 0.0638 - val_fbeta_score: 0.0709\n",
      "Epoch 135/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.5131 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4333 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 136/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.4309 - acc: 0.9425 - recall: 0.0230 - fbeta_score: 0.0245 - val_loss: 0.4025 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 137/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.4613 - acc: 0.9402 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.4160 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 138/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.4290 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4245 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 139/1000\n",
      "2106/2106 [==============================] - 1s 445us/step - loss: 0.4275 - acc: 0.9440 - recall: 0.0356 - fbeta_score: 0.0380 - val_loss: 0.4151 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 140/1000\n",
      "2106/2106 [==============================] - 1s 451us/step - loss: 0.4399 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4209 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 141/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 442us/step - loss: 0.4164 - acc: 0.9402 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.4256 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 142/1000\n",
      "2106/2106 [==============================] - 1s 437us/step - loss: 0.4635 - acc: 0.9387 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.4659 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 143/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.4312 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4498 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 144/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.4449 - acc: 0.9402 - recall: 0.0182 - fbeta_score: 0.0198 - val_loss: 0.4206 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 145/1000\n",
      "2106/2106 [==============================] - 1s 403us/step - loss: 0.4131 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4076 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 146/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.4556 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4546 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 147/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.4269 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4824 - val_acc: 0.9489 - val_recall: 0.1702 - val_fbeta_score: 0.1702\n",
      "Epoch 148/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.4547 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0063 - val_loss: 0.4046 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 149/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.4602 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.4559 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 150/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.4561 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.6086 - val_acc: 0.9489 - val_recall: 0.3617 - val_fbeta_score: 0.3191\n",
      "Epoch 151/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.4393 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4113 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 152/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.4477 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3884 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 153/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.4658 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4344 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 154/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.4324 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3893 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 155/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.4797 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3933 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 156/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.4265 - acc: 0.9430 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3522 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 157/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.4442 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.4019 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 158/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.4543 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.5582 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 159/1000\n",
      "2106/2106 [==============================] - 1s 435us/step - loss: 0.4563 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4400 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 160/1000\n",
      "2106/2106 [==============================] - 1s 449us/step - loss: 0.4201 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5857 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 161/1000\n",
      "2106/2106 [==============================] - 1s 464us/step - loss: 0.4872 - acc: 0.9421 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3991 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 162/1000\n",
      "2106/2106 [==============================] - 1s 474us/step - loss: 0.4348 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4075 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 163/1000\n",
      "2106/2106 [==============================] - 1s 464us/step - loss: 0.4375 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4606 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 164/1000\n",
      "2106/2106 [==============================] - 1s 470us/step - loss: 0.4365 - acc: 0.9421 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4685 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 165/1000\n",
      "2106/2106 [==============================] - 1s 462us/step - loss: 0.4249 - acc: 0.9392 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3685 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 166/1000\n",
      "2106/2106 [==============================] - 1s 473us/step - loss: 0.3919 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4646 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 167/1000\n",
      "2106/2106 [==============================] - 1s 462us/step - loss: 0.4358 - acc: 0.9425 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.4035 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 168/1000\n",
      "2106/2106 [==============================] - 1s 464us/step - loss: 0.4414 - acc: 0.9387 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3994 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 169/1000\n",
      "2106/2106 [==============================] - 1s 463us/step - loss: 0.4343 - acc: 0.9406 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.4492 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 170/1000\n",
      "2106/2106 [==============================] - 1s 451us/step - loss: 0.4545 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.4685 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 171/1000\n",
      "2106/2106 [==============================] - 1s 475us/step - loss: 0.4213 - acc: 0.9430 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3864 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 172/1000\n",
      "2106/2106 [==============================] - 1s 524us/step - loss: 0.4029 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.4191 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 173/1000\n",
      "2106/2106 [==============================] - 1s 439us/step - loss: 0.4093 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4633 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 174/1000\n",
      "2106/2106 [==============================] - 1s 460us/step - loss: 0.4564 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4257 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 175/1000\n",
      "2106/2106 [==============================] - 1s 494us/step - loss: 0.4122 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3741 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/1000\n",
      "2106/2106 [==============================] - 1s 439us/step - loss: 0.3901 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4442 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 177/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.4365 - acc: 0.9402 - recall: 0.0083 - fbeta_score: 0.0098 - val_loss: 0.3774 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 178/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.4284 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4223 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 179/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.4265 - acc: 0.9387 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3548 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 180/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.4086 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4076 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 181/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.4995 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4230 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 182/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3911 - acc: 0.9421 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3912 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 183/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3956 - acc: 0.9435 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4274 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 184/1000\n",
      "2106/2106 [==============================] - 1s 426us/step - loss: 0.4127 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4137 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 185/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.3912 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3750 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 186/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.4256 - acc: 0.9397 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.4292 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 187/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3976 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3579 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 188/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3905 - acc: 0.9411 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.4253 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 189/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.4114 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4089 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 190/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.4171 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.4087 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 191/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3967 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3854 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 192/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.4246 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3420 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 193/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3886 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3918 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 194/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3842 - acc: 0.9387 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3984 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 195/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3823 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4080 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 196/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3871 - acc: 0.9425 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3644 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 197/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.4349 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3786 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 198/1000\n",
      "2106/2106 [==============================] - 1s 419us/step - loss: 0.4352 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3974 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 199/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.4354 - acc: 0.9421 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3994 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 200/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.4163 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4575 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 201/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3979 - acc: 0.9406 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3666 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 202/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.4125 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.8355 - val_acc: 0.7957 - val_recall: 0.4681 - val_fbeta_score: 0.3035\n",
      "Epoch 203/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.4485 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.4063 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 204/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3961 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3490 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 205/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.4028 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3980 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 206/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3950 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4143 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 207/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.4169 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3786 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 208/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3928 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.5819 - val_acc: 0.9447 - val_recall: 0.0213 - val_fbeta_score: 0.0284\n",
      "Epoch 209/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.4003 - acc: 0.9368 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3945 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 210/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3984 - acc: 0.9430 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3889 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 211/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.4147 - acc: 0.9402 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4540 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 212/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.4261 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3303 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 213/1000\n",
      "2106/2106 [==============================] - 1s 439us/step - loss: 0.3739 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3356 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 214/1000\n",
      "2106/2106 [==============================] - 1s 428us/step - loss: 0.3807 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3371 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 215/1000\n",
      "2106/2106 [==============================] - 1s 419us/step - loss: 0.3878 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.5981 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 216/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.5158 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3535 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 217/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3966 - acc: 0.9416 - recall: 0.0111 - fbeta_score: 0.0119 - val_loss: 0.3632 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 218/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3715 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3556 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 219/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3740 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3798 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 220/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.4120 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4127 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 221/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3951 - acc: 0.9421 - recall: 0.0119 - fbeta_score: 0.0142 - val_loss: 0.3943 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 222/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3818 - acc: 0.9373 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.4319 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 223/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.4149 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3340 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 224/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3893 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3428 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 225/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3993 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3355 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 226/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3908 - acc: 0.9421 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3448 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 227/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3735 - acc: 0.9421 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3895 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 228/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.4093 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4375 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 229/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.4034 - acc: 0.9387 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.4126 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 230/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.4203 - acc: 0.9435 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3999 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 231/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.4121 - acc: 0.9406 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4654 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 232/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.4176 - acc: 0.9373 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3842 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 233/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3956 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4194 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 234/1000\n",
      "2106/2106 [==============================] - 1s 446us/step - loss: 0.3775 - acc: 0.9425 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3601 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 235/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3632 - acc: 0.9392 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3654 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 236/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.4066 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3841 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 237/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3994 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3847 - val_acc: 0.9574 - val_recall: 0.1064 - val_fbeta_score: 0.1135\n",
      "Epoch 238/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.4013 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4261 - val_acc: 0.9617 - val_recall: 0.2766 - val_fbeta_score: 0.2837\n",
      "Epoch 239/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3793 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3789 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 240/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3951 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3439 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 241/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3977 - acc: 0.9425 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4222 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 242/1000\n",
      "2106/2106 [==============================] - 1s 429us/step - loss: 0.4000 - acc: 0.9402 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3656 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 243/1000\n",
      "2106/2106 [==============================] - 1s 426us/step - loss: 0.3968 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.4056 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 244/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.3744 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3494 - val_acc: 0.9617 - val_recall: 0.1489 - val_fbeta_score: 0.1560\n",
      "Epoch 245/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.3688 - acc: 0.9383 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3620 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 246/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 403us/step - loss: 0.3823 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3532 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 247/1000\n",
      "2106/2106 [==============================] - 1s 403us/step - loss: 0.3734 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4006 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 248/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3792 - acc: 0.9397 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3671 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 249/1000\n",
      "2106/2106 [==============================] - 1s 400us/step - loss: 0.4031 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3800 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 250/1000\n",
      "2106/2106 [==============================] - 1s 403us/step - loss: 0.3807 - acc: 0.9402 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.4212 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 251/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.4013 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3614 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 252/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3880 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3791 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 253/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3992 - acc: 0.9425 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3629 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 254/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3946 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3332 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 255/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3665 - acc: 0.9425 - recall: 0.0261 - fbeta_score: 0.0285 - val_loss: 0.3303 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 256/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3523 - acc: 0.9416 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3615 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 257/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3746 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3556 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 258/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3835 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3937 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 259/1000\n",
      "2106/2106 [==============================] - 1s 427us/step - loss: 0.3915 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3678 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 260/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3631 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3459 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 261/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.3530 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3717 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 262/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.3927 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3642 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 263/1000\n",
      "2106/2106 [==============================] - 1s 428us/step - loss: 0.3660 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3311 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 264/1000\n",
      "2106/2106 [==============================] - 1s 457us/step - loss: 0.3651 - acc: 0.9425 - recall: 0.0214 - fbeta_score: 0.0237 - val_loss: 0.3944 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 265/1000\n",
      "2106/2106 [==============================] - 1s 470us/step - loss: 0.3770 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3409 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 266/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3747 - acc: 0.9425 - recall: 0.0202 - fbeta_score: 0.0225 - val_loss: 0.4148 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 267/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3870 - acc: 0.9425 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3522 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 268/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3787 - acc: 0.9430 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4065 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 269/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3828 - acc: 0.9397 - recall: 0.0182 - fbeta_score: 0.0198 - val_loss: 0.5045 - val_acc: 0.8851 - val_recall: 0.4043 - val_fbeta_score: 0.3631\n",
      "Epoch 270/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3794 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3767 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 271/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3670 - acc: 0.9435 - recall: 0.0522 - fbeta_score: 0.0522 - val_loss: 0.3772 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 272/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3751 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3667 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 273/1000\n",
      "2106/2106 [==============================] - 1s 440us/step - loss: 0.3667 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3656 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 274/1000\n",
      "2106/2106 [==============================] - 1s 466us/step - loss: 0.3719 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3767 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 275/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.4063 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3805 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 276/1000\n",
      "2106/2106 [==============================] - 1s 419us/step - loss: 0.3911 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3845 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 277/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.3716 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3490 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 278/1000\n",
      "2106/2106 [==============================] - 1s 428us/step - loss: 0.3870 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3793 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 279/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3560 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3527 - val_acc: 0.9617 - val_recall: 0.1489 - val_fbeta_score: 0.1560\n",
      "Epoch 280/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.3770 - acc: 0.9406 - recall: 0.0499 - fbeta_score: 0.0506 - val_loss: 0.3241 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 281/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.3538 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3533 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 282/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3729 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4240 - val_acc: 0.9277 - val_recall: 0.2553 - val_fbeta_score: 0.2411\n",
      "Epoch 283/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3692 - acc: 0.9406 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3328 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 284/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3748 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3698 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 285/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3693 - acc: 0.9402 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.4250 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 286/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3767 - acc: 0.9430 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3448 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 287/1000\n",
      "2106/2106 [==============================] - 1s 435us/step - loss: 0.3848 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3507 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 288/1000\n",
      "2106/2106 [==============================] - 1s 450us/step - loss: 0.3946 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3666 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 289/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.3765 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3498 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 290/1000\n",
      "2106/2106 [==============================] - 1s 435us/step - loss: 0.3539 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3390 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 291/1000\n",
      "2106/2106 [==============================] - 1s 474us/step - loss: 0.3775 - acc: 0.9402 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.4029 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 292/1000\n",
      "2106/2106 [==============================] - 1s 492us/step - loss: 0.3728 - acc: 0.9421 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3472 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 293/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3646 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3373 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 294/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.3610 - acc: 0.9425 - recall: 0.0253 - fbeta_score: 0.0261 - val_loss: 0.4049 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 295/1000\n",
      "2106/2106 [==============================] - 1s 453us/step - loss: 0.3596 - acc: 0.9425 - recall: 0.0332 - fbeta_score: 0.0348 - val_loss: 0.3813 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 296/1000\n",
      "2106/2106 [==============================] - 1s 471us/step - loss: 0.3675 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3984 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 297/1000\n",
      "2106/2106 [==============================] - 1s 463us/step - loss: 0.3832 - acc: 0.9397 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3440 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 298/1000\n",
      "2106/2106 [==============================] - 1s 497us/step - loss: 0.3532 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3541 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 299/1000\n",
      "2106/2106 [==============================] - 1s 531us/step - loss: 0.3813 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3143 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 300/1000\n",
      "2106/2106 [==============================] - 1s 471us/step - loss: 0.3590 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4253 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 301/1000\n",
      "2106/2106 [==============================] - 1s 465us/step - loss: 0.3691 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3565 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 302/1000\n",
      "2106/2106 [==============================] - 1s 462us/step - loss: 0.3933 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3527 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 303/1000\n",
      "2106/2106 [==============================] - 1s 469us/step - loss: 0.3832 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4645 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 304/1000\n",
      "2106/2106 [==============================] - 1s 453us/step - loss: 0.4060 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3552 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 305/1000\n",
      "2106/2106 [==============================] - ETA: 0s - loss: 0.3632 - acc: 0.9407 - recall: 0.0075 - fbeta_score: 0.00 - 1s 463us/step - loss: 0.3671 - acc: 0.9392 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4043 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 306/1000\n",
      "2106/2106 [==============================] - 1s 492us/step - loss: 0.3574 - acc: 0.9378 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3361 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 307/1000\n",
      "2106/2106 [==============================] - 1s 473us/step - loss: 0.3702 - acc: 0.9421 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.3524 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 308/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3487 - acc: 0.9421 - recall: 0.0404 - fbeta_score: 0.0412 - val_loss: 0.3215 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 309/1000\n",
      "2106/2106 [==============================] - 1s 453us/step - loss: 0.3809 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4069 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 310/1000\n",
      "2106/2106 [==============================] - 1s 519us/step - loss: 0.3713 - acc: 0.9392 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4356 - val_acc: 0.9574 - val_recall: 0.1277 - val_fbeta_score: 0.1277\n",
      "Epoch 311/1000\n",
      "2106/2106 [==============================] - 1s 426us/step - loss: 0.3719 - acc: 0.9406 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3479 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 312/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3993 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3516 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 313/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.3600 - acc: 0.9387 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3398 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 314/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3518 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3405 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 315/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.3876 - acc: 0.9421 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3574 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 316/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3607 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3591 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 317/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3608 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3528 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 318/1000\n",
      "2106/2106 [==============================] - 1s 403us/step - loss: 0.3662 - acc: 0.9425 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3506 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 319/1000\n",
      "2106/2106 [==============================] - 1s 403us/step - loss: 0.3647 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3728 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 320/1000\n",
      "2106/2106 [==============================] - 1s 400us/step - loss: 0.3739 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3790 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 321/1000\n",
      "2106/2106 [==============================] - 1s 400us/step - loss: 0.3816 - acc: 0.9406 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3560 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 322/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3715 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3592 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 323/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3770 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3452 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 324/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3573 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3813 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 325/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3764 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3584 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 326/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3627 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3907 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 327/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3527 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3780 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 328/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3590 - acc: 0.9421 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3413 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 329/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3831 - acc: 0.9397 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4042 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 330/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.3708 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3399 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 331/1000\n",
      "2106/2106 [==============================] - 1s 431us/step - loss: 0.3635 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3461 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 332/1000\n",
      "2106/2106 [==============================] - 1s 439us/step - loss: 0.3405 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3149 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 333/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.3668 - acc: 0.9383 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3657 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 334/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3584 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3730 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 335/1000\n",
      "2106/2106 [==============================] - 1s 427us/step - loss: 0.3678 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3675 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 336/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.3620 - acc: 0.9378 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3969 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 337/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3948 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3565 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 338/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3714 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3185 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 339/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3579 - acc: 0.9416 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3015 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 340/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3641 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3474 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 341/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.3534 - acc: 0.9435 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3502 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 342/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3519 - acc: 0.9416 - recall: 0.0309 - fbeta_score: 0.0332 - val_loss: 0.3006 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 343/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.3774 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3419 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 344/1000\n",
      "2106/2106 [==============================] - 1s 419us/step - loss: 0.3720 - acc: 0.9411 - recall: 0.0467 - fbeta_score: 0.0483 - val_loss: 0.3715 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 345/1000\n",
      "2106/2106 [==============================] - 1s 426us/step - loss: 0.3560 - acc: 0.9411 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3537 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 346/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3605 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3516 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 347/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3413 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3308 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 348/1000\n",
      "2106/2106 [==============================] - 1s 452us/step - loss: 0.3598 - acc: 0.9430 - recall: 0.0451 - fbeta_score: 0.0475 - val_loss: 0.4808 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 349/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3778 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3445 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 350/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3648 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3455 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3573 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3793 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 352/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3595 - acc: 0.9421 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3819 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 353/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3486 - acc: 0.9392 - recall: 0.0206 - fbeta_score: 0.0214 - val_loss: 0.3449 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 354/1000\n",
      "2106/2106 [==============================] - 1s 403us/step - loss: 0.3741 - acc: 0.9411 - recall: 0.0309 - fbeta_score: 0.0332 - val_loss: 0.3236 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 355/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.3608 - acc: 0.9402 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.3419 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 356/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3693 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3692 - val_acc: 0.9191 - val_recall: 0.4043 - val_fbeta_score: 0.3574\n",
      "Epoch 357/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.3700 - acc: 0.9421 - recall: 0.0332 - fbeta_score: 0.0348 - val_loss: 0.3715 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 358/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3358 - acc: 0.9425 - recall: 0.0570 - fbeta_score: 0.0570 - val_loss: 0.3215 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 359/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.3644 - acc: 0.9392 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3449 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 360/1000\n",
      "2106/2106 [==============================] - 1s 467us/step - loss: 0.3365 - acc: 0.9392 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3017 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 361/1000\n",
      "2106/2106 [==============================] - 1s 442us/step - loss: 0.3748 - acc: 0.9425 - recall: 0.0380 - fbeta_score: 0.0380 - val_loss: 0.4053 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 362/1000\n",
      "2106/2106 [==============================] - 1s 461us/step - loss: 0.3586 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3537 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 363/1000\n",
      "2106/2106 [==============================] - 1s 448us/step - loss: 0.3735 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3404 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 364/1000\n",
      "2106/2106 [==============================] - 1s 440us/step - loss: 0.3670 - acc: 0.9383 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3242 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 365/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3403 - acc: 0.9411 - recall: 0.0404 - fbeta_score: 0.0412 - val_loss: 0.3263 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 366/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3477 - acc: 0.9397 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3396 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 367/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3541 - acc: 0.9378 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3397 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 368/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.3674 - acc: 0.9402 - recall: 0.0222 - fbeta_score: 0.0253 - val_loss: 0.3690 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 369/1000\n",
      "2106/2106 [==============================] - 1s 430us/step - loss: 0.3475 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3106 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 370/1000\n",
      "2106/2106 [==============================] - 1s 454us/step - loss: 0.3488 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3381 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 371/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3772 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3211 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 372/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3337 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3611 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 373/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3604 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3530 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 374/1000\n",
      "2106/2106 [==============================] - 1s 476us/step - loss: 0.3801 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3330 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 375/1000\n",
      "2106/2106 [==============================] - 1s 476us/step - loss: 0.3547 - acc: 0.9402 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3698 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 376/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.3548 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3677 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 377/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.3679 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3606 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 378/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3592 - acc: 0.9416 - recall: 0.0427 - fbeta_score: 0.0443 - val_loss: 0.3314 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 379/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3576 - acc: 0.9402 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3340 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 380/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.3384 - acc: 0.9411 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3326 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 381/1000\n",
      "2106/2106 [==============================] - 1s 458us/step - loss: 0.3440 - acc: 0.9416 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3183 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 382/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.3382 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3429 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 383/1000\n",
      "2106/2106 [==============================] - 1s 457us/step - loss: 0.3563 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3253 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 384/1000\n",
      "2106/2106 [==============================] - 1s 535us/step - loss: 0.3601 - acc: 0.9406 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.3205 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 385/1000\n",
      "2106/2106 [==============================] - 1s 452us/step - loss: 0.3531 - acc: 0.9392 - recall: 0.0277 - fbeta_score: 0.0293 - val_loss: 0.3770 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 386/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3789 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3477 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 387/1000\n",
      "2106/2106 [==============================] - 1s 451us/step - loss: 0.3702 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0063 - val_loss: 0.3410 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 388/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3631 - acc: 0.9387 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3331 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 389/1000\n",
      "2106/2106 [==============================] - 1s 434us/step - loss: 0.3569 - acc: 0.9425 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.4129 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 390/1000\n",
      "2106/2106 [==============================] - 1s 429us/step - loss: 0.3457 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3829 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 391/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3712 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3323 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 392/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3569 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3882 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 393/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3582 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3570 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 394/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3556 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3446 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 395/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3606 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3332 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 396/1000\n",
      "2106/2106 [==============================] - 1s 419us/step - loss: 0.3745 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3534 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 397/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.3684 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3599 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 398/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3452 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3243 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 399/1000\n",
      "2106/2106 [==============================] - 1s 436us/step - loss: 0.3541 - acc: 0.9397 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3744 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 400/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3695 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3987 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 401/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3826 - acc: 0.9397 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3672 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 402/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3688 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3589 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 403/1000\n",
      "2106/2106 [==============================] - 1s 427us/step - loss: 0.3435 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3325 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 404/1000\n",
      "2106/2106 [==============================] - 1s 451us/step - loss: 0.3764 - acc: 0.9411 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3744 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 405/1000\n",
      "2106/2106 [==============================] - 1s 455us/step - loss: 0.3498 - acc: 0.9402 - recall: 0.0206 - fbeta_score: 0.0214 - val_loss: 0.3689 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 406/1000\n",
      "2106/2106 [==============================] - 1s 445us/step - loss: 0.3625 - acc: 0.9435 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4396 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 407/1000\n",
      "2106/2106 [==============================] - 1s 444us/step - loss: 0.3483 - acc: 0.9411 - recall: 0.0348 - fbeta_score: 0.0356 - val_loss: 0.3485 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 408/1000\n",
      "2106/2106 [==============================] - 1s 436us/step - loss: 0.3386 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3479 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 409/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.3698 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3682 - val_acc: 0.9660 - val_recall: 0.3191 - val_fbeta_score: 0.3121\n",
      "Epoch 410/1000\n",
      "2106/2106 [==============================] - 1s 426us/step - loss: 0.3556 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3478 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 411/1000\n",
      "2106/2106 [==============================] - 1s 430us/step - loss: 0.3329 - acc: 0.9387 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3579 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 412/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3525 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3500 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 413/1000\n",
      "2106/2106 [==============================] - 1s 434us/step - loss: 0.3599 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3392 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 414/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.3423 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3370 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 415/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3577 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3284 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 416/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.3604 - acc: 0.9387 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3705 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 417/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.3485 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3322 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 418/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3582 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3962 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 419/1000\n",
      "2106/2106 [==============================] - 1s 473us/step - loss: 0.3623 - acc: 0.9378 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3788 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 420/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3438 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3485 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 421/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3626 - acc: 0.9387 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3403 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 422/1000\n",
      "2106/2106 [==============================] - 1s 435us/step - loss: 0.3556 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3361 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 423/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3502 - acc: 0.9378 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3619 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 424/1000\n",
      "2106/2106 [==============================] - 1s 450us/step - loss: 0.3471 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3308 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 425/1000\n",
      "2106/2106 [==============================] - 1s 462us/step - loss: 0.3652 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3413 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 426/1000\n",
      "2106/2106 [==============================] - 1s 492us/step - loss: 0.3541 - acc: 0.9416 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3443 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 427/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3687 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4165 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 428/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3601 - acc: 0.9402 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3776 - val_acc: 0.9489 - val_recall: 0.2340 - val_fbeta_score: 0.2411\n",
      "Epoch 429/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.3544 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3562 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 430/1000\n",
      "2106/2106 [==============================] - 1s 579us/step - loss: 0.3431 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4036 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 431/1000\n",
      "2106/2106 [==============================] - 1s 525us/step - loss: 0.3553 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3094 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 432/1000\n",
      "2106/2106 [==============================] - 1s 533us/step - loss: 0.3429 - acc: 0.9392 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3387 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 433/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3573 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3122 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 434/1000\n",
      "2106/2106 [==============================] - 1s 496us/step - loss: 0.3462 - acc: 0.9378 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4271 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 435/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3617 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3625 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 436/1000\n",
      "2106/2106 [==============================] - 1s 476us/step - loss: 0.3507 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3213 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 437/1000\n",
      "2106/2106 [==============================] - 1s 521us/step - loss: 0.3608 - acc: 0.9416 - recall: 0.0301 - fbeta_score: 0.0309 - val_loss: 0.3808 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 438/1000\n",
      "2106/2106 [==============================] - 1s 542us/step - loss: 0.3480 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3215 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 439/1000\n",
      "2106/2106 [==============================] - 1s 541us/step - loss: 0.3387 - acc: 0.9378 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3592 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 440/1000\n",
      "2106/2106 [==============================] - 1s 477us/step - loss: 0.3561 - acc: 0.9411 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3762 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 441/1000\n",
      "2106/2106 [==============================] - 1s 428us/step - loss: 0.3527 - acc: 0.9373 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3260 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 442/1000\n",
      "2106/2106 [==============================] - 1s 551us/step - loss: 0.3405 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3289 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 443/1000\n",
      "2106/2106 [==============================] - 1s 521us/step - loss: 0.3402 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3261 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 444/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.3344 - acc: 0.9392 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3253 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 445/1000\n",
      "2106/2106 [==============================] - 1s 445us/step - loss: 0.3415 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3478 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 446/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3497 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3373 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 447/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3524 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3895 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 448/1000\n",
      "2106/2106 [==============================] - 1s 433us/step - loss: 0.3509 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3425 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 449/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3978 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3666 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 450/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3448 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3212 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 451/1000\n",
      "2106/2106 [==============================] - 1s 460us/step - loss: 0.3447 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3175 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 452/1000\n",
      "2106/2106 [==============================] - 1s 428us/step - loss: 0.3372 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3549 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 453/1000\n",
      "2106/2106 [==============================] - 1s 448us/step - loss: 0.3417 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3058 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 454/1000\n",
      "2106/2106 [==============================] - 1s 442us/step - loss: 0.3308 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3263 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 455/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3356 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3501 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 456/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 437us/step - loss: 0.3484 - acc: 0.9378 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3355 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 457/1000\n",
      "2106/2106 [==============================] - 1s 431us/step - loss: 0.3432 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3257 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 458/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3443 - acc: 0.9392 - recall: 0.0135 - fbeta_score: 0.0150 - val_loss: 0.3438 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 459/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3485 - acc: 0.9402 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3775 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 460/1000\n",
      "2106/2106 [==============================] - 1s 419us/step - loss: 0.3631 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3272 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 461/1000\n",
      "2106/2106 [==============================] - 1s 427us/step - loss: 0.3620 - acc: 0.9416 - recall: 0.0356 - fbeta_score: 0.0380 - val_loss: 0.3237 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 462/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.3302 - acc: 0.9411 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3139 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 463/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.3532 - acc: 0.9383 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3842 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 464/1000\n",
      "2106/2106 [==============================] - 1s 439us/step - loss: 0.3561 - acc: 0.9440 - recall: 0.0332 - fbeta_score: 0.0348 - val_loss: 0.3306 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 465/1000\n",
      "2106/2106 [==============================] - 1s 443us/step - loss: 0.3360 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3209 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 466/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3378 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3088 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 467/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3499 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3454 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 468/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3554 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3462 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 469/1000\n",
      "2106/2106 [==============================] - 1s 419us/step - loss: 0.3613 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3335 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 470/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.3688 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3602 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 471/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.3499 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0063 - val_loss: 0.3341 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 472/1000\n",
      "2106/2106 [==============================] - 1s 428us/step - loss: 0.3599 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3199 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 473/1000\n",
      "2106/2106 [==============================] - 1s 429us/step - loss: 0.3506 - acc: 0.9406 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3431 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 474/1000\n",
      "2106/2106 [==============================] - 1s 453us/step - loss: 0.3606 - acc: 0.9430 - recall: 0.0617 - fbeta_score: 0.0633 - val_loss: 0.3550 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 475/1000\n",
      "2106/2106 [==============================] - 1s 447us/step - loss: 0.3531 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3393 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 476/1000\n",
      "2106/2106 [==============================] - 1s 442us/step - loss: 0.3446 - acc: 0.9430 - recall: 0.0451 - fbeta_score: 0.0491 - val_loss: 0.3233 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 477/1000\n",
      "2106/2106 [==============================] - 1s 440us/step - loss: 0.3573 - acc: 0.9387 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.3320 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 478/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.3487 - acc: 0.9392 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3376 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 479/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.3534 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3149 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 480/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3493 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3349 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 481/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3519 - acc: 0.9387 - recall: 0.0546 - fbeta_score: 0.0570 - val_loss: 0.3534 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 482/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3913 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3256 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 483/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3543 - acc: 0.9402 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3504 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 484/1000\n",
      "2106/2106 [==============================] - 1s 427us/step - loss: 0.3409 - acc: 0.9425 - recall: 0.0427 - fbeta_score: 0.0427 - val_loss: 0.3514 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 485/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.3420 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3515 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 486/1000\n",
      "2106/2106 [==============================] - 1s 442us/step - loss: 0.3496 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3753 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 487/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.3296 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3224 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 488/1000\n",
      "2106/2106 [==============================] - 1s 441us/step - loss: 0.3533 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3444 - val_acc: 0.9319 - val_recall: 0.4043 - val_fbeta_score: 0.3830\n",
      "Epoch 489/1000\n",
      "2106/2106 [==============================] - 1s 430us/step - loss: 0.3360 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3507 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 490/1000\n",
      "2106/2106 [==============================] - 1s 434us/step - loss: 0.3557 - acc: 0.9402 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3326 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 491/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3499 - acc: 0.9383 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3324 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 492/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3425 - acc: 0.9378 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3910 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 493/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3576 - acc: 0.9364 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3450 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 494/1000\n",
      "2106/2106 [==============================] - 1s 400us/step - loss: 0.3537 - acc: 0.9387 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3457 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 495/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3425 - acc: 0.9387 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3305 - val_acc: 0.9532 - val_recall: 0.0851 - val_fbeta_score: 0.0993\n",
      "Epoch 496/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3520 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3217 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 497/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3409 - acc: 0.9416 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3120 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 498/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.3684 - acc: 0.9383 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3348 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 499/1000\n",
      "2106/2106 [==============================] - 1s 401us/step - loss: 0.3325 - acc: 0.9392 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3171 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 500/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3508 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3354 - val_acc: 0.9574 - val_recall: 0.1064 - val_fbeta_score: 0.1135\n",
      "Epoch 501/1000\n",
      "2106/2106 [==============================] - 1s 429us/step - loss: 0.3572 - acc: 0.9383 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3439 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 502/1000\n",
      "2106/2106 [==============================] - 1s 439us/step - loss: 0.3290 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3635 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 503/1000\n",
      "2106/2106 [==============================] - 1s 451us/step - loss: 0.3606 - acc: 0.9397 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3597 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 504/1000\n",
      "2106/2106 [==============================] - 1s 430us/step - loss: 0.3551 - acc: 0.9416 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3371 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 505/1000\n",
      "2106/2106 [==============================] - 1s 438us/step - loss: 0.3615 - acc: 0.9406 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3148 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 506/1000\n",
      "2106/2106 [==============================] - 1s 436us/step - loss: 0.3394 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3273 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 507/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3365 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3563 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 508/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3591 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3171 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 509/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3636 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3547 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 510/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3432 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3909 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 511/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3525 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0142 - val_loss: 0.3267 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 512/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3514 - acc: 0.9397 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3512 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 513/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3496 - acc: 0.9387 - recall: 0.0071 - fbeta_score: 0.0071 - val_loss: 0.3587 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 514/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3652 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3374 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 515/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3513 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.4389 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 516/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.3711 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3468 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 517/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3330 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3413 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 518/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3404 - acc: 0.9411 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3454 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 519/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.3567 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3839 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 520/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3494 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3273 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 521/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3415 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3506 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 522/1000\n",
      "2106/2106 [==============================] - 1s 428us/step - loss: 0.3643 - acc: 0.9397 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.4189 - val_acc: 0.9362 - val_recall: 0.3617 - val_fbeta_score: 0.3262\n",
      "Epoch 523/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3520 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3496 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 524/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3485 - acc: 0.9421 - recall: 0.0261 - fbeta_score: 0.0285 - val_loss: 0.3169 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 525/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3283 - acc: 0.9387 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3006 - val_acc: 0.9489 - val_recall: 0.0638 - val_fbeta_score: 0.0709\n",
      "Epoch 526/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 430us/step - loss: 0.3481 - acc: 0.9383 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3638 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 527/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3631 - acc: 0.9368 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3327 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 528/1000\n",
      "2106/2106 [==============================] - 1s 403us/step - loss: 0.3390 - acc: 0.9440 - recall: 0.0475 - fbeta_score: 0.0491 - val_loss: 0.3605 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 529/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3654 - acc: 0.9397 - recall: 0.0087 - fbeta_score: 0.0103 - val_loss: 0.3837 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 530/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3441 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3095 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 531/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3348 - acc: 0.9364 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3550 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 532/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3331 - acc: 0.9416 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3407 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 533/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3521 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3243 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 534/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3342 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.3114 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 535/1000\n",
      "2106/2106 [==============================] - 1s 429us/step - loss: 0.3432 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3667 - val_acc: 0.9617 - val_recall: 0.3617 - val_fbeta_score: 0.3546\n",
      "Epoch 536/1000\n",
      "2106/2106 [==============================] - 1s 446us/step - loss: 0.3478 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3471 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 537/1000\n",
      "2106/2106 [==============================] - 1s 434us/step - loss: 0.3466 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3461 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 538/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3531 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3498 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 539/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3361 - acc: 0.9435 - recall: 0.0451 - fbeta_score: 0.0459 - val_loss: 0.3424 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 540/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.3543 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3724 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 541/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3593 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3352 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 542/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3425 - acc: 0.9383 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3622 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 543/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3517 - acc: 0.9378 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3163 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 544/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3425 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3202 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 545/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3511 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3331 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 546/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3293 - acc: 0.9387 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3597 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 547/1000\n",
      "2106/2106 [==============================] - 1s 437us/step - loss: 0.3449 - acc: 0.9416 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3386 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 548/1000\n",
      "2106/2106 [==============================] - 1s 436us/step - loss: 0.3489 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3679 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 549/1000\n",
      "2106/2106 [==============================] - 1s 445us/step - loss: 0.3730 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3323 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 550/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.3414 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3466 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 551/1000\n",
      "2106/2106 [==============================] - 1s 426us/step - loss: 0.3424 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3473 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 552/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3568 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3502 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 553/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3613 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3533 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 554/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3722 - acc: 0.9397 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3223 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 555/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3423 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3634 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 556/1000\n",
      "2106/2106 [==============================] - 1s 436us/step - loss: 0.3615 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3173 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 557/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.3585 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3401 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 558/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3545 - acc: 0.9383 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3880 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 559/1000\n",
      "2106/2106 [==============================] - 1s 470us/step - loss: 0.3445 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3708 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 560/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3693 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3235 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 561/1000\n",
      "2106/2106 [==============================] - 1s 461us/step - loss: 0.3408 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3266 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 562/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.3560 - acc: 0.9383 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3844 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 563/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3306 - acc: 0.9425 - recall: 0.0380 - fbeta_score: 0.0380 - val_loss: 0.3593 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 564/1000\n",
      "2106/2106 [==============================] - 1s 469us/step - loss: 0.3839 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4108 - val_acc: 0.9574 - val_recall: 0.3617 - val_fbeta_score: 0.3404\n",
      "Epoch 565/1000\n",
      "2106/2106 [==============================] - 1s 467us/step - loss: 0.3666 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4054 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 566/1000\n",
      "2106/2106 [==============================] - 1s 453us/step - loss: 0.3491 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3286 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 567/1000\n",
      "2106/2106 [==============================] - 1s 454us/step - loss: 0.3343 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3596 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 568/1000\n",
      "2106/2106 [==============================] - 1s 464us/step - loss: 0.3486 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3440 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 569/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.3601 - acc: 0.9416 - recall: 0.0261 - fbeta_score: 0.0285 - val_loss: 0.3451 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 570/1000\n",
      "2106/2106 [==============================] - 1s 444us/step - loss: 0.3495 - acc: 0.9406 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3207 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 571/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3856 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4203 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 572/1000\n",
      "2106/2106 [==============================] - 1s 501us/step - loss: 0.3769 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3332 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 573/1000\n",
      "2106/2106 [==============================] - 1s 432us/step - loss: 0.3503 - acc: 0.9392 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4272 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 574/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3450 - acc: 0.9387 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3045 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 575/1000\n",
      "2106/2106 [==============================] - 1s 447us/step - loss: 0.3663 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3128 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 576/1000\n",
      "2106/2106 [==============================] - 1s 435us/step - loss: 0.3470 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3274 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 577/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.3502 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3223 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 578/1000\n",
      "2106/2106 [==============================] - 1s 431us/step - loss: 0.3405 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3739 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 579/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3501 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3252 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 580/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3633 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3193 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 581/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3737 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3825 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 582/1000\n",
      "2106/2106 [==============================] - 1s 429us/step - loss: 0.3551 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3546 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 583/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3559 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3351 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 584/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3483 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3251 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 585/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3460 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3674 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 586/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3634 - acc: 0.9364 - recall: 0.0214 - fbeta_score: 0.0237 - val_loss: 0.3362 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 587/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3714 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3360 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 588/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3363 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3418 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 589/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3645 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3618 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 590/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3535 - acc: 0.9421 - recall: 0.0063 - fbeta_score: 0.0071 - val_loss: 0.3391 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 591/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3472 - acc: 0.9406 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.3371 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 592/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3496 - acc: 0.9416 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.4784 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 593/1000\n",
      "2106/2106 [==============================] - 1s 419us/step - loss: 0.3527 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3524 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 594/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3567 - acc: 0.9397 - recall: 0.0253 - fbeta_score: 0.0261 - val_loss: 0.3569 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 595/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.3307 - acc: 0.9397 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3574 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 596/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3636 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3871 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 597/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3653 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3182 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 598/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3323 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3223 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 599/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.3308 - acc: 0.9406 - recall: 0.0104 - fbeta_score: 0.0111 - val_loss: 0.3257 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 600/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3473 - acc: 0.9383 - recall: 0.0380 - fbeta_score: 0.0396 - val_loss: 0.3187 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 601/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3523 - acc: 0.9402 - recall: 0.0063 - fbeta_score: 0.0071 - val_loss: 0.3386 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 602/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3561 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3368 - val_acc: 0.9702 - val_recall: 0.2340 - val_fbeta_score: 0.2411\n",
      "Epoch 603/1000\n",
      "2106/2106 [==============================] - 1s 401us/step - loss: 0.3485 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3238 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 604/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3454 - acc: 0.9421 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4183 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 605/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3554 - acc: 0.9387 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3208 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 606/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3378 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3224 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 607/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3547 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3088 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 608/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3380 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3412 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 609/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3407 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3352 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 610/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3289 - acc: 0.9435 - recall: 0.0427 - fbeta_score: 0.0427 - val_loss: 0.3283 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 611/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3545 - acc: 0.9392 - recall: 0.0111 - fbeta_score: 0.0119 - val_loss: 0.3064 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 612/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3524 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3453 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 613/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3442 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3507 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 614/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3447 - acc: 0.9421 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3515 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 615/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3588 - acc: 0.9416 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3191 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 616/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3524 - acc: 0.9430 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3387 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 617/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3473 - acc: 0.9397 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3450 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 618/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3367 - acc: 0.9392 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3548 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 619/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3593 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3082 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 620/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.3357 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3411 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 621/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.3405 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3380 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 622/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3481 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3284 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 623/1000\n",
      "2106/2106 [==============================] - 1s 487us/step - loss: 0.3309 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3473 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 624/1000\n",
      "2106/2106 [==============================] - 1s 464us/step - loss: 0.3522 - acc: 0.9411 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3801 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 625/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3776 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3907 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 626/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3467 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3415 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 627/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.3390 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3801 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 628/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3596 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3667 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 629/1000\n",
      "2106/2106 [==============================] - 1s 419us/step - loss: 0.3555 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3285 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 630/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3476 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3963 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 631/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3494 - acc: 0.9406 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3375 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 632/1000\n",
      "2106/2106 [==============================] - 1s 436us/step - loss: 0.3380 - acc: 0.9411 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3321 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 633/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.3507 - acc: 0.9378 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3095 - val_acc: 0.9702 - val_recall: 0.2340 - val_fbeta_score: 0.2411\n",
      "Epoch 634/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3324 - acc: 0.9421 - recall: 0.0309 - fbeta_score: 0.0332 - val_loss: 0.3124 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 635/1000\n",
      "2106/2106 [==============================] - 1s 400us/step - loss: 0.3414 - acc: 0.9435 - recall: 0.0309 - fbeta_score: 0.0332 - val_loss: 0.3103 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 636/1000\n",
      "2106/2106 [==============================] - 1s 401us/step - loss: 0.3466 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3589 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 637/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3439 - acc: 0.9416 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.3293 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 638/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3511 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3214 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 639/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3348 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0142 - val_loss: 0.3096 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 640/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3336 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.4254 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 641/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3399 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3155 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 642/1000\n",
      "2106/2106 [==============================] - ETA: 0s - loss: 0.3450 - acc: 0.9410 - recall: 0.0155 - fbeta_score: 0.01 - 1s 408us/step - loss: 0.3446 - acc: 0.9411 - recall: 0.0154 - fbeta_score: 0.0161 - val_loss: 0.3368 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 643/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3422 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3551 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 644/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3323 - acc: 0.9383 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3508 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 645/1000\n",
      "2106/2106 [==============================] - 1s 451us/step - loss: 0.3364 - acc: 0.9392 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3350 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 646/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3390 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3208 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 647/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3408 - acc: 0.9387 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3533 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 648/1000\n",
      "2106/2106 [==============================] - 1s 434us/step - loss: 0.3330 - acc: 0.9421 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3421 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 649/1000\n",
      "2106/2106 [==============================] - 1s 457us/step - loss: 0.3538 - acc: 0.9406 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3332 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 650/1000\n",
      "2106/2106 [==============================] - 1s 473us/step - loss: 0.3455 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3118 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 651/1000\n",
      "2106/2106 [==============================] - 1s 460us/step - loss: 0.3435 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3157 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 652/1000\n",
      "2106/2106 [==============================] - 1s 463us/step - loss: 0.3386 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3594 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 653/1000\n",
      "2106/2106 [==============================] - 1s 459us/step - loss: 0.3424 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3213 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 654/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3510 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3599 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 655/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3615 - acc: 0.9383 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3886 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 656/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3487 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3229 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 657/1000\n",
      "2106/2106 [==============================] - 1s 429us/step - loss: 0.3354 - acc: 0.9425 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3056 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 658/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.3397 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4092 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 659/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3610 - acc: 0.9392 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3318 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 660/1000\n",
      "2106/2106 [==============================] - 1s 427us/step - loss: 0.3300 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3320 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 661/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.3396 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4059 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 662/1000\n",
      "2106/2106 [==============================] - 1s 428us/step - loss: 0.3432 - acc: 0.9397 - recall: 0.0380 - fbeta_score: 0.0396 - val_loss: 0.3510 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 663/1000\n",
      "2106/2106 [==============================] - 1s 438us/step - loss: 0.3302 - acc: 0.9397 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3347 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 664/1000\n",
      "2106/2106 [==============================] - 1s 438us/step - loss: 0.3521 - acc: 0.9378 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.4070 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 665/1000\n",
      "2106/2106 [==============================] - 1s 441us/step - loss: 0.3465 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3291 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 666/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3463 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3466 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 667/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3330 - acc: 0.9383 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3747 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 668/1000\n",
      "2106/2106 [==============================] - 1s 428us/step - loss: 0.3464 - acc: 0.9387 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3321 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 669/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3442 - acc: 0.9440 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3354 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 670/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3539 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0063 - val_loss: 0.4314 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 671/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3690 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3443 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 672/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3458 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3576 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 673/1000\n",
      "2106/2106 [==============================] - 1s 401us/step - loss: 0.3529 - acc: 0.9411 - recall: 0.0016 - fbeta_score: 0.0024 - val_loss: 0.3424 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 674/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3279 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3273 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 675/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3433 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3191 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 676/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3646 - acc: 0.9402 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3342 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 677/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3278 - acc: 0.9421 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3115 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 678/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3361 - acc: 0.9378 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3478 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 679/1000\n",
      "2106/2106 [==============================] - 1s 416us/step - loss: 0.3529 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3022 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 680/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3282 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3279 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 681/1000\n",
      "2106/2106 [==============================] - 1s 403us/step - loss: 0.3553 - acc: 0.9378 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3204 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 682/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3399 - acc: 0.9392 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3392 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 683/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3519 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3188 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 684/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3379 - acc: 0.9416 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3473 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 685/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3562 - acc: 0.9373 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3265 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 686/1000\n",
      "2106/2106 [==============================] - 1s 434us/step - loss: 0.3396 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3046 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 687/1000\n",
      "2106/2106 [==============================] - 1s 412us/step - loss: 0.3354 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3558 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 688/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.3476 - acc: 0.9430 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3186 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 689/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.3309 - acc: 0.9406 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3229 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 690/1000\n",
      "2106/2106 [==============================] - 1s 434us/step - loss: 0.3512 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3453 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 691/1000\n",
      "2106/2106 [==============================] - 1s 472us/step - loss: 0.3388 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.3051 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 692/1000\n",
      "2106/2106 [==============================] - 1s 471us/step - loss: 0.3372 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3370 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 693/1000\n",
      "2106/2106 [==============================] - 1s 474us/step - loss: 0.3456 - acc: 0.9373 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3344 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 694/1000\n",
      "2106/2106 [==============================] - 1s 474us/step - loss: 0.3369 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3028 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 695/1000\n",
      "2106/2106 [==============================] - 1s 568us/step - loss: 0.3299 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3533 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 696/1000\n",
      "2106/2106 [==============================] - 1s 499us/step - loss: 0.3529 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3242 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 697/1000\n",
      "2106/2106 [==============================] - 1s 510us/step - loss: 0.3451 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3174 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 698/1000\n",
      "2106/2106 [==============================] - 1s 553us/step - loss: 0.3423 - acc: 0.9402 - recall: 0.0087 - fbeta_score: 0.0103 - val_loss: 0.3241 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 699/1000\n",
      "2106/2106 [==============================] - 1s 470us/step - loss: 0.3335 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3252 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 700/1000\n",
      "2106/2106 [==============================] - 1s 438us/step - loss: 0.3538 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3589 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 701/1000\n",
      "2106/2106 [==============================] - 1s 469us/step - loss: 0.3607 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3310 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 702/1000\n",
      "2106/2106 [==============================] - 1s 451us/step - loss: 0.3437 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3559 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 703/1000\n",
      "2106/2106 [==============================] - 1s 474us/step - loss: 0.3421 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3309 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 704/1000\n",
      "2106/2106 [==============================] - 1s 397us/step - loss: 0.3580 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3191 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 705/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.3469 - acc: 0.9411 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3617 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 706/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3471 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3336 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 707/1000\n",
      "2106/2106 [==============================] - 1s 446us/step - loss: 0.3398 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3223 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 708/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3494 - acc: 0.9430 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3599 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 709/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3460 - acc: 0.9397 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3602 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 710/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.3375 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3241 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 711/1000\n",
      "2106/2106 [==============================] - 1s 400us/step - loss: 0.3438 - acc: 0.9402 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3834 - val_acc: 0.9532 - val_recall: 0.3617 - val_fbeta_score: 0.3688\n",
      "Epoch 712/1000\n",
      "2106/2106 [==============================] - 1s 388us/step - loss: 0.3517 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3481 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 713/1000\n",
      "2106/2106 [==============================] - 1s 393us/step - loss: 0.3460 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3191 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 714/1000\n",
      "2106/2106 [==============================] - 1s 381us/step - loss: 0.3425 - acc: 0.9421 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3587 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 715/1000\n",
      "2106/2106 [==============================] - 1s 376us/step - loss: 0.3499 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3384 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 716/1000\n",
      "2106/2106 [==============================] - 1s 387us/step - loss: 0.3371 - acc: 0.9430 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3250 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 717/1000\n",
      "2106/2106 [==============================] - 1s 385us/step - loss: 0.3365 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3303 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 718/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3403 - acc: 0.9402 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3363 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 719/1000\n",
      "2106/2106 [==============================] - 1s 382us/step - loss: 0.3458 - acc: 0.9392 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3326 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 720/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.3505 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4050 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 721/1000\n",
      "2106/2106 [==============================] - 1s 448us/step - loss: 0.3492 - acc: 0.9425 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3119 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 722/1000\n",
      "2106/2106 [==============================] - 1s 437us/step - loss: 0.3258 - acc: 0.9411 - recall: 0.0427 - fbeta_score: 0.0443 - val_loss: 0.3391 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 723/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3543 - acc: 0.9402 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3265 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 724/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3442 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3261 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 725/1000\n",
      "2106/2106 [==============================] - 1s 401us/step - loss: 0.3429 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3982 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 726/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3502 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4002 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 727/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3286 - acc: 0.9416 - recall: 0.0380 - fbeta_score: 0.0380 - val_loss: 0.3582 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 728/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.3551 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3671 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 729/1000\n",
      "2106/2106 [==============================] - 1s 429us/step - loss: 0.3537 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3272 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 730/1000\n",
      "2106/2106 [==============================] - 1s 395us/step - loss: 0.3426 - acc: 0.9387 - recall: 0.0063 - fbeta_score: 0.0071 - val_loss: 0.3135 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 731/1000\n",
      "2106/2106 [==============================] - 1s 420us/step - loss: 0.3460 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3738 - val_acc: 0.9447 - val_recall: 0.0213 - val_fbeta_score: 0.0284\n",
      "Epoch 732/1000\n",
      "2106/2106 [==============================] - 1s 403us/step - loss: 0.3550 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3626 - val_acc: 0.9660 - val_recall: 0.2766 - val_fbeta_score: 0.2837\n",
      "Epoch 733/1000\n",
      "2106/2106 [==============================] - 1s 390us/step - loss: 0.3269 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3313 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 734/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3437 - acc: 0.9440 - recall: 0.0570 - fbeta_score: 0.0570 - val_loss: 0.3248 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 735/1000\n",
      "2106/2106 [==============================] - 1s 441us/step - loss: 0.3425 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3336 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 736/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3414 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3692 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 737/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3475 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3359 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 738/1000\n",
      "2106/2106 [==============================] - 1s 395us/step - loss: 0.3587 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3285 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 739/1000\n",
      "2106/2106 [==============================] - 1s 397us/step - loss: 0.3536 - acc: 0.9421 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3382 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 740/1000\n",
      "2106/2106 [==============================] - 1s 387us/step - loss: 0.3401 - acc: 0.9397 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3252 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 741/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3580 - acc: 0.9402 - recall: 0.0214 - fbeta_score: 0.0237 - val_loss: 0.3138 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 742/1000\n",
      "2106/2106 [==============================] - 1s 395us/step - loss: 0.3573 - acc: 0.9392 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3136 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 743/1000\n",
      "2106/2106 [==============================] - 1s 386us/step - loss: 0.3586 - acc: 0.9416 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3295 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 744/1000\n",
      "2106/2106 [==============================] - 1s 378us/step - loss: 0.3573 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3613 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 745/1000\n",
      "2106/2106 [==============================] - 1s 381us/step - loss: 0.3532 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3774 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 746/1000\n",
      "2106/2106 [==============================] - 1s 376us/step - loss: 0.3513 - acc: 0.9378 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3885 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 747/1000\n",
      "2106/2106 [==============================] - 1s 386us/step - loss: 0.3521 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3476 - val_acc: 0.9489 - val_recall: 0.0426 - val_fbeta_score: 0.0426\n",
      "Epoch 748/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3418 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3317 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 749/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3487 - acc: 0.9416 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3842 - val_acc: 0.9447 - val_recall: 0.0213 - val_fbeta_score: 0.0284\n",
      "Epoch 750/1000\n",
      "2106/2106 [==============================] - 1s 466us/step - loss: 0.3619 - acc: 0.9397 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4238 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 751/1000\n",
      "2106/2106 [==============================] - 1s 396us/step - loss: 0.3518 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4369 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 752/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3517 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3216 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 753/1000\n",
      "2106/2106 [==============================] - 1s 389us/step - loss: 0.3340 - acc: 0.9430 - recall: 0.0324 - fbeta_score: 0.0340 - val_loss: 0.3556 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 754/1000\n",
      "2106/2106 [==============================] - 1s 390us/step - loss: 0.3462 - acc: 0.9425 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3523 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 755/1000\n",
      "2106/2106 [==============================] - 1s 387us/step - loss: 0.3539 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3210 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 756/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3473 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3263 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 757/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3426 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3360 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 758/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3315 - acc: 0.9421 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3371 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 759/1000\n",
      "2106/2106 [==============================] - 1s 406us/step - loss: 0.3452 - acc: 0.9411 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3680 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 760/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3470 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3781 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 761/1000\n",
      "2106/2106 [==============================] - 1s 385us/step - loss: 0.3669 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3660 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 762/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3575 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3129 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 763/1000\n",
      "2106/2106 [==============================] - 1s 383us/step - loss: 0.3620 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3722 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 764/1000\n",
      "2106/2106 [==============================] - 1s 381us/step - loss: 0.3468 - acc: 0.9402 - recall: 0.0380 - fbeta_score: 0.0396 - val_loss: 0.3028 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 765/1000\n",
      "2106/2106 [==============================] - 1s 398us/step - loss: 0.3303 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3868 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 766/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3747 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3680 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 767/1000\n",
      "2106/2106 [==============================] - 1s 385us/step - loss: 0.3613 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3442 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 768/1000\n",
      "2106/2106 [==============================] - 1s 378us/step - loss: 0.3498 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.4930 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 769/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3719 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3220 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 770/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3583 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3429 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 771/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3400 - acc: 0.9425 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3814 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 772/1000\n",
      "2106/2106 [==============================] - 1s 398us/step - loss: 0.3549 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3438 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 773/1000\n",
      "2106/2106 [==============================] - 1s 389us/step - loss: 0.3528 - acc: 0.9425 - recall: 0.0380 - fbeta_score: 0.0380 - val_loss: 0.3130 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 774/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3377 - acc: 0.9406 - recall: 0.0206 - fbeta_score: 0.0230 - val_loss: 0.3640 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 775/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3557 - acc: 0.9383 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3169 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 776/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3427 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3199 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 777/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3622 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3745 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 778/1000\n",
      "2106/2106 [==============================] - 1s 392us/step - loss: 0.3484 - acc: 0.9416 - recall: 0.0135 - fbeta_score: 0.0150 - val_loss: 0.3439 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 779/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3578 - acc: 0.9402 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3876 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 780/1000\n",
      "2106/2106 [==============================] - 1s 387us/step - loss: 0.3487 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3994 - val_acc: 0.8936 - val_recall: 0.4043 - val_fbeta_score: 0.3262\n",
      "Epoch 781/1000\n",
      "2106/2106 [==============================] - 1s 384us/step - loss: 0.3436 - acc: 0.9383 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3235 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 782/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3404 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3218 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 783/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3660 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3310 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 784/1000\n",
      "2106/2106 [==============================] - 1s 382us/step - loss: 0.3433 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3213 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 785/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3273 - acc: 0.9402 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3278 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 786/1000\n",
      "2106/2106 [==============================] - 1s 393us/step - loss: 0.3519 - acc: 0.9411 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3161 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 787/1000\n",
      "2106/2106 [==============================] - 1s 385us/step - loss: 0.3432 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3108 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 788/1000\n",
      "2106/2106 [==============================] - 1s 392us/step - loss: 0.3607 - acc: 0.9373 - recall: 0.0111 - fbeta_score: 0.0119 - val_loss: 0.3576 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 789/1000\n",
      "2106/2106 [==============================] - 1s 389us/step - loss: 0.3500 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3384 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 790/1000\n",
      "2106/2106 [==============================] - 1s 387us/step - loss: 0.3342 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0142 - val_loss: 0.3400 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 791/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.3625 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3064 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 792/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.3377 - acc: 0.9402 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3658 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 793/1000\n",
      "2106/2106 [==============================] - 1s 437us/step - loss: 0.3519 - acc: 0.9397 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3146 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 794/1000\n",
      "2106/2106 [==============================] - 1s 450us/step - loss: 0.3459 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3741 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 795/1000\n",
      "2106/2106 [==============================] - 1s 444us/step - loss: 0.3483 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3452 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 796/1000\n",
      "2106/2106 [==============================] - 1s 439us/step - loss: 0.3608 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3195 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 797/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3402 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3109 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 798/1000\n",
      "2106/2106 [==============================] - 1s 430us/step - loss: 0.3520 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3488 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 799/1000\n",
      "2106/2106 [==============================] - 1s 471us/step - loss: 0.3499 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3685 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 800/1000\n",
      "2106/2106 [==============================] - 1s 451us/step - loss: 0.3459 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3107 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 801/1000\n",
      "2106/2106 [==============================] - 1s 472us/step - loss: 0.3447 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3207 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 802/1000\n",
      "2106/2106 [==============================] - 1s 443us/step - loss: 0.3312 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3637 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 803/1000\n",
      "2106/2106 [==============================] - 1s 455us/step - loss: 0.3755 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3338 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 804/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3498 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.3949 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 805/1000\n",
      "2106/2106 [==============================] - 1s 435us/step - loss: 0.3490 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3537 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 806/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 544us/step - loss: 0.3719 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3478 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 807/1000\n",
      "2106/2106 [==============================] - 1s 434us/step - loss: 0.3489 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3260 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 808/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.3607 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3316 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 809/1000\n",
      "2106/2106 [==============================] - 1s 428us/step - loss: 0.3384 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3114 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 810/1000\n",
      "2106/2106 [==============================] - 1s 414us/step - loss: 0.3486 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3534 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 811/1000\n",
      "2106/2106 [==============================] - 1s 456us/step - loss: 0.3547 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3290 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 812/1000\n",
      "2106/2106 [==============================] - 1s 457us/step - loss: 0.3620 - acc: 0.9425 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3355 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 813/1000\n",
      "2106/2106 [==============================] - 1s 421us/step - loss: 0.3483 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3317 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 814/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3491 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3846 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 815/1000\n",
      "2106/2106 [==============================] - 1s 432us/step - loss: 0.3575 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3407 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 816/1000\n",
      "2106/2106 [==============================] - 1s 436us/step - loss: 0.3353 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3333 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 817/1000\n",
      "2106/2106 [==============================] - 1s 430us/step - loss: 0.3355 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3064 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 818/1000\n",
      "2106/2106 [==============================] - 1s 431us/step - loss: 0.3603 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3411 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 819/1000\n",
      "2106/2106 [==============================] - 1s 450us/step - loss: 0.3509 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3448 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 820/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3543 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3438 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 821/1000\n",
      "2106/2106 [==============================] - 1s 432us/step - loss: 0.3437 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3180 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 822/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.3337 - acc: 0.9416 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3241 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 823/1000\n",
      "2106/2106 [==============================] - 1s 391us/step - loss: 0.3365 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3593 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 824/1000\n",
      "2106/2106 [==============================] - 1s 407us/step - loss: 0.3569 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3466 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 825/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3497 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3161 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 826/1000\n",
      "2106/2106 [==============================] - 1s 384us/step - loss: 0.3400 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3352 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 827/1000\n",
      "2106/2106 [==============================] - 1s 436us/step - loss: 0.3541 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3019 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 828/1000\n",
      "2106/2106 [==============================] - 1s 475us/step - loss: 0.3313 - acc: 0.9392 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3422 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 829/1000\n",
      "2106/2106 [==============================] - 1s 476us/step - loss: 0.3479 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3091 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 830/1000\n",
      "2106/2106 [==============================] - 1s 473us/step - loss: 0.3521 - acc: 0.9411 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3589 - val_acc: 0.9617 - val_recall: 0.1489 - val_fbeta_score: 0.1560\n",
      "Epoch 831/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.3414 - acc: 0.9411 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3700 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 832/1000\n",
      "2106/2106 [==============================] - 1s 531us/step - loss: 0.3609 - acc: 0.9373 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3441 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 833/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3510 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3398 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 834/1000\n",
      "2106/2106 [==============================] - 1s 460us/step - loss: 0.3522 - acc: 0.9416 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3398 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 835/1000\n",
      "2106/2106 [==============================] - 1s 475us/step - loss: 0.3394 - acc: 0.9425 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3168 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 836/1000\n",
      "2106/2106 [==============================] - 1s 457us/step - loss: 0.3325 - acc: 0.9425 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3254 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 837/1000\n",
      "2106/2106 [==============================] - 1s 477us/step - loss: 0.3554 - acc: 0.9378 - recall: 0.0047 - fbeta_score: 0.0063 - val_loss: 0.3462 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 838/1000\n",
      "2106/2106 [==============================] - 1s 460us/step - loss: 0.3584 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3310 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 839/1000\n",
      "2106/2106 [==============================] - 1s 466us/step - loss: 0.3539 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3020 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 840/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3464 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3406 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 841/1000\n",
      "2106/2106 [==============================] - 1s 409us/step - loss: 0.3588 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3317 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 842/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3340 - acc: 0.9421 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3246 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 843/1000\n",
      "2106/2106 [==============================] - 1s 489us/step - loss: 0.3443 - acc: 0.9411 - recall: 0.0111 - fbeta_score: 0.0119 - val_loss: 0.3166 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 844/1000\n",
      "2106/2106 [==============================] - 1s 428us/step - loss: 0.3423 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.2975 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 845/1000\n",
      "2106/2106 [==============================] - 1s 451us/step - loss: 0.3424 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3782 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 846/1000\n",
      "2106/2106 [==============================] - 1s 441us/step - loss: 0.3625 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3511 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 847/1000\n",
      "2106/2106 [==============================] - 1s 425us/step - loss: 0.3371 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0190 - val_loss: 0.3335 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 848/1000\n",
      "2106/2106 [==============================] - 1s 397us/step - loss: 0.3652 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3406 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 849/1000\n",
      "2106/2106 [==============================] - 1s 545us/step - loss: 0.3320 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3230 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 850/1000\n",
      "2106/2106 [==============================] - 1s 549us/step - loss: 0.3495 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3705 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 851/1000\n",
      "2106/2106 [==============================] - 1s 553us/step - loss: 0.3504 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3624 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 852/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3551 - acc: 0.9402 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3092 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 853/1000\n",
      "2106/2106 [==============================] - 1s 615us/step - loss: 0.3352 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3238 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 854/1000\n",
      "2106/2106 [==============================] - 1s 519us/step - loss: 0.3395 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.3273 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 855/1000\n",
      "2106/2106 [==============================] - 1s 464us/step - loss: 0.3419 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3637 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 856/1000\n",
      "2106/2106 [==============================] - 1s 433us/step - loss: 0.3432 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3493 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 857/1000\n",
      "2106/2106 [==============================] - 1s 402us/step - loss: 0.3312 - acc: 0.9373 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3557 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 858/1000\n",
      "2106/2106 [==============================] - 1s 415us/step - loss: 0.3473 - acc: 0.9387 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3187 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 859/1000\n",
      "2106/2106 [==============================] - 1s 405us/step - loss: 0.3411 - acc: 0.9392 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3513 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 860/1000\n",
      "2106/2106 [==============================] - 1s 417us/step - loss: 0.3329 - acc: 0.9425 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3087 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 861/1000\n",
      "2106/2106 [==============================] - 1s 427us/step - loss: 0.3719 - acc: 0.9383 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3836 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 862/1000\n",
      "2106/2106 [==============================] - 1s 410us/step - loss: 0.3515 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3300 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 863/1000\n",
      "2106/2106 [==============================] - 1s 378us/step - loss: 0.3391 - acc: 0.9425 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.3118 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 864/1000\n",
      "2106/2106 [==============================] - 1s 424us/step - loss: 0.3409 - acc: 0.9435 - recall: 0.0499 - fbeta_score: 0.0506 - val_loss: 0.3421 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 865/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3437 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3601 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 866/1000\n",
      "2106/2106 [==============================] - 1s 413us/step - loss: 0.3439 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3214 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 867/1000\n",
      "2106/2106 [==============================] - 1s 398us/step - loss: 0.3284 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3388 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 868/1000\n",
      "2106/2106 [==============================] - 1s 436us/step - loss: 0.3426 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3379 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 869/1000\n",
      "2106/2106 [==============================] - 1s 434us/step - loss: 0.3576 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3192 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 870/1000\n",
      "2106/2106 [==============================] - 1s 401us/step - loss: 0.3524 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3409 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 871/1000\n",
      "2106/2106 [==============================] - 1s 422us/step - loss: 0.3324 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3407 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 872/1000\n",
      "2106/2106 [==============================] - 1s 418us/step - loss: 0.3643 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3299 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 873/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3513 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3625 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 874/1000\n",
      "2106/2106 [==============================] - 1s 378us/step - loss: 0.3475 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3346 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 875/1000\n",
      "2106/2106 [==============================] - 1s 499us/step - loss: 0.3503 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3171 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 876/1000\n",
      "2106/2106 [==============================] - 1s 643us/step - loss: 0.3452 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3217 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 877/1000\n",
      "2106/2106 [==============================] - 1s 394us/step - loss: 0.3417 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3354 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 878/1000\n",
      "2106/2106 [==============================] - 1s 378us/step - loss: 0.3413 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3196 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 879/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3379 - acc: 0.9383 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3204 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 880/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3602 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4010 - val_acc: 0.9702 - val_recall: 0.3617 - val_fbeta_score: 0.3617\n",
      "Epoch 881/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3680 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3277 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 882/1000\n",
      "2106/2106 [==============================] - 1s 375us/step - loss: 0.3489 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3483 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 883/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3520 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3092 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 884/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3469 - acc: 0.9416 - recall: 0.0040 - fbeta_score: 0.0055 - val_loss: 0.3551 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 885/1000\n",
      "2106/2106 [==============================] - 1s 382us/step - loss: 0.3434 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3558 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 886/1000\n",
      "2106/2106 [==============================] - 1s 382us/step - loss: 0.3548 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3188 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 887/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3522 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3470 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 888/1000\n",
      "2106/2106 [==============================] - 1s 383us/step - loss: 0.3514 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3305 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 889/1000\n",
      "2106/2106 [==============================] - 1s 376us/step - loss: 0.3411 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3300 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 890/1000\n",
      "2106/2106 [==============================] - 1s 378us/step - loss: 0.3467 - acc: 0.9421 - recall: 0.0348 - fbeta_score: 0.0356 - val_loss: 0.3151 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 891/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3395 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3246 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 892/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3369 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3786 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 893/1000\n",
      "2106/2106 [==============================] - 1s 378us/step - loss: 0.3654 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3593 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 894/1000\n",
      "2106/2106 [==============================] - 1s 381us/step - loss: 0.3567 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3292 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 895/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3460 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3684 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 896/1000\n",
      "2106/2106 [==============================] - 1s 375us/step - loss: 0.3529 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3223 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 897/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3567 - acc: 0.9406 - recall: 0.0158 - fbeta_score: 0.0182 - val_loss: 0.3291 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 898/1000\n",
      "2106/2106 [==============================] - 1s 376us/step - loss: 0.3588 - acc: 0.9421 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4132 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 899/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3775 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3451 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 900/1000\n",
      "2106/2106 [==============================] - 1s 387us/step - loss: 0.3303 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3185 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 901/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3454 - acc: 0.9430 - recall: 0.0475 - fbeta_score: 0.0491 - val_loss: 0.3290 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 902/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3598 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3874 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 903/1000\n",
      "2106/2106 [==============================] - 1s 375us/step - loss: 0.3617 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3411 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 904/1000\n",
      "2106/2106 [==============================] - 1s 384us/step - loss: 0.3392 - acc: 0.9402 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3437 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 905/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3750 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3502 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 906/1000\n",
      "2106/2106 [==============================] - 1s 376us/step - loss: 0.3398 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4311 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 907/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3516 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3311 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 908/1000\n",
      "2106/2106 [==============================] - 1s 374us/step - loss: 0.3385 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3142 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 909/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3393 - acc: 0.9397 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4042 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 910/1000\n",
      "2106/2106 [==============================] - 1s 376us/step - loss: 0.3505 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3512 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 911/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3535 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3466 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 912/1000\n",
      "2106/2106 [==============================] - 1s 382us/step - loss: 0.3489 - acc: 0.9397 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3588 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 913/1000\n",
      "2106/2106 [==============================] - 1s 388us/step - loss: 0.3440 - acc: 0.9406 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3247 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 914/1000\n",
      "2106/2106 [==============================] - 1s 378us/step - loss: 0.3362 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4620 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 915/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3494 - acc: 0.9416 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3044 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 916/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3421 - acc: 0.9397 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3313 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 917/1000\n",
      "2106/2106 [==============================] - 1s 376us/step - loss: 0.3420 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3648 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 918/1000\n",
      "2106/2106 [==============================] - 1s 430us/step - loss: 0.3512 - acc: 0.9392 - recall: 0.0063 - fbeta_score: 0.0071 - val_loss: 0.3073 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 919/1000\n",
      "2106/2106 [==============================] - 1s 403us/step - loss: 0.3292 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3216 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 920/1000\n",
      "2106/2106 [==============================] - 1s 401us/step - loss: 0.3633 - acc: 0.9402 - recall: 0.0063 - fbeta_score: 0.0071 - val_loss: 0.3432 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 921/1000\n",
      "2106/2106 [==============================] - 1s 400us/step - loss: 0.3433 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3871 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 922/1000\n",
      "2106/2106 [==============================] - 1s 394us/step - loss: 0.3413 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3159 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 923/1000\n",
      "2106/2106 [==============================] - 1s 378us/step - loss: 0.3578 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3359 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 924/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3754 - acc: 0.9425 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4000 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 925/1000\n",
      "2106/2106 [==============================] - 1s 384us/step - loss: 0.3618 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3540 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 926/1000\n",
      "2106/2106 [==============================] - 1s 383us/step - loss: 0.3432 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3403 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 927/1000\n",
      "2106/2106 [==============================] - 1s 380us/step - loss: 0.3484 - acc: 0.9421 - recall: 0.0451 - fbeta_score: 0.0459 - val_loss: 0.3095 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 928/1000\n",
      "2106/2106 [==============================] - 1s 388us/step - loss: 0.3404 - acc: 0.9411 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3353 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 929/1000\n",
      "2106/2106 [==============================] - 1s 395us/step - loss: 0.3489 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3477 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 930/1000\n",
      "2106/2106 [==============================] - 1s 378us/step - loss: 0.3430 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3707 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 931/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3641 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3888 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 932/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3499 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3275 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 933/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3504 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3574 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 934/1000\n",
      "2106/2106 [==============================] - 1s 376us/step - loss: 0.3416 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3952 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 935/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3492 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3275 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 936/1000\n",
      "2106/2106 [==============================] - 1s 379us/step - loss: 0.3442 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.2997 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 937/1000\n",
      "2106/2106 [==============================] - 1s 386us/step - loss: 0.3506 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3317 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 938/1000\n",
      "2106/2106 [==============================] - 1s 382us/step - loss: 0.3408 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3353 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 939/1000\n",
      "2106/2106 [==============================] - 1s 375us/step - loss: 0.3586 - acc: 0.9416 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3518 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 940/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3822 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3554 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 941/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3273 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3784 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 942/1000\n",
      "2106/2106 [==============================] - 1s 376us/step - loss: 0.3488 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3217 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 943/1000\n",
      "2106/2106 [==============================] - 1s 375us/step - loss: 0.3624 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3869 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 944/1000\n",
      "2106/2106 [==============================] - 1s 378us/step - loss: 0.3544 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3535 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 945/1000\n",
      "2106/2106 [==============================] - 1s 384us/step - loss: 0.3507 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.4063 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 946/1000\n",
      "2106/2106 [==============================] - 1s 372us/step - loss: 0.3662 - acc: 0.9392 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3463 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 947/1000\n",
      "2106/2106 [==============================] - 1s 374us/step - loss: 0.3417 - acc: 0.9387 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3166 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 948/1000\n",
      "2106/2106 [==============================] - 1s 408us/step - loss: 0.3378 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3446 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 949/1000\n",
      "2106/2106 [==============================] - 1s 433us/step - loss: 0.3644 - acc: 0.9378 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3317 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 950/1000\n",
      "2106/2106 [==============================] - 1s 431us/step - loss: 0.3414 - acc: 0.9397 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3250 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 951/1000\n",
      "2106/2106 [==============================] - 1s 433us/step - loss: 0.3419 - acc: 0.9378 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3328 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 952/1000\n",
      "2106/2106 [==============================] - 1s 399us/step - loss: 0.3568 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3723 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 953/1000\n",
      "2106/2106 [==============================] - 1s 389us/step - loss: 0.3542 - acc: 0.9421 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3549 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 954/1000\n",
      "2106/2106 [==============================] - 1s 385us/step - loss: 0.3646 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3746 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 955/1000\n",
      "2106/2106 [==============================] - 1s 374us/step - loss: 0.3677 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3271 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 956/1000\n",
      "2106/2106 [==============================] - 1s 373us/step - loss: 0.3383 - acc: 0.9411 - recall: 0.0111 - fbeta_score: 0.0119 - val_loss: 0.3331 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 957/1000\n",
      "2106/2106 [==============================] - 1s 376us/step - loss: 0.3457 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.3160 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 958/1000\n",
      "2106/2106 [==============================] - 1s 386us/step - loss: 0.3717 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3029 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 959/1000\n",
      "2106/2106 [==============================] - 1s 374us/step - loss: 0.3237 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4476 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 960/1000\n",
      "2106/2106 [==============================] - 1s 375us/step - loss: 0.3584 - acc: 0.9392 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3496 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 961/1000\n",
      "2106/2106 [==============================] - 1s 375us/step - loss: 0.3360 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3197 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 962/1000\n",
      "2106/2106 [==============================] - 1s 383us/step - loss: 0.3349 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3329 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 963/1000\n",
      "2106/2106 [==============================] - 1s 374us/step - loss: 0.3494 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3883 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 964/1000\n",
      "2106/2106 [==============================] - 1s 376us/step - loss: 0.3421 - acc: 0.9402 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3492 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 965/1000\n",
      "2106/2106 [==============================] - 1s 446us/step - loss: 0.3647 - acc: 0.9421 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3553 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 966/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3442 - acc: 0.9397 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3726 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 967/1000\n",
      "2106/2106 [==============================] - 1s 459us/step - loss: 0.3669 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4433 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 968/1000\n",
      "2106/2106 [==============================] - 1s 432us/step - loss: 0.3618 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3681 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 969/1000\n",
      "2106/2106 [==============================] - 1s 442us/step - loss: 0.3592 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.3591 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 970/1000\n",
      "2106/2106 [==============================] - 1s 449us/step - loss: 0.3423 - acc: 0.9387 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3857 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 971/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3586 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4176 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 972/1000\n",
      "2106/2106 [==============================] - 1s 430us/step - loss: 0.3842 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3385 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 973/1000\n",
      "2106/2106 [==============================] - 1s 430us/step - loss: 0.3442 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3442 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 974/1000\n",
      "2106/2106 [==============================] - 1s 448us/step - loss: 0.3334 - acc: 0.9392 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3303 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 975/1000\n",
      "2106/2106 [==============================] - 1s 430us/step - loss: 0.3397 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3428 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 976/1000\n",
      "2106/2106 [==============================] - 1s 426us/step - loss: 0.3431 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3667 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 977/1000\n",
      "2106/2106 [==============================] - 1s 463us/step - loss: 0.3580 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3131 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 978/1000\n",
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.3484 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3334 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 979/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3377 - acc: 0.9416 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3245 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 980/1000\n",
      "2106/2106 [==============================] - 1s 396us/step - loss: 0.3395 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3350 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 981/1000\n",
      "2106/2106 [==============================] - 1s 400us/step - loss: 0.3380 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3151 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 982/1000\n",
      "2106/2106 [==============================] - 1s 527us/step - loss: 0.3443 - acc: 0.9402 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3262 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 983/1000\n",
      "2106/2106 [==============================] - 1s 469us/step - loss: 0.3498 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3280 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 984/1000\n",
      "2106/2106 [==============================] - 1s 401us/step - loss: 0.3509 - acc: 0.9406 - recall: 0.0063 - fbeta_score: 0.0071 - val_loss: 0.3182 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 985/1000\n",
      "2106/2106 [==============================] - 1s 373us/step - loss: 0.3488 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3774 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 986/1000\n",
      "2106/2106 [==============================] - 1s 404us/step - loss: 0.3570 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3517 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 987/1000\n",
      "2106/2106 [==============================] - 1s 423us/step - loss: 0.3391 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3455 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 988/1000\n",
      "2106/2106 [==============================] - 1s 386us/step - loss: 0.3427 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3196 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 989/1000\n",
      "2106/2106 [==============================] - 1s 376us/step - loss: 0.3422 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3052 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 990/1000\n",
      "2106/2106 [==============================] - 1s 372us/step - loss: 0.3474 - acc: 0.9392 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3390 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 991/1000\n",
      "2106/2106 [==============================] - 1s 373us/step - loss: 0.3735 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3354 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 992/1000\n",
      "2106/2106 [==============================] - 1s 377us/step - loss: 0.3428 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3461 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 993/1000\n",
      "2106/2106 [==============================] - 1s 373us/step - loss: 0.3490 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3222 - val_acc: 0.9617 - val_recall: 0.1702 - val_fbeta_score: 0.1702\n",
      "Epoch 994/1000\n",
      "2106/2106 [==============================] - 1s 372us/step - loss: 0.3326 - acc: 0.9392 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3617 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 995/1000\n",
      "2106/2106 [==============================] - 1s 375us/step - loss: 0.3410 - acc: 0.9397 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3171 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 996/1000\n",
      "2106/2106 [==============================] - 1s 397us/step - loss: 0.3757 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3979 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 997/1000\n",
      "2106/2106 [==============================] - 1s 399us/step - loss: 0.3457 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3111 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 998/1000\n",
      "2106/2106 [==============================] - 1s 411us/step - loss: 0.3397 - acc: 0.9440 - recall: 0.0404 - fbeta_score: 0.0412 - val_loss: 0.3787 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 999/1000\n",
      "2106/2106 [==============================] - 1s 385us/step - loss: 0.3338 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3415 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 1000/1000\n",
      "2106/2106 [==============================] - 1s 387us/step - loss: 0.3483 - acc: 0.9416 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3210 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19922072160>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_scale_train, y_train,\n",
    "                epochs=1000,\n",
    "                batch_size=10,\n",
    "                shuffle=True,\n",
    "#                callbacks = callback,\n",
    "                 validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586/586 [==============================] - 0s 45us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2936485053227623, 0.9539249146757679, 0.0, 0.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.evaluate(X_scale_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.918041476181011"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score = mlp.predict(X_scale_test)\n",
    "#y_score =y_score[:,1]>y_score[:,0]\n",
    "y_score[0:100]\n",
    "#y_test[0:100]\n",
    "\n",
    "roc_auc_score(y_test, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmcTfX/wPHX24zZGMsgFdn3NJRJpBBZQpsWJFr0LYlCSUJKaZEoEVp8/apvqZSSNUopEaMs2YUYyW6sM2Z5//44x7jGLHeYO3eW9/PxmId7zvmcc97nuPe+7/l8zvl8RFUxxhhj0lPI3wEYY4zJ3SxRGGOMyZAlCmOMMRmyRGGMMSZDliiMMcZkyBKFMcaYDFmiMFkmIl1F5Dt/x+FvIlJBRI6JSEAO7rOSiKiIBObUPn1JRNaKSPPzWM/egzlI7DmKvE1EtgNlgSTgGDAX6K2qx/wZV37knuuHVHWBH2OoBGwDCqtqor/icGNRoLqqbvHxfiqRS465oLIrivzhZlUtCtQHrgQG+Tme8+LPX8n55Rd6Vtj5Nt6yRJGPqOq/wDychAGAiASLyCgR2SEie0RkooiEeiy/VURWisgREflLRNq684uLyAcisltEdonIS6erWETkfhH5xX09UURGecYhIt+ISH/39aUi8qWI7BORbSLyuEe550Vkmoh8LCJHgPtTH5Mbx4fu+n+LyBARKeQRx2IReVtEYkVkg4i0TLVuRsewWETGiMhB4HkRqSoiP4jIARHZLyL/E5ESbvmPgArAt25109Opq4FE5EcRedHd7lER+U5ESnvE0909hgMiMlREtovIjWn9X4pIqIi84ZaPFZFfPP/fgK7u/+l+ERnssV5DEVkiIofd4x4nIkEey1VEHhORzcBmd95bIrLTfQ+sEJHrPcoHiMiz7nvjqLv8MhFZ5BZZ5Z6PTm75Du776bCI/CoikR7b2i4iA0VkNXBcRAI9z4Ebe7Qbxx4RGe2uenpfh919NfZ8D7rrXi4i80XkoLvus2mdV3OeVNX+8vAfsB240X1dHlgDvOWx/E1gBhABhAPfAq+4yxoCsUArnB8N5YBa7rKvgUlAEeAiYBnwiLvsfuAX93VTYCdnqjFLAieBS91trgCeA4KAKsBWoI1b9nkgAbjNLRuaxvF9CHzjxl4J2AT08IgjEegHFAY6uccT4eUxJAJ9gEAgFKjmnotgoAzOF9SbaZ1rd7oSoECgO/0j8BdQw93ej8Cr7rI6OFWD17nnYpR77Dem8/863l2/HBAAXOvGdXqf77n7qAfEA7Xd9RoAjdxjqgSsB/p6bFeB+Tjvh1B33r1AKXedJ4F/gRB32QCc91RNQNz9lfLYVjWPbV8F7AWucWO+zz1nwR7nbyVwmce+U84psATo5r4uCjRK6zyn8R4MB3a7sYe409f4+7OZn/78HoD9XeB/oPNBOwYcdT9M3wMl3GUCHAeqepRvDGxzX08CxqSxzbLul0+ox7wuwEL3teeHVIAdQFN3+j/AD+7ra4AdqbY9CPiv+/p5YFEGxxbgxlHHY94jwI8ecfyDm6TcecuAbl4ew4709u2WuQ34I9W5zixRDPFY3guY675+DvjUY1kYcIo0EgVO0jwJ1Etj2el9lk91zJ3TOYa+wHSPaQVaZHLch07vG9gI3JpOudSJYgLwYqoyG4FmHufvwTTev6cTxSLgBaB0OsecXqLo4vn/ZH/Z/2f1hPnDbaq6QESaAZ8ApYHDOL+Kw4AVInK6rOB8AYPzy252GturiPMLfbfHeoVwrhzOoqoqIlNxPqyLgHuAjz22c6mIHPZYJQD42WP6nG16KI3z6/tvj3l/4/zKPm2Xut8WHssv9fIYztq3iFwEjAWux/lVWgjnSzMr/vV4fQLnlzFuTCn7U9UTInIgnW2Uxvll/FdW9yMiNYDRQBTO/30gzlWdp9TH/STwkBujAsXcGMB5j2QUh6eKwH0i0sdjXpC73TT3nUoPYDiwQUS2AS+o6kwv9puVGM15sDaKfERVfwKm4FRrAOzH+WV6uaqWcP+Kq9PwDc6Htmoam9qJ82u8tMd6xVT18nR2/Slwp4hUxLmK+NJjO9s8tlFCVcNVtZ1n2Bkc0n6c6pmKHvMqALs8psuJRyZwl//j5TGk3vcr7rxIVS2GUyUjGZTPit04VYOA0waBU92Tlv1AHGn/32RmArAB526kYsCznH0M4HEcbnvEQOBuoKSqlsCpvju9TnrvkbTsBEak+v8OU9VP09p3aqq6WVW74FQTvgZME5EiGa1zHjGa82CJIv95E2glIvVVNRmnLnuM+2sZESknIm3csh8AD4hISxEp5C6rpaq7ge+AN0SkmLusqnvFcg5V/QPYB7wPzFPV01cQy4AjbgNmqNswWldErvbmQFQ1CfgcGCEi4W4i6s+ZKxZwvlQeF5HCInIXUBuYndVjcIXjVOMdFpFyOPXznvbgtLOcj2nAzSJyrdu4/ALnfoED4P6/TQZGi3MzQIDbgBvsxX7CgSPAMRGpBTzqRflEnP+/QBF5DueK4rT3gRdFpLo4IkXkdIJLfT7eA3qKyDVu2SIi0l5Ewr2IGxG5V0TKuMd/+j2U5MaWTPrnfiZwsYj0FefmjXARucabfRrvWKLIZ1R1H04D8FB31kBgC7BUnDuLFuA0TKKqy4AHgDE4vyJ/4syv9+441QbrcKpfpgGXZLDrT4Ebcaq+TseSBNyMcxfWNpxfyu8DxbNwSH1w2lm2Ar+425/ssfw3oLq77RHAnap6ukonq8fwAk6DbCwwC/gq1fJXgCHuHT1PZeEYUNW17rFMxbm6OIrT8BufzipP4TQiLwcO4vzC9ubz+hRO9d9RnC/uzzIpPw+Yg3OTwN84VzKe1UOjcZL1dzgJ6AOcRnRw2pj+zz0fd6tqNE4b1Tic872FNO5ky0BbYK2IHAPewml3iVPVEzj/t4vdfTXyXElVj+LchHAzTpXcZuCGLOzXZMIeuDN5lojcj/MA3HX+jiWrRKQozq/m6qq6zd/xGJMRu6IwJoeIyM0iEubWu4/CuWLY7t+ojMmcJQpjcs6tOA3t/+BUl3VWu6Q3eYBVPRljjMmQXVEYY4zJUJ574K506dJaqVIlf4dhjDF5yooVK/arapnzWTfPJYpKlSoRHR3t7zCMMSZPEZG/My+VNqt6MsYYkyFLFMYYYzJkicIYY0yGLFEYY4zJkCUKY4wxGbJEYYwxJkM+SxQiMllE9orIn+ksFxEZKyJbRGS1iFzlq1iMMcacP19eUUzB6TY4PTfh9HdTHXgYZ8AVY4wx2ezUqaQLWt9nD9yp6iIRqZRBkVuBD91O0ZaKSAkRucQdcMYYc9pX7WFbWiPWGpO5Ad+24o9/MhqGJXP+bKMox9kDpMRw9ljIKUTkYRGJFpHoffv25UhwxuQaliTMBah78V5+3lrhgrbhzy480hoGMs2ubFX1XeBdgKioKOvu1hRMT9pb32Ru3bp9/P77bu69NxKA7qo0ezWWypVfOu9t+jNRxACXeUyXx+mn3xhjTBadOJHASy8t4vXXfyUgQGjUqDzVqkUgIlSqVOKCtu3PRDED6C0iU4FrgFhrnzDGmKybM2czjz02m23bDgPQo0cDSpUKzWQt7/ksUYjIp0BzoLSIxADDgMIAqjoRmA20wxmA/QTwgK9iMcaY/GjXriP07TuPadPWARAZWZaJE9vTuPFlmayZNb6866lLJssVeMxX+zfGmPzuscdm8803GwkLK8zw4c154olGBAZm/z1KeW48CmOyld16avKYxMTklGTw2ms3UrhwAG+80ZoKFYr7bJ/WhYcp2PJKkqjczt8RGD+LjY2jT5/ZtG//CU6FDNSsWZovvrjLp0kC7IrCGIfdempyKVXliy/W0bfvXHbvPkZAgLBy5b9ceeWFPUSXFZYojDEml/rrr4P07j2HuXO3ANC4cXkmTuxAZGTZHI3DEoUxxuRCo0b9ytChC4mLS6REiRBee+1GHnroKgoVSutZZd+yRGGMMbnQiRMJxMUl0q1bJKNGteaii4r4LRZLFMYYkwvs23ecjRsPcN11Tr9MAwc2oXnzSjRtWtHPkdldT8YY41fJycr77/9OzZrj6NjxMw4ePAlAcHBgrkgSYFcUuZPd229MgfDnn3vp2XMmixc7HWm3alWFEycSiIjIvu43soMlitzIkkTOsmcUTA47fvwUw4f/xOjRS0lMTKZs2SK8+WZbOnW6HJGcb6zOjCWK3Mzu7TcmX7rzzi+YO3cLItCrVxQjRrSkRIkQf4eVLksUxhiTwwYObMKePceYMKE911xT3t/hZMoShTHG+FBiYjJvv/0b27cf5q23bgKgefNKREc/7JdnIs6HJQpjjPGRZct28cgjM1m58l8AHn64AZdffhFAnkkSYLfHGmNMtjt8OI5evWbRqNH7rFz5LxUrFufbb7ukJIm8xq4ojDEmG02d+id9+85lz57jBAYW4sknGzN0aFOKFAnyd2jnzRKFMcZko++++4s9e47TpMllTJjQniuuyNkO/HzBEoUxxlyA+PhEdu06SpUqJQEYObIV119fgfvuq5+n2iEyYm0Uxhhznn74YRuRkRNp3/4TTp1KAqB06TAeeODKfJMkwBKFMcZk2Z49x+jWbTotW37Ipk0HAIiJOeLnqHzHqp6MMcZLycnKe++t4Jlnvufw4ThCQgIZMuR6BgxoQlBQgL/D8xlLFMYY46Xbb/+MGTM2AtCmTVXGj29H1aoRfo7K96zqyRhjvNSxYy0uvrgon312J3PmdC0QSQLsiiJnWLfhxuRJM2ZsJCbmCL16XQ1A9+716NixNuHhwX6OLGdZosgJ55MkrOtrY/xmx45YHn98Dt98s5Hg4ADatq1GlSolEZEClyTAEkXOsm7DjcnVEhKSGDv2N4YN+5HjxxMIDw/ipZdaULFicX+H5leWKIwxBli6NIZHHpnJ6tV7ALjrrjqMGdOGcuWK+Tky/7NEYYwxwNChC1m9eg+VK5dg3Lh2tGtX3d8h5RqWKIwxBZKqcvToKYoVc9ocxo27iQ8/XMXgwU0JCyvs5+hyF7s91hhT4GzcuJ8bb/yIjh0/Q9VpO6xZszQjRrS0JJEGu6IwxhQYcXGJvPLKz7z66mJOnUqiVKlQtm8/TOXKJf0dWq5micJX7NkJY3KV+fP/olev2WzZchCABx+sz8iRrShVKszPkeV+Pq16EpG2IrJRRLaIyDNpLK8gIgtF5A8RWS0i+efhgdRJwp6LMMYvVJUHH/yG1q0/ZsuWg9SpU4ZFi+7ngw9utSThJZ9dUYhIADAeaAXEAMtFZIaqrvMoNgT4XFUniEgdYDZQyVcx+YU9O2GMX4kIlSqVIDQ0kOeea0b//o3zdQd+vuDLqqeGwBZV3QogIlOBWwHPRKHA6ZuUiwP/+DAeY0wBsXLlv+zefZSbbnJucR04sAndukVaW8R58mXVUzlgp8d0jDvP0/PAvSISg3M10SetDYnIwyISLSLR+/bt80Wsxph84OjRePr3n0eDBu9y331fc/DgSQCCgwMtSVwAXyaKtIZ3Sl0P0wWYoqrlgXbARyJyTkyq+q6qRqlqVJkyZXwQqjEmL1NVpk9fT5067zBmzFIA7rnnCgoXticAsoMvq55igMs8pstzbtVSD6AtgKouEZEQoDSw14dxGWPykb//Pkzv3nOYOXMTAFFRlzJpUgeuuuoSP0eWf/gy3S4HqotIZREJAjoDM1KV2QG0BBCR2kAIYHVLxhivqCp33PE5M2duolixYMaNu4mlS3tYkshmPruiUNVEEekNzAMCgMmqulZEhgPRqjoDeBJ4T0T64VRL3a+nH5PM7ew5CWP8JjlZKVRIEBFGjWrNxInRjBnThksuCfd3aPmS5JXv5dOioqI0Ojra32HAG2k1waRSuR10nOX7WIwpIA4cOMEzzywA4L33bvFzNHmLiKxQ1ajzWdeezL5Q9pyEMT6nqnz44Sqeemo++/efICgogGHDmlO+vHUBnhMsURhjcrX16/fx6KOz+OmnvwFo3rwSEya0tySRgyxRGGNyJVXluecW8tpri0lISKZ06TDeeKM13bpFIuJF1a/JNpYojDG5koiwa9dREhKS+c9/ruLVV28kIiLU32EVSJYojDG5xj//HGX//hNERpYFYOTIVvTocSVNmlTwc2QFmz22eD6+au/vCIzJV5KSkhk3bhm1a4+nc+dpnDqVBEDp0mGWJHIBu6I4H6efn7Cuw425YL//vptHHplJdLTTcUPTphU5ciSe0qWtC/DcwqtE4T5ZXUFVt/g4nrzFnpEw5rwdORLP0KE/MG7ccpKTlfLlizF2bFtuu62WNVbnMpkmChFpD4wGgoDKIlIfGKaqt/s6OGNM/qSqNG36X1at2kNAgNC/fyOef7454eHB/g7NpMGbNorhwDXAYQBVXQlU82VQxpj8TUTo168RDRuWIzr6Yd54o40liVzMm6qnBFU9nOpS0B5HNsZ47dSpJEaPXkJAgDBgQBMAunevx733RhIQYPfU5HbeJIr1InI3UEhEKgNPAEt9G5YxJr/4+ee/6dlzFuvW7SM4OIDu3etRtmxRRISAAGuLyAu8SeW9gQZAMvAVEIeTLIwxJl3795/gwQe/oWnTKaxbt4/q1SOYOfMeypYt6u/QTBZ5c0XRRlUHAgNPzxCRjjhJo2CxrsWNyZSqMmXKSgYMmM+BAycJCgpg0KDreOaZ6wgJsTvy8yJvriiGpDFvcHYHkid4Jgl7hsKYdH388RoOHDhJixaVWb26J88/39ySRB6W7v+ciLTBGaa0nIiM9lhUDKcaquCyrsWNOcuJEwnExsZxySXhiAjvvNOO5cv/oWvXK+yZiHwgoxS/F/gTp01ircf8o8AzvgzKGJN3zJmzmccem02VKiWZP78bIkLNmqWpWbO0v0Mz2STdRKGqfwB/iMj/VDUuB2MyxuQBu3YdoW/feUybtg6A8PBgDhw4aV1v5EPeVBqWE5ERQB0g5PRMVa3hs6iMMblWUlIy48cvZ8iQHzh69BRFihRm+PAbePzxawgMtGci8iNvEsUU4CVgFHAT8AAFvY3CmAIqOVlp1mwKixfvBOC222rx1lttqVChuJ8jM77kTfoPU9V5AKr6l6oOAW7wbVjGmNyoUCGhdeuqXHZZMb75pjPTp3eyJFEAeHNFES/ObQt/iUhPYBdwkW/D8hF7DsKYLFFVPv98LYGBhbjjjjoADBzYhP79G1O0aJCfozM5xZtE0Q8oCjwOjACKAw/6MiifyY4kYc9PmALir78O0qvXbL777i/KlAmjRYvKlCwZSnBwIMHWf1+BkmmiUNXf3JdHgW4AIlLel0H5nD0HYUy64uMTef31Xxkx4mfi4hIpWTKEESNaULx4SOYrm3wpw0QhIlcD5YBfVHW/iFyO05VHCyBvJwtjzDl+/HE7jz46iw0b9gPQrVsko0a15qKLivg5MuNP6TZmi8grwP+ArsBcERkMLARWAXZrrDH5TFJSMr16OUmiZs1S/PBDdz788HZLEibDK4pbgXqqelJEIoB/3OmNOROaMcbXkpOVuLhEwsIKExBQiAkT2rNo0d88/XQTgoOtbybjyOidEKeqJwFU9aCIbLAkYUz+sWbNHnr2nEWtWqX44INbAWjWrBLNmlXyb2Am18koUVQRkdNdiQtQyWMaVe3o08iMMT5x/Pgphg//idGjl5KYmMy2bYc4dOgkJUuG+js0k0tllCjuSDU9zpeBGGN879tvN9K79xx27IhFBHr1imLEiJaUKGF3NJn0ZdQp4Pc5GYgxxncSE5Pp1GkaX321HoD69S9m0qQONGxYzs+RmbzAWquMKQACAwtRvHgwRYsG8eKLN9C7d0PrwM94zafvFBFpKyIbRWSLiKQ5hoWI3C0i60RkrYh84st4jClIfvstht9+i0mZfv31Vqxf/xh9+zayJGGyxOsrChEJVtX4LJQPAMYDrYAYYLmIzFDVdR5lqgODgCaqekhE8mYfUsbkIocPxzFo0AImTVpBrVqlWbmyJ0FBAZQqZeNEmPOT6c8KEWkoImuAze50PRF524ttNwS2qOpWVT0FTMV5NsPTf4DxqnoIQFX3Zil6Y0wKVeWTT9ZQq9Y4Jk5cQUBAIW65pSZJSTYqgLkw3lxRjAU6AF8DqOoqEfGmm/FywE6P6RjgmlRlagCIyGIgAHheVed6sW1jjIfNmw/Qq9dsFizYCkCTJpcxcWIH6ta1i3Rz4bxJFIVU9e9UA6QnebFeWiOqp+6NLxCoDjTH6TvqZxGpq6qHz9qQyMPAwwAVKlTIfM/WnbgpQBISkmjR4kNiYo4QERHKyJE38sADV1KoUFofQWOyzptEsVNEGgLqtjv0ATZ5sV4McJnHdHmcbkBSl1mqqgnANhHZiJM4lnsWUtV3gXcBoqKiMu/6NaMkYd2Em3xCVRERChcOYMSIFixcuJ2RI2+kTBnrm8lkL28SxaM41U8VgD3AAndeZpYD1UWkMs5gR52Be1KV+RroAkwRkdI4VVFbvQvdC9aduMmH9uw5xlNPzadGjQiGDm0GQPfu9ejevZ6fIzP5lTeJIlFVO2d1w6qaKCK9gXk47Q+TVXWtiAwHolV1hrustYisw6nOGqCqB7K6L2MKguRk5b33VvDMM99z+HAcJUqE0LdvI8LDbRQh41veJIrlbpXQZ8BXqnrU242r6mxgdqp5z3m8VqC/+2eMSceqVf/Ss+csli51noto27Ya48e3syRhcoQ3I9xVFZFrcaqOXhCRlcBUVZ3q8+iMKeASEpIYNOh73nxzKUlJyiWXFOWtt9py5511SHWDiTE+49Xjmar6q6o+DlwFHMEZ0MgY42OBgYX4449/SU5W+vRpyPr1j3HXXZdbkjA5KtMrChEpivOgXGegNvANcK2P4zKmwNqxI5akpGQqVy6JiDBxYntiY+OJirrU36GZAsqbNoo/gW+Bkar6s4/juTD2/ITJwxISknjrrd8YNuxHGjcuz/z53RARqlcv5e/QTAHnTaKooqp5ow8AzyRhz0uYPGTJkp307DmL1av3ABAREcqJEwkUKRLk58iMySBRiMgbqvok8KWInPNAQq4e4c6enzB5xKFDJ3nmmQW8++7vAFSuXILx49tx003V/RyZMWdkdEXxmfuvjWxnjA/ExydSv/4kduyIpXDhQgwYcC2DBzclLKywv0Mz5iwZjXC3zH1ZW1XPShbug3Q2Ap4xFyA4OJAePa7k+++3MWFCe+rUKePvkIxJkze3xz6Yxrwe2R2IMfldXFwiw4Yt5JNP1qTMe/bZ6/nxx/ssSZhcLaM2ik44t8RWFpGvPBaFA4fTXssYk5b58/+iV6/ZbNlykIsuKsLtt9ciNLSwjTRn8oSM2iiWAQdwen0d7zH/KPCHL4MyJr/4999j9O8/j08//ROAyy8vw8SJHQgNtXYIk3dk1EaxDdiG01usMSYLkpKSmTRpBc8++z2xsfGEhgYybFgz+vVrTFBQgL/DMyZLMqp6+klVm4nIIc4ecEhw+vOL8Hl0xuRRSUnK228vIzY2nnbtqjNu3E1UrlzS32EZc14yqno6Pdxp6ZwIxJi87ujReJKSlBIlQggKCuC9925mz55jdOxY2/pmMnlaui1pHk9jXwYEqGoS0Bh4BLAhtIxxqSpffbWe2rXH8+ST81LmX3ddBe64w3p5NXmfN7dcfI0zDGpV4EOcjgE/8WlUxuQR27cf5pZbpnLHHZ+za9dR/vxzH3Fxif4Oy5hs5U2iSHbHtO4IvKmqfYByvg3LmNwtISGJ1177hTp1xjNz5iaKFQtm3Lib+PXXBwkJ8aYLNWPyDq+GQhWRu4BuwG3uPLu3zxRYJ04k0KjR+6xZsxeAzp3rMnp0ay65JNzPkRnjG94kigeBXjjdjG8VkcrAp74Ny5jcKyysMFFRl3LiRALvvNOe1q2r+jskY3zKm6FQ/xSRx4FqIlIL2KKqI3wfmjG5g6ry4YerqFo1guuuqwDAmDFtCAoKsAfnTIHgzQh31wMfAbtwnqG4WES6qepiXwdnjL+tX7+PRx+dxU8//U3t2qVZubInQUEBFC8e4u/QjMkx3lQ9jQHaqeo6ABGpjZM4onwZmDH+dPJkAiNG/MzIkYtJSEimTJkwBg26jsKFrW8mU/B4kyiCTicJAFVdLyI27JbJt+bO3cJjj81m69ZDAPznP1fx6qs3EhER6ufIjPEPbxLF7yIyCecqAqAr1imgyaeOHTtFt27T2b//BHXrXsTEie1p0qSCv8Myxq+8SRQ9gceBp3HaKBYBb/syKGNyUlJSMsnJSuHCARQtGsRbb7UlJuYI/fo1onBh68DPmAwThYhcAVQFpqvqyJwJyZics2LFPzzyyExuvbUmQ4c2A+Cee67wc1TG5C7ptsyJyLM43Xd0BeaLSFoj3eUOX7WHN6w/HeO9I0fieeKJOTRs+D4rVuzmo49Wk5CQ5O+wjMmVMrqi6ApEqupxESkDzAYm50xYWbRt9pnXldv5Lw6T66kq06at44kn5rJ79zECAoT+/Rvxwgs3WDWTMenIKFHEq+pxAFXdJyK5/77AJzXzMqbAOno0nk6dpjFnzhYArrmmHBMndqB+/Yv9HJkxuVtGiaKKx1jZAlT1HDtbVTv6NDJjslnRokHExydRvHgwr756Iw8/3IBChazK0pjMZJQo7kg1Pc6XgRjjC4sW/c0llxSlevVSiAiTJ99CSEggZcsW9XdoxuQZGY2Z/X1OBmJMdtq//wRPPz2f//53JS1bVmb+/G6ICBUrlvB3aMbkOdZxvslXkpOVKVNWMmDAfA4ePElQUADXX1+BpCQlMNCqmYw5Hz5toBaRtiKyUUS2iMgzGZS7U0RURLLWf5TdFms8rF27l+bNp9CjxwwOHjxJy5aVWbPmUYYNa05gYO6/F8OY3MrrKwoRCVbV+CyUDwDGA62AGGC5iMzw7DfKLReO8+T3b95uO4XdFmtcsbFxNGr0AceOneKii4owenRr7rnnChuv2phs4E034w2BD4DiQAURqQc85A6JmpGGOGNXbHW3MxW4FViXqtyLwEjgqSzGfobdFltgqSoiQvHiIQwc2IRdu47w8sstKVnSOvAzJrt4cz0+FugAHABQ1VXADV6sVw4StSP/AAAdJ0lEQVTY6TEdQ6qxtkXkSuAyVZ2Z0YZE5GERiRaR6H379nmxa5Pf7dp1hDvv/JyPP16dMm/w4OuZMKGDJQljspk3iaKQqv6dap43fR2kdc2f8tPffYBvDPBkZhtS1XdVNUpVo8qUKePFrk1+lZiYzFtvLaVWrfF8+eV6hg37kaSkZACrZjLGR7xpo9jpVj+p2+7QB9jkxXoxwGUe0+WBfzymw4G6wI/uB/xiYIaI3KKq0d4EbwqW5ct30bPnLH7/fTcAt91Wi7Fj2xIQYA3VxviSN4niUZzqpwrAHmCBOy8zy4HqIlIZZxjVzsA9pxeqaixQ+vS0iPwIPGVJwqR2/PgpBg5cwDvvLEcVKlQozttv38Qtt9T0d2jGFAiZJgpV3YvzJZ8lqpooIr2BeUAAMFlV14rIcCBaVWdkOVpTIAUGFmLBgq0UKiT079+YYcOaUaSIDbJoTE7x5q6n9/BoWzhNVR/ObF1VnY3T66znvOfSKds8s+2ZguOvvw5SokQIpUqFERwcyEcf3U5ISCBXXFHW36EZU+B4U7m7APje/VsMXAR4/TyFMVkRH5/ISy8tom7dCQwcuCBl/tVXl7MkYYyfeFP19JnntIh8BMz3WUSmwPrxx+08+ugsNmzYDzh3OCUlJVtjtTF+dj59PVUGKmZ3IKbg2rv3OAMGzOfDD1cBULNmKSZMaM8NN1T2c2TGGPCujeIQZ9ooCgEHgXT7bTImK/bvP0Ht2uM5ePAkwcEBDB58PU8/3YTgYOuv0pjcIsNPozgPONTDub0VIFlVrb8Mk21Klw7j1ltrEhNzhHfeaU+1ahH+DskYk0qGiUJVVUSmq2qDnArI5G/Hj59i+PCfaN++Bk2bOjWY77zTnuDgAHuy2phcyptWwmUicpXPIzH53rffbqROnXcYOfJXevWaRXKyc3EaEhJoScKYXCzdKwoRCVTVROA64D8i8hdwHKcPJ1VVSx7GKzt3xvLEE3OZPn0DAFdeeTGTJnWw8aqNySMyqnpaBlwF3JZDsZh8JjExmbFjf+O55xZy/HgCRYsG8dJLN/DYYw1tICFj8pCMEoUAqOpfORSLyWeOHInnlVd+4fjxBO64ozZvvtmW8uWL+TssY0wWZZQoyohI//QWqupoH8Rj8rjDh+MIDQ0kODiQiIhQJk3qQHBwAO3b1/B3aMaY85TR9X8AUBSnO/C0/oxJoap88skaatYcx8iRi1Pmd+xY25KEMXlcRlcUu1V1eI5FYvKsTZsO0KvXLL7/fhsAixbtSBmi1BiT92XaRmFMeuLiEnnttV94+eVfOHUqiYiIUF5/vRX331/fkoQx+UhGiaJljkVh8px//z1G06b/ZfPmgwDcf399Xn+9FaVLh/k5MmNMdks3UajqwZwMxOQtZcsW4bLLihMYWIgJE9rTrFklf4dkjPER63nNeCU5WXnvvRXccENlatQohYjwyScdKVkylKCgAH+HZ4zxIXvqyWRq1ap/adJkMj17zqJXr1mc7heybNmiliSMKQDsisKk69ixUzz//I+8+eZSkpKUSy8Np2fPKH+HZYzJYZYoTJq+/noDffrMISbmCIUKCX36NOSll1pQrFiwv0MzxuQwSxTmHLt2HaFz52nExyfRoMElTJzYgaioS/0dljHGTyxRGAASEpIIDCyEiFCuXDFGjGhBUFAAvXpdbWNWG1PA5c1vgK/awxv2QFd2+fXXnTRo8C4ff7w6Zd6TT15Lnz7XWJIwxuTRRLFt9pnXldv5L4487uDBkzzyyLc0aTKZNWv28s470dhIt8aY1PJ21dOT9qV2PlSVjz9ezZNPfse+fScoXLgQTz/dhMGDr7euN4wx58jbicJk2Z49x+jS5UsWLtwOQLNmFZkwoT21a5fxb2DGmFzLEkUBU6JECLt3H6N06TBGjWpF9+717CrCGJMhSxQFwPz5f3HVVZdQqlQYwcGBfPHFXVxySVFKlbIO/IwxmcubjdnGK7t3H6VLly9p3fpjBg5ckDK/bt2LLEkYY7xmVxT5UFJSMpMmrWDQoO85ciSe0NBAatYsZYMJGWPOiyWKfOb333fTs+dMli//B4D27aszblw7KlUq4efIjDF5lSWKfGT79sM0bPgeSUlKuXLhjB17E7ffXsuuIowxF8SniUJE2gJvAQHA+6r6aqrl/YGHgERgH/Cgqv7ty5jys0qVSvDAA/UJDw/mhReaEx5uHfgZYy6czxqzRSQAGA/cBNQBuohInVTF/gCiVDUSmAaM9FU8+dH27Ye5+eZP+emn7Snz3n33ZkaPbmNJwhiTbXx5RdEQ2KKqWwFEZCpwK7DudAFVXehRfilwrw/jyTcSEpIYPXoJL7zwEydPJrJ//wmWLOkBYNVMxphs58tEUQ7Y6TEdA1yTQfkewJy0FojIw8DDABUqVMiu+PKkX37ZQc+eM1m7dh8AnTvXZfTo1n6OyhiTn/kyUaT10zbNzplE5F4gCmiW1nJVfRd4FyAqKkphR3bFmGccOnSSAQPm88EHfwBQtWpJ3nmnPa1bV/VzZMaY/M6XiSIGuMxjujzwT+pCInIjMBhopqrxPownT0tOVr75ZiOFCxfimWeuY9Cg6wgNLezvsIwxBYAvE8VyoLqIVAZ2AZ2BezwLiMiVwCSgraru9WEsedKGDfupXLkEwcGBlCoVxv/+15EKFYpTq1Zpf4dmjClAfHbXk6omAr2BecB64HNVXSsiw0XkFrfY60BR4AsRWSkiM3wVT15y4kQCgwd/T2TkBEaOXJwyv3XrqpYkjDE5zqfPUajqbGB2qnnPeby+0Zf7z4vmzt1Cr16z2LbtMAD795/wc0TGmILOnszOJf755yh9+87liy+cu4evuOIiJk7swLXXXpbJmsYY41uWKHKBTZsOEBX1LkePniIsrDDPP9+Mvn0bUbhwgL9DM8YYSxS5QfXqEVx9dTmKFCnM22/fRMWK1oGfMSb3sEThB0eOxPPccwvp1etqatQohYgwY0ZnihQJ8ndoxhhzDksUOUhVmTZtHU88MZfdu4+xYcN+5s51ei2xJGGMya0sUeSQrVsP0bv3bObM2QJAo0blee01u+nLGJP7WaLwsVOnkhg16ldefHERcXGJlCgRwquvtuQ//2lAoULWgZ8xJvezROFjO3fGMnz4T8THJ9G16xW88UZrypYt6u+wjDHGa5YofODQoZOUKBGCiFC1agRvvdWWatUiaNmyir9DM8aYLPNZFx4FUXKyMnnyH1Sr9jYff7w6Zf4jj0RZkjDG5FmWKLLJ2rV7ad58Cj16zODgwZMpjdbGGJPXWdXTBTpxIoEXX/yJUaOWkJiYzEUXFWHMmDZ06VLX36EZY0y2sERxATZtOkCbNh+zffthRKBnzwa8/HJLSpYM9XdoxhiTbSxRXICKFYsTEhJIvXplmTixA40alfd3SCYXSUhIICYmhri4OH+HYgqQkJAQypcvT+HC2TewmSWKLEhMTGbixGi6dKlLqVJhBAcHMnduV8qVK0ZgoDX3mLPFxMQQHh5OpUqVELFnZozvqSoHDhwgJiaGypUrZ9t27dvNS8uW7aJhw/fo02cOAwcuSJlfsWIJSxImTXFxcZQqVcqShMkxIkKpUqWy/SrWrigyERsbx+DBP/DOO8tRhQoVinPrrTX9HZbJIyxJmJzmi/ecJYp0qCqffbaWfv3m8e+/xwgMLET//o147rlm1oGfMaZAsTqTdKxatYcuXb7k33+Pce21l/H77w/z2mutLEmYPCUgIID69etTt25dbr75Zg4fPpyybO3atbRo0YIaNWpQvXp1XnzxRVQ1ZfmcOXOIioqidu3a1KpVi6eeesofh5ChP/74g4ceesjfYWTolVdeoVq1atSsWZN58+alWeaHH37gqquuom7dutx3330kJiYC8L///Y/IyEgiIyO59tprWbVqFQCnTp2iadOmKeV8TlXz1F+DqsVUR+H8ZbPExKSzpvv1m6vvvbdCk5KSs31fJv9bt26dv0PQIkWKpLzu3r27vvTSS6qqeuLECa1SpYrOmzdPVVWPHz+ubdu21XHjxqmq6po1a7RKlSq6fv16VVVNSEjQ8ePHZ2tsCQkJF7yNO++8U1euXJmj+8yKtWvXamRkpMbFxenWrVu1SpUqmpiYeFaZpKQkLV++vG7cuFFVVYcOHarvv/++qqouXrxYDx48qKqqs2fP1oYNG6as9/zzz+vHH3+c5n7Teu8B0Xqe37t5r+op/ojzb+V22brZhQu30avXbCZN6kDTphUBGD26TbbuwxRgb/ioreJJzbyMq3Hjxqxe7XQt88knn9CkSRNat24NQFhYGOPGjaN58+Y89thjjBw5ksGDB1OrVi0AAgMD6dWr1znbPHbsGH369CE6OhoRYdiwYdxxxx0ULVqUY8eOATBt2jRmzpzJlClTuP/++4mIiOCPP/6gfv36TJ8+nZUrV1KihDOqY7Vq1Vi8eDGFChWiZ8+e7NixA4A333yTJk2anLXvo0ePsnr1aurVqwfAsmXL6Nu3LydPniQ0NJT//ve/1KxZkylTpjBr1izi4uI4fvw4P/zwA6+//jqff/458fHx3H777bzwwgsA3HbbbezcuZO4uDieeOIJHn74Ya/Pb1q++eYbOnfuTHBwMJUrV6ZatWosW7aMxo0bp5Q5cOAAwcHB1KhRA4BWrVrxyiuv0KNHD6699tqUco0aNSImJiZl+rbbbmPQoEF07dr1gmL0Rt5LFKd1nJUtm9m79zgDBsznww+dS7rRo5ekJApj8oukpCS+//57evToATjVTg0aNDirTNWqVTl27BhHjhzhzz//5Mknn8x0uy+++CLFixdnzZo1ABw6dCjTdTZt2sSCBQsICAggOTmZ6dOn88ADD/Dbb79RqVIlypYtyz333EO/fv247rrr2LFjB23atGH9+vVnbSc6Opq6dc/0gFCrVi0WLVpEYGAgCxYs4Nlnn+XLL78EYMmSJaxevZqIiAi+++47Nm/ezLJly1BVbrnlFhYtWkTTpk2ZPHkyERERnDx5kquvvpo77riDUqVKnbXffv36sXDhwnOOq3PnzjzzzDNnzdu1axeNGjVKmS5fvjy7du06q0zp0qVJSEggOjqaqKgopk2bxs6dO8/Z/gcffMBNN92UMl23bl2WL1+e2enOFnk3UVyg5GTlgw9+Z+DABRw6FEdwcABDhjRlwIBrM1/ZmKzKwi//7HTy5Enq16/P9u3badCgAa1atQKcKuf07o7Jyl0zCxYsYOrUqSnTJUuWzHSdu+66i4CAAAA6derE8OHDeeCBB5g6dSqdOnVK2e66detS1jly5AhHjx4lPDw8Zd7u3bspU6ZMynRsbCz33XcfmzdvRkRISEhIWdaqVSsiIiIA+O677/juu++48sorAeeqaPPmzTRt2pSxY8cyffp0AHbu3MnmzZvPSRRjxozx7uTAWW0+p6U+vyLC1KlT6devH/Hx8bRu3ZrAwLO/mhcuXMgHH3zAL7/8kjIvICCAoKCgc86LLxTIRLFt2yHuvXc6v/7qZO3Wrasyfnw7qlWL8HNkxmSv0NBQVq5cSWxsLB06dGD8+PE8/vjjXH755SxatOisslu3bqVo0aKEh4dz+eWXs2LFipRqnfSkl3A856W+p79IkSIprxs3bsyWLVvYt28fX3/9NUOGDAEgOTmZJUuWEBqafnc4oaGhZ2176NCh3HDDDUyfPp3t27fTvHnzNPepqgwaNIhHHnnkrO39+OOPLFiwgCVLlhAWFkbz5s3TfB4hK1cU5cuXP+vqICYmhksvvfScdRs3bszPP/8MOIls06ZNKctWr17NQw89xJw5c85JWvHx8YSEhJyzvexWIO96KlYsmE2bDnDxxUWZOvUO5s7taknC5GvFixdn7NixjBo1ioSEBLp27covv/zCggXOw6MnT57k8ccf5+mnnwZgwIABvPzyyylfWMnJyYwePfqc7bZu3Zpx48alTJ+ueipbtizr169PqVpKj4hw++23079/f2rXrp3yRZh6uytXrjxn3dq1a7Nly5lemmNjYylXrhwAU6ZMSXefbdq0YfLkySltKLt27WLv3r3ExsZSsmRJwsLC2LBhA0uXLk1z/TFjxrBy5cpz/lInCYBbbrmFqVOnEh8fz7Zt29i8eTMNGzY8p9zevXsB54v/tddeo2fPngDs2LGDjh078tFHH6W0YZx24MABypQpk61ddaSnwCSKefO2EB/v3EpWqlQYM2Z0ZsOGx+jUqa49FGUKhCuvvJJ69eoxdepUQkND+eabb3jppZeoWbMmV1xxBVdffTW9e/cGIDIykjfffJMuXbpQu3Zt6taty+7du8/Z5pAhQzh06BB169alXr16Kb+0X331VTp06ECLFi245JJLMoyrU6dOfPzxxynVTgBjx44lOjqayMhI6tSpw8SJE89Zr1atWsTGxnL06FEAnn76aQYNGkSTJk1ISkpKd3+tW7fmnnvuoXHjxlxxxRXceeedHD16lLZt25KYmEhkZCRDhw49q23hfF1++eXcfffd1KlTh7Zt2zJ+/PiUard27drxzz//APD6669Tu3ZtIiMjufnmm2nRogUAw4cP58CBA/Tq1Yv69esTFRWVsu2FCxfSrl323tSTHkmrDi03i7pMNLovXtf57twZy+OPz+Xrrzfw4os3MGRIU98GaIxr/fr11K5d299h5GtjxowhPDw81z9L4QsdO3bklVdeoWbNc3uKSOu9JyIrVDXqnMJeyLdXFImJyYwevYTatcfz9dcbKFo0iIgI6/7bmPzk0UcfJTg42N9h5LhTp05x2223pZkkfCFfNmYvXRpDz54zWbVqDwB33FGbt95qS7lyxfwcmTEmO4WEhNCtWzd/h5HjgoKC6N69e47tL98lit9+i+Haaz9AFSpVKsG4cTfRvn2NzFc0xgcyug3VGF/wRXNCvksUDRuWo02balx55cUMGdKUsDDf3xFgTFpCQkI4cOCAdTVucoy641Fk9y2zeT5RbN58gH795jF6dBtq1HA+kLNm3UOhQvbBNP5Vvnx5YmJi2Ldvn79DMQXI6RHuslOeTRTx8Ym8+uovvPLKL8THJxESEsi0aXcDWJIwuULhwoWzdZQxY/zFp3c9iUhbEdkoIltE5JynUUQkWEQ+c5f/JiKVvNnu95srExk5keef/4n4+CQeeKA+Eyd2yO7wjTHG4MPnKEQkANgEtAJigOVAF1Vd51GmFxCpqj1FpDNwu6p2SnODrlJFSurBE30BqF27NBMndrBO/IwxJhO59TmKhsAWVd2qqqeAqcCtqcrcCvyf+3oa0FIyafU7dCKUkMAEXn65BStX9rQkYYwxPubLK4o7gbaq+pA73Q24RlV7e5T50y0T407/5ZbZn2pbDwOnO4avC/zpk6DzntLA/kxLFQx2Ls6wc3GGnYszaqrqeXUz68vG7LSuDFJnJW/KoKrvAu8CiEj0+V4+5Td2Ls6wc3GGnYsz7FycISLR57uuL6ueYoDLPKbLA/+kV0ZEAoHiwEEfxmSMMSaLfJkolgPVRaSyiAQBnYEZqcrMAO5zX98J/KB5rZdCY4zJ53xW9aSqiSLSG5gHBACTVXWtiAzHGeR7BvAB8JGIbMG5kujsxabf9VXMeZCdizPsXJxh5+IMOxdnnPe5yHPdjBtjjMlZ+babcWOMMdnDEoUxxpgM5dpE4avuP/IiL85FfxFZJyKrReR7Ecm3TyFmdi48yt0pIioi+fbWSG/OhYjc7b431orIJzkdY07x4jNSQUQWisgf7uckZ8YQzWEiMllE9rrPqKW1XERkrHueVovIVV5tWFVz3R9O4/dfQBUgCFgF1ElVphcw0X3dGfjM33H78VzcAIS5rx8tyOfCLRcOLAKWAlH+jtuP74vqwB9ASXf6In/H7cdz8S7wqPu6DrDd33H76Fw0Ba4C/kxneTtgDs4zbI2A37zZbm69ovBJ9x95VKbnQlUXquoJd3IpzjMr+ZE37wuAF4GRQFxOBpfDvDkX/wHGq+ohAFXdm8Mx5hRvzoUCp4e4LM65z3TlC6q6iIyfRbsV+FAdS4ESInJJZtvNrYmiHLDTYzrGnZdmGVVNBGKBUjkSXc7y5lx46oHziyE/yvRciMiVwGWqOjMnA/MDb94XNYAaIrJYRJaKSNsciy5neXMungfuFZEYYDbQJ2dCy3Wy+n0C5N7xKLKt+498wOvjFJF7gSigmU8j8p8Mz4WIFALGAPfnVEB+5M37IhCn+qk5zlXmzyJSV1UP+zi2nObNuegCTFHVN0SkMc7zW3VVNdn34eUq5/W9mVuvKKz7jzO8OReIyI3AYOAWVY3PodhyWmbnIhyn08gfRWQ7Th3sjHzaoO3tZ+QbVU1Q1W3ARpzEkd94cy56AJ8DqOoSIASnw8CCxqvvk9Rya6Kw7j/OyPRcuNUtk3CSRH6th4ZMzoWqxqpqaVWtpKqVcNprblHV8+4MLRfz5jPyNc6NDohIaZyqqK05GmXO8OZc7ABaAohIbZxEURDHqJ0BdHfvfmoExKrq7sxWypVVT+q77j/yHC/PxetAUeALtz1/h6re4regfcTLc1EgeHku5gGtRWQdkAQMUNUD/ovaN7w8F08C74lIP5yqlvvz4w9LEfkUp6qxtNseMwwoDKCqE3HaZ9oBW4ATwANebTcfnitjjDHZKLdWPRljjMklLFEYY4zJkCUKY4wxGbJEYYwxJkOWKIwxxmTIEoXJdUQkSURWevxVyqBspfR6ysziPn90ex9d5XZ5UfM8ttFTRLq7r+8XkUs9lr0vInWyOc7lIlLfi3X6ikjYhe7bFFyWKExudFJV63v8bc+h/XZV1Xo4nU2+ntWVVXWiqn7oTt4PXOqx7CFVXZctUZ6J8x28i7MvYInCnDdLFCZPcK8cfhaR392/a9Moc7mILHOvQlaLSHV3/r0e8yeJSEAmu1sEVHPXbemOYbDG7es/2J3/qpwZA2SUO+95EXlKRO7E6XPrf+4+Q90rgSgReVRERnrEfL+IvH2ecS7Bo0M3EZkgItHijD3xgjvvcZyEtVBEFrrzWovIEvc8fiEiRTPZjyngLFGY3CjUo9ppujtvL9BKVa8COgFj01ivJ/CWqtbH+aKOcbtr6AQ0cecnAV0z2f/NwBoRCQGmAJ1U9QqcngweFZEI4HbgclWNBF7yXFlVpwHROL/866vqSY/F04COHtOdgM/OM862ON10nDZYVaOASKCZiESq6licvnxuUNUb3K48hgA3uucyGuifyX5MAZcru/AwBd5J98vSU2FgnFsnn4TTb1FqS4DBIlIe+EpVN4tIS6ABsNzt3iQUJ+mk5X8ichLYjtMNdU1gm6pucpf/H/AYMA5nrIv3RWQW4HWX5qq6T0S2uv3sbHb3sdjdblbiLILTXYXnCGV3i8jDOJ/rS3AG6Fmdat1G7vzF7n6CcM6bMemyRGHyin7AHqAezpXwOYMSqeonIvIb0B6YJyIP4XSr/H+qOsiLfXT17EBQRNIc38TtW6ghTidznYHeQIssHMtnwN3ABmC6qqo439pex4kziturwHigo4hUBp4CrlbVQyIyBafju9QEmK+qXbIQryngrOrJ5BXFgd3u+AHdcH5Nn0VEqgBb3eqWGThVMN8Dd4rIRW6ZCPF+TPENQCURqeZOdwN+cuv0i6vqbJyG4rTuPDqK0+15Wr4CbsMZI+Ezd16W4lTVBJwqpEZutVUx4DgQKyJlgZvSiWUp0OT0MYlImIikdXVmTApLFCaveAe4T0SW4lQ7HU+jTCfgTxFZCdTCGfJxHc4X6ncishqYj1MtkylVjcPpXfMLEVkDJAMTcb50Z7rb+wnnaie1KcDE043ZqbZ7CFgHVFTVZe68LMfptn28ATylqqtwxsdeC0zGqc467V1gjogsVNV9OHdkferuZynOuTImXdZ7rDHGmAzZFYUxxpgMWaIwxhiTIUsUxhhjMmSJwhhjTIYsURhjjMmQJQpjjDEZskRhjDEmQ/8PfAiKKnns6ocAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "\n",
    "\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bastante similar al modelo de una capa, esto me hace pensar que la opción \"class_weight='balanced'\" funciona bastante bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
