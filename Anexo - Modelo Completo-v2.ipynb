{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como anexo a la práctica, voy a intentar construir el modelo completo de red neuronal. Usando variables sintéticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x2115006b9b0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as randint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from keras.optimizers import Nadam\n",
    "from keras.layers import Dropout\n",
    "#from imblearn.under_sampling import NearMiss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "datos_path = \"./\"\n",
    "clientes_file = \"Clientes_train.csv\"\n",
    "zonas_file = \"Zonas.csv\"\n",
    "\n",
    "clientes = pd.read_csv(os.path.join(datos_path, clientes_file), sep='\\t')\n",
    "zonas = pd.read_csv(os.path.join(datos_path, zonas_file), sep='\\t')\n",
    "\n",
    "datos = pd.merge(clientes, zonas, on=\"ID_Zona\", how=\"inner\")\n",
    "\n",
    "\n",
    "\n",
    "clientes_file_2 = \"Clientes_test.csv\"\n",
    "\n",
    "\n",
    "clientes2 = pd.read_csv(os.path.join(datos_path, clientes_file_2), sep='\\t')\n",
    "zonas = pd.read_csv(os.path.join(datos_path, zonas_file), sep='\\t')\n",
    "\n",
    "datos2 = pd.merge(clientes2, zonas, on=\"ID_Zona\", how=\"inner\")\n",
    "\n",
    "#targets2 = datos2[\"Seguro_Vivienda\"]\n",
    "#variables2 = datos2.drop([\"Seguro_Vivienda\"], axis=1, inplace=False)\n",
    "\n",
    "\n",
    "\n",
    "X_test_2 = datos2._get_numeric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Cliente</th>\n",
       "      <th>Fecha_Nacimiento</th>\n",
       "      <th>Fecha_Alta</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>ID_Zona</th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>...</th>\n",
       "      <th>Poblacion_Funcionario</th>\n",
       "      <th>Poblacion_Trabajador_Cualificado</th>\n",
       "      <th>Poblacion_Trabajador_No_Cualificado</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3088</td>\n",
       "      <td>29/03/1968</td>\n",
       "      <td>27/03/1989</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>617.55</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>21.01</td>\n",
       "      <td>27.13</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3441</td>\n",
       "      <td>01/05/1962</td>\n",
       "      <td>26/12/1984</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>21.01</td>\n",
       "      <td>27.13</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0412</td>\n",
       "      <td>19/01/1967</td>\n",
       "      <td>29/04/1987</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>21.01</td>\n",
       "      <td>27.13</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3565</td>\n",
       "      <td>20/04/1948</td>\n",
       "      <td>06/09/1969</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3315.54</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>21.01</td>\n",
       "      <td>27.13</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0315</td>\n",
       "      <td>28/07/1979</td>\n",
       "      <td>18/06/2001</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2561.45</td>\n",
       "      <td>...</td>\n",
       "      <td>28.17</td>\n",
       "      <td>21.01</td>\n",
       "      <td>27.13</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_Cliente Fecha_Nacimiento  Fecha_Alta    Sexo ID_Zona  Productos_Vida  \\\n",
       "0      C3088       29/03/1968  27/03/1989   Mujer   Z1143               0   \n",
       "1      C3441       01/05/1962  26/12/1984  Hombre   Z1143               0   \n",
       "2      C0412       19/01/1967  29/04/1987  Hombre   Z1143               0   \n",
       "3      C3565       20/04/1948  06/09/1969  Hombre   Z1143               0   \n",
       "4      C0315       28/07/1979  18/06/2001   Mujer   Z1143               0   \n",
       "\n",
       "   Productos_Vehiculos  Productos_Otros  Gasto_Vida  Gasto_Vehiculos  \\\n",
       "0                    1                0         0.0           617.55   \n",
       "1                    0                0         0.0             0.00   \n",
       "2                    0                1         0.0             0.00   \n",
       "3                    1                0         0.0          3315.54   \n",
       "4                    1                2         0.0          2561.45   \n",
       "\n",
       "           ...           Poblacion_Funcionario  \\\n",
       "0          ...                           28.17   \n",
       "1          ...                           28.17   \n",
       "2          ...                           28.17   \n",
       "3          ...                           28.17   \n",
       "4          ...                           28.17   \n",
       "\n",
       "   Poblacion_Trabajador_Cualificado  Poblacion_Trabajador_No_Cualificado  \\\n",
       "0                             21.01                                27.13   \n",
       "1                             21.01                                27.13   \n",
       "2                             21.01                                27.13   \n",
       "3                             21.01                                27.13   \n",
       "4                             21.01                                27.13   \n",
       "\n",
       "   Vivienda_Propiedad  Vivienda_Alquiler  Medico_Seguro_Privado  \\\n",
       "0               71.34              28.66                  32.77   \n",
       "1               71.34              28.66                  32.77   \n",
       "2               71.34              28.66                  32.77   \n",
       "3               71.34              28.66                  32.77   \n",
       "4               71.34              28.66                  32.77   \n",
       "\n",
       "   Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "0                    67.23                   2.23   \n",
       "1                    67.23                   2.23   \n",
       "2                    67.23                   2.23   \n",
       "3                    67.23                   2.23   \n",
       "4                    67.23                   2.23   \n",
       "\n",
       "   Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  \n",
       "0                           1.47                  96.3  \n",
       "1                           1.47                  96.3  \n",
       "2                           1.47                  96.3  \n",
       "3                           1.47                  96.3  \n",
       "4                           1.47                  96.3  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Cliente</th>\n",
       "      <th>Fecha_Nacimiento</th>\n",
       "      <th>Fecha_Alta</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>ID_Zona</th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>...</th>\n",
       "      <th>Poblacion_Funcionario</th>\n",
       "      <th>Poblacion_Trabajador_Cualificado</th>\n",
       "      <th>Poblacion_Trabajador_No_Cualificado</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C2172</td>\n",
       "      <td>05/10/1981</td>\n",
       "      <td>04/02/2005</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0403</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>735.14</td>\n",
       "      <td>2535.49</td>\n",
       "      <td>...</td>\n",
       "      <td>41.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.04</td>\n",
       "      <td>66.96</td>\n",
       "      <td>78.78</td>\n",
       "      <td>21.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3005</td>\n",
       "      <td>05/04/1974</td>\n",
       "      <td>26/11/1995</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>41.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.04</td>\n",
       "      <td>66.96</td>\n",
       "      <td>78.78</td>\n",
       "      <td>21.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1627</td>\n",
       "      <td>21/09/1983</td>\n",
       "      <td>27/12/2004</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3195.94</td>\n",
       "      <td>...</td>\n",
       "      <td>46.35</td>\n",
       "      <td>16.59</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>31.24</td>\n",
       "      <td>68.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3120</td>\n",
       "      <td>16/02/1986</td>\n",
       "      <td>24/09/2007</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0700</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4610.12</td>\n",
       "      <td>...</td>\n",
       "      <td>46.35</td>\n",
       "      <td>16.59</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>31.24</td>\n",
       "      <td>68.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0649</td>\n",
       "      <td>24/01/1945</td>\n",
       "      <td>02/12/1967</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>21.16</td>\n",
       "      <td>19.46</td>\n",
       "      <td>41.63</td>\n",
       "      <td>17.29</td>\n",
       "      <td>82.71</td>\n",
       "      <td>17.86</td>\n",
       "      <td>82.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>91.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_Cliente Fecha_Nacimiento  Fecha_Alta    Sexo ID_Zona  Productos_Vida  \\\n",
       "0      C2172       05/10/1981  04/02/2005  Hombre   Z0403               1   \n",
       "1      C3005       05/04/1974  26/11/1995  Hombre   Z0403               0   \n",
       "2      C1627       21/09/1983  27/12/2004   Mujer   Z0700               0   \n",
       "3      C3120       16/02/1986  24/09/2007   Mujer   Z0700               0   \n",
       "4      C0649       24/01/1945  02/12/1967  Hombre   Z1023               0   \n",
       "\n",
       "   Productos_Vehiculos  Productos_Otros  Gasto_Vida  Gasto_Vehiculos  \\\n",
       "0                    2                2      735.14          2535.49   \n",
       "1                    0                1        0.00             0.00   \n",
       "2                    1                1        0.00          3195.94   \n",
       "3                    3                0        0.00          4610.12   \n",
       "4                    0                0        0.00             0.00   \n",
       "\n",
       "           ...           Poblacion_Funcionario  \\\n",
       "0          ...                           41.42   \n",
       "1          ...                           41.42   \n",
       "2          ...                           46.35   \n",
       "3          ...                           46.35   \n",
       "4          ...                           21.16   \n",
       "\n",
       "   Poblacion_Trabajador_Cualificado  Poblacion_Trabajador_No_Cualificado  \\\n",
       "0                              0.00                                 0.00   \n",
       "1                              0.00                                 0.00   \n",
       "2                             16.59                                 1.30   \n",
       "3                             16.59                                 1.30   \n",
       "4                             19.46                                41.63   \n",
       "\n",
       "   Vivienda_Propiedad  Vivienda_Alquiler  Medico_Seguro_Privado  \\\n",
       "0               33.04              66.96                  78.78   \n",
       "1               33.04              66.96                  78.78   \n",
       "2                0.00             100.00                  31.24   \n",
       "3                0.00             100.00                  31.24   \n",
       "4               17.29              82.71                  17.86   \n",
       "\n",
       "   Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "0                    21.22                    0.0   \n",
       "1                    21.22                    0.0   \n",
       "2                    68.76                    0.0   \n",
       "3                    68.76                    0.0   \n",
       "4                    82.14                    0.0   \n",
       "\n",
       "   Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  \n",
       "0                            0.0                100.00  \n",
       "1                            0.0                100.00  \n",
       "2                            0.0                100.00  \n",
       "3                            0.0                100.00  \n",
       "4                            8.9                 91.11  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creamos variables sintéticas para ámbas tablas: Edad (años) y Antiguedad (meses):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datos['Edad'] = ((pd.to_datetime(\"06/10/2018\", format='%d/%m/%Y') - pd.to_datetime(datos.Fecha_Nacimiento, format='%d/%m/%Y')) \n",
    "          /np.timedelta64(1, 'Y')).astype(int)\n",
    "\n",
    "datos['Antiguedad'] = ((pd.to_datetime(\"06/10/2018\", format='%d/%m/%Y') - pd.to_datetime(datos.Fecha_Alta, format='%d/%m/%Y')) \n",
    "          /np.timedelta64(1, 'M')).astype(int)\n",
    "\n",
    "datos['Sexo_2'] = 0\n",
    "datos['Sexo_2'][datos.Sexo=='Hombre'] =1\n",
    "\n",
    "\n",
    "datos2['Edad'] = ((pd.to_datetime(\"06/10/2018\", format='%d/%m/%Y') - pd.to_datetime(datos.Fecha_Nacimiento, format='%d/%m/%Y')) \n",
    "          /np.timedelta64(1, 'Y')).astype(int)\n",
    "\n",
    "datos2['Antiguedad'] = ((pd.to_datetime(\"06/10/2018\", format='%d/%m/%Y') - pd.to_datetime(datos.Fecha_Alta, format='%d/%m/%Y')) \n",
    "          /np.timedelta64(1, 'M')).astype(int)\n",
    "\n",
    "datos2['Sexo_2'] = 0\n",
    "datos2['Sexo_2'][datos2.Sexo=='Hombre'] =1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Cliente</th>\n",
       "      <th>Fecha_Nacimiento</th>\n",
       "      <th>Fecha_Alta</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>ID_Zona</th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>...</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Antiguedad</th>\n",
       "      <th>Sexo_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3088</td>\n",
       "      <td>29/03/1968</td>\n",
       "      <td>27/03/1989</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>617.55</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>50</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3441</td>\n",
       "      <td>01/05/1962</td>\n",
       "      <td>26/12/1984</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>56</td>\n",
       "      <td>405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0412</td>\n",
       "      <td>19/01/1967</td>\n",
       "      <td>29/04/1987</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>51</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3565</td>\n",
       "      <td>20/04/1948</td>\n",
       "      <td>06/09/1969</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3315.54</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>70</td>\n",
       "      <td>588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0315</td>\n",
       "      <td>28/07/1979</td>\n",
       "      <td>18/06/2001</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2561.45</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>39</td>\n",
       "      <td>207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_Cliente Fecha_Nacimiento  Fecha_Alta    Sexo ID_Zona  Productos_Vida  \\\n",
       "0      C3088       29/03/1968  27/03/1989   Mujer   Z1143               0   \n",
       "1      C3441       01/05/1962  26/12/1984  Hombre   Z1143               0   \n",
       "2      C0412       19/01/1967  29/04/1987  Hombre   Z1143               0   \n",
       "3      C3565       20/04/1948  06/09/1969  Hombre   Z1143               0   \n",
       "4      C0315       28/07/1979  18/06/2001   Mujer   Z1143               0   \n",
       "\n",
       "   Productos_Vehiculos  Productos_Otros  Gasto_Vida  Gasto_Vehiculos   ...    \\\n",
       "0                    1                0         0.0           617.55   ...     \n",
       "1                    0                0         0.0             0.00   ...     \n",
       "2                    0                1         0.0             0.00   ...     \n",
       "3                    1                0         0.0          3315.54   ...     \n",
       "4                    1                2         0.0          2561.45   ...     \n",
       "\n",
       "   Vivienda_Propiedad  Vivienda_Alquiler  Medico_Seguro_Privado  \\\n",
       "0               71.34              28.66                  32.77   \n",
       "1               71.34              28.66                  32.77   \n",
       "2               71.34              28.66                  32.77   \n",
       "3               71.34              28.66                  32.77   \n",
       "4               71.34              28.66                  32.77   \n",
       "\n",
       "   Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "0                    67.23                   2.23   \n",
       "1                    67.23                   2.23   \n",
       "2                    67.23                   2.23   \n",
       "3                    67.23                   2.23   \n",
       "4                    67.23                   2.23   \n",
       "\n",
       "   Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  Edad  Antiguedad  \\\n",
       "0                           1.47                  96.3    50         354   \n",
       "1                           1.47                  96.3    56         405   \n",
       "2                           1.47                  96.3    51         377   \n",
       "3                           1.47                  96.3    70         588   \n",
       "4                           1.47                  96.3    39         207   \n",
       "\n",
       "   Sexo_2  \n",
       "0       0  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Cliente</th>\n",
       "      <th>Fecha_Nacimiento</th>\n",
       "      <th>Fecha_Alta</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>ID_Zona</th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>...</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Antiguedad</th>\n",
       "      <th>Sexo_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C2172</td>\n",
       "      <td>05/10/1981</td>\n",
       "      <td>04/02/2005</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0403</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>735.14</td>\n",
       "      <td>2535.49</td>\n",
       "      <td>...</td>\n",
       "      <td>33.04</td>\n",
       "      <td>66.96</td>\n",
       "      <td>78.78</td>\n",
       "      <td>21.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50</td>\n",
       "      <td>354</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3005</td>\n",
       "      <td>05/04/1974</td>\n",
       "      <td>26/11/1995</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>33.04</td>\n",
       "      <td>66.96</td>\n",
       "      <td>78.78</td>\n",
       "      <td>21.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>56</td>\n",
       "      <td>405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1627</td>\n",
       "      <td>21/09/1983</td>\n",
       "      <td>27/12/2004</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3195.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>31.24</td>\n",
       "      <td>68.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>51</td>\n",
       "      <td>377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3120</td>\n",
       "      <td>16/02/1986</td>\n",
       "      <td>24/09/2007</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0700</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4610.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>31.24</td>\n",
       "      <td>68.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>70</td>\n",
       "      <td>588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0649</td>\n",
       "      <td>24/01/1945</td>\n",
       "      <td>02/12/1967</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>17.29</td>\n",
       "      <td>82.71</td>\n",
       "      <td>17.86</td>\n",
       "      <td>82.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>91.11</td>\n",
       "      <td>39</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_Cliente Fecha_Nacimiento  Fecha_Alta    Sexo ID_Zona  Productos_Vida  \\\n",
       "0      C2172       05/10/1981  04/02/2005  Hombre   Z0403               1   \n",
       "1      C3005       05/04/1974  26/11/1995  Hombre   Z0403               0   \n",
       "2      C1627       21/09/1983  27/12/2004   Mujer   Z0700               0   \n",
       "3      C3120       16/02/1986  24/09/2007   Mujer   Z0700               0   \n",
       "4      C0649       24/01/1945  02/12/1967  Hombre   Z1023               0   \n",
       "\n",
       "   Productos_Vehiculos  Productos_Otros  Gasto_Vida  Gasto_Vehiculos   ...    \\\n",
       "0                    2                2      735.14          2535.49   ...     \n",
       "1                    0                1        0.00             0.00   ...     \n",
       "2                    1                1        0.00          3195.94   ...     \n",
       "3                    3                0        0.00          4610.12   ...     \n",
       "4                    0                0        0.00             0.00   ...     \n",
       "\n",
       "   Vivienda_Propiedad  Vivienda_Alquiler  Medico_Seguro_Privado  \\\n",
       "0               33.04              66.96                  78.78   \n",
       "1               33.04              66.96                  78.78   \n",
       "2                0.00             100.00                  31.24   \n",
       "3                0.00             100.00                  31.24   \n",
       "4               17.29              82.71                  17.86   \n",
       "\n",
       "   Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "0                    21.22                    0.0   \n",
       "1                    21.22                    0.0   \n",
       "2                    68.76                    0.0   \n",
       "3                    68.76                    0.0   \n",
       "4                    82.14                    0.0   \n",
       "\n",
       "   Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  Edad  Antiguedad  \\\n",
       "0                            0.0                100.00    50         354   \n",
       "1                            0.0                100.00    56         405   \n",
       "2                            0.0                100.00    51         377   \n",
       "3                            0.0                100.00    70         588   \n",
       "4                            8.9                 91.11    39         207   \n",
       "\n",
       "   Sexo_2  \n",
       "0       1  \n",
       "1       1  \n",
       "2       0  \n",
       "3       0  \n",
       "4       1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos la tabla para entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Cliente</th>\n",
       "      <th>Fecha_Nacimiento</th>\n",
       "      <th>Fecha_Alta</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>ID_Zona</th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>...</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Antiguedad</th>\n",
       "      <th>Sexo_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C1947</td>\n",
       "      <td>10/09/1948</td>\n",
       "      <td>08/09/1973</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>492.21</td>\n",
       "      <td>3890.19</td>\n",
       "      <td>...</td>\n",
       "      <td>92.04</td>\n",
       "      <td>7.96</td>\n",
       "      <td>43.84</td>\n",
       "      <td>56.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.38</td>\n",
       "      <td>96.62</td>\n",
       "      <td>70</td>\n",
       "      <td>540</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>C2877</td>\n",
       "      <td>05/11/1966</td>\n",
       "      <td>19/11/1988</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0789</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6712.59</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.08</td>\n",
       "      <td>79.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>51</td>\n",
       "      <td>358</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>C1031</td>\n",
       "      <td>04/02/1980</td>\n",
       "      <td>20/01/2001</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0789</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6824.90</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.08</td>\n",
       "      <td>79.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>38</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>C1093</td>\n",
       "      <td>29/10/1948</td>\n",
       "      <td>18/03/1970</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0664</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5915.24</td>\n",
       "      <td>...</td>\n",
       "      <td>16.11</td>\n",
       "      <td>83.89</td>\n",
       "      <td>1.91</td>\n",
       "      <td>98.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.01</td>\n",
       "      <td>69</td>\n",
       "      <td>582</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>C3214</td>\n",
       "      <td>03/03/1948</td>\n",
       "      <td>02/09/1970</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1707</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6262.06</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>70.70</td>\n",
       "      <td>29.30</td>\n",
       "      <td>16.02</td>\n",
       "      <td>12.17</td>\n",
       "      <td>71.81</td>\n",
       "      <td>70</td>\n",
       "      <td>577</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>C2780</td>\n",
       "      <td>27/07/1952</td>\n",
       "      <td>13/10/1972</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0738</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6246.52</td>\n",
       "      <td>...</td>\n",
       "      <td>79.05</td>\n",
       "      <td>20.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>66</td>\n",
       "      <td>551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>C0013</td>\n",
       "      <td>29/11/1973</td>\n",
       "      <td>08/08/2000</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0314</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7616.57</td>\n",
       "      <td>...</td>\n",
       "      <td>5.65</td>\n",
       "      <td>94.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.53</td>\n",
       "      <td>93.47</td>\n",
       "      <td>44</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>C2650</td>\n",
       "      <td>17/04/1986</td>\n",
       "      <td>01/12/2006</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3529.81</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>8.78</td>\n",
       "      <td>91.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.59</td>\n",
       "      <td>90.41</td>\n",
       "      <td>32</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>C1412</td>\n",
       "      <td>06/04/1971</td>\n",
       "      <td>05/06/1991</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0522</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8725.95</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.26</td>\n",
       "      <td>22.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.57</td>\n",
       "      <td>78.43</td>\n",
       "      <td>47</td>\n",
       "      <td>328</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>C0799</td>\n",
       "      <td>16/12/1989</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0522</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5042.51</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.26</td>\n",
       "      <td>22.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.57</td>\n",
       "      <td>78.43</td>\n",
       "      <td>28</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>C2081</td>\n",
       "      <td>27/07/1945</td>\n",
       "      <td>07/08/1966</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0987</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>167.85</td>\n",
       "      <td>4596.58</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.52</td>\n",
       "      <td>58.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.91</td>\n",
       "      <td>92.09</td>\n",
       "      <td>73</td>\n",
       "      <td>625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>C3512</td>\n",
       "      <td>15/10/1960</td>\n",
       "      <td>24/01/1983</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0517</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4421.79</td>\n",
       "      <td>...</td>\n",
       "      <td>82.93</td>\n",
       "      <td>17.07</td>\n",
       "      <td>5.91</td>\n",
       "      <td>94.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.01</td>\n",
       "      <td>57</td>\n",
       "      <td>428</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>C1810</td>\n",
       "      <td>15/04/1982</td>\n",
       "      <td>09/04/2002</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1116</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4666.23</td>\n",
       "      <td>...</td>\n",
       "      <td>83.89</td>\n",
       "      <td>16.11</td>\n",
       "      <td>17.40</td>\n",
       "      <td>82.60</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>98.41</td>\n",
       "      <td>36</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>C1226</td>\n",
       "      <td>01/12/1989</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1171</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5126.79</td>\n",
       "      <td>...</td>\n",
       "      <td>41.07</td>\n",
       "      <td>58.93</td>\n",
       "      <td>17.98</td>\n",
       "      <td>82.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.05</td>\n",
       "      <td>97.95</td>\n",
       "      <td>28</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>C0336</td>\n",
       "      <td>28/12/1974</td>\n",
       "      <td>22/04/1997</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1171</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4721.17</td>\n",
       "      <td>...</td>\n",
       "      <td>41.07</td>\n",
       "      <td>58.93</td>\n",
       "      <td>17.98</td>\n",
       "      <td>82.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.05</td>\n",
       "      <td>97.95</td>\n",
       "      <td>43</td>\n",
       "      <td>257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>C0310</td>\n",
       "      <td>06/09/1952</td>\n",
       "      <td>17/03/1979</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0220</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7970.05</td>\n",
       "      <td>...</td>\n",
       "      <td>54.69</td>\n",
       "      <td>45.31</td>\n",
       "      <td>32.78</td>\n",
       "      <td>67.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.22</td>\n",
       "      <td>90.77</td>\n",
       "      <td>66</td>\n",
       "      <td>474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>C1055</td>\n",
       "      <td>11/10/1944</td>\n",
       "      <td>23/07/1965</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0447</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5852.51</td>\n",
       "      <td>...</td>\n",
       "      <td>26.23</td>\n",
       "      <td>73.77</td>\n",
       "      <td>69.75</td>\n",
       "      <td>30.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.42</td>\n",
       "      <td>94.59</td>\n",
       "      <td>73</td>\n",
       "      <td>638</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>C3117</td>\n",
       "      <td>14/10/1987</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0447</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>357.70</td>\n",
       "      <td>3759.02</td>\n",
       "      <td>...</td>\n",
       "      <td>26.23</td>\n",
       "      <td>73.77</td>\n",
       "      <td>69.75</td>\n",
       "      <td>30.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.42</td>\n",
       "      <td>94.59</td>\n",
       "      <td>30</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>C0963</td>\n",
       "      <td>17/03/1975</td>\n",
       "      <td>29/09/1996</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0849</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5010.69</td>\n",
       "      <td>...</td>\n",
       "      <td>79.87</td>\n",
       "      <td>20.13</td>\n",
       "      <td>43.50</td>\n",
       "      <td>56.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>43</td>\n",
       "      <td>264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>C0085</td>\n",
       "      <td>04/09/1955</td>\n",
       "      <td>03/05/1978</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0679</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3476.08</td>\n",
       "      <td>...</td>\n",
       "      <td>95.89</td>\n",
       "      <td>4.11</td>\n",
       "      <td>12.46</td>\n",
       "      <td>87.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>63</td>\n",
       "      <td>485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>C1298</td>\n",
       "      <td>02/11/1982</td>\n",
       "      <td>14/04/2005</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1167</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5381.28</td>\n",
       "      <td>...</td>\n",
       "      <td>46.77</td>\n",
       "      <td>53.23</td>\n",
       "      <td>5.14</td>\n",
       "      <td>94.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>35</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>C2124</td>\n",
       "      <td>28/01/1977</td>\n",
       "      <td>17/07/1997</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0444</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3272.01</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.73</td>\n",
       "      <td>20.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>41</td>\n",
       "      <td>254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>C0211</td>\n",
       "      <td>21/01/1950</td>\n",
       "      <td>08/12/1972</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1257</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>355.19</td>\n",
       "      <td>4479.41</td>\n",
       "      <td>...</td>\n",
       "      <td>60.87</td>\n",
       "      <td>39.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>68</td>\n",
       "      <td>549</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>C3672</td>\n",
       "      <td>31/10/1981</td>\n",
       "      <td>04/05/2002</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0290</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7207.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>36</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>C3633</td>\n",
       "      <td>22/10/1965</td>\n",
       "      <td>21/05/1986</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3397.36</td>\n",
       "      <td>...</td>\n",
       "      <td>48.18</td>\n",
       "      <td>51.82</td>\n",
       "      <td>14.38</td>\n",
       "      <td>85.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>98.01</td>\n",
       "      <td>52</td>\n",
       "      <td>388</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>C1296</td>\n",
       "      <td>04/06/1985</td>\n",
       "      <td>03/03/2007</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1210</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6715.75</td>\n",
       "      <td>...</td>\n",
       "      <td>97.01</td>\n",
       "      <td>2.99</td>\n",
       "      <td>25.69</td>\n",
       "      <td>74.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.16</td>\n",
       "      <td>91.85</td>\n",
       "      <td>33</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>C0934</td>\n",
       "      <td>05/06/1951</td>\n",
       "      <td>11/12/1972</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0648</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2244.72</td>\n",
       "      <td>...</td>\n",
       "      <td>42.51</td>\n",
       "      <td>57.49</td>\n",
       "      <td>33.80</td>\n",
       "      <td>66.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.74</td>\n",
       "      <td>94.26</td>\n",
       "      <td>67</td>\n",
       "      <td>549</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>C1673</td>\n",
       "      <td>11/02/1966</td>\n",
       "      <td>13/07/1986</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0723</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>93.77</td>\n",
       "      <td>7133.86</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.26</td>\n",
       "      <td>79.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>52</td>\n",
       "      <td>386</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>C3171</td>\n",
       "      <td>06/07/1984</td>\n",
       "      <td>13/02/2006</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0777</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3328.49</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32.87</td>\n",
       "      <td>67.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>34</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>C2304</td>\n",
       "      <td>19/05/1990</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1612</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1793.68</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.21</td>\n",
       "      <td>86.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.47</td>\n",
       "      <td>93.53</td>\n",
       "      <td>28</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>C3278</td>\n",
       "      <td>22/01/1961</td>\n",
       "      <td>21/11/1981</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1706</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9539.93</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.89</td>\n",
       "      <td>45.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>57</td>\n",
       "      <td>442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>C1100</td>\n",
       "      <td>02/06/1990</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1301</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8090.46</td>\n",
       "      <td>...</td>\n",
       "      <td>27.62</td>\n",
       "      <td>72.38</td>\n",
       "      <td>19.29</td>\n",
       "      <td>80.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.37</td>\n",
       "      <td>86.64</td>\n",
       "      <td>28</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>C2398</td>\n",
       "      <td>09/06/1987</td>\n",
       "      <td>08/06/2007</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0580</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6060.83</td>\n",
       "      <td>...</td>\n",
       "      <td>11.85</td>\n",
       "      <td>88.15</td>\n",
       "      <td>6.07</td>\n",
       "      <td>93.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.43</td>\n",
       "      <td>96.57</td>\n",
       "      <td>31</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>C2460</td>\n",
       "      <td>25/12/1962</td>\n",
       "      <td>18/03/1984</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1482</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4494.38</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>55</td>\n",
       "      <td>414</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>C2966</td>\n",
       "      <td>03/10/1948</td>\n",
       "      <td>15/10/1970</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1270</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3819.82</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.36</td>\n",
       "      <td>43.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>70</td>\n",
       "      <td>575</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>C1282</td>\n",
       "      <td>21/07/1979</td>\n",
       "      <td>18/03/2001</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1722</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5927.76</td>\n",
       "      <td>...</td>\n",
       "      <td>85.99</td>\n",
       "      <td>14.01</td>\n",
       "      <td>57.89</td>\n",
       "      <td>42.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.03</td>\n",
       "      <td>58.97</td>\n",
       "      <td>39</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>C2375</td>\n",
       "      <td>11/03/1982</td>\n",
       "      <td>27/10/2003</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3151.20</td>\n",
       "      <td>...</td>\n",
       "      <td>82.59</td>\n",
       "      <td>17.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>36</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>C1691</td>\n",
       "      <td>27/08/1960</td>\n",
       "      <td>09/03/1983</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1513</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5703.46</td>\n",
       "      <td>...</td>\n",
       "      <td>84.75</td>\n",
       "      <td>15.25</td>\n",
       "      <td>40.83</td>\n",
       "      <td>59.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>58</td>\n",
       "      <td>426</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>C2897</td>\n",
       "      <td>15/09/1989</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4137.85</td>\n",
       "      <td>...</td>\n",
       "      <td>84.75</td>\n",
       "      <td>15.25</td>\n",
       "      <td>40.83</td>\n",
       "      <td>59.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>29</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>C0076</td>\n",
       "      <td>16/12/1980</td>\n",
       "      <td>31/01/2002</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1120</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9391.84</td>\n",
       "      <td>...</td>\n",
       "      <td>15.32</td>\n",
       "      <td>84.68</td>\n",
       "      <td>27.99</td>\n",
       "      <td>72.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.67</td>\n",
       "      <td>93.34</td>\n",
       "      <td>37</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>C1895</td>\n",
       "      <td>25/10/1951</td>\n",
       "      <td>12/03/1972</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0311</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1909.58</td>\n",
       "      <td>...</td>\n",
       "      <td>45.34</td>\n",
       "      <td>54.66</td>\n",
       "      <td>29.69</td>\n",
       "      <td>70.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>66</td>\n",
       "      <td>558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>C0446</td>\n",
       "      <td>22/12/1965</td>\n",
       "      <td>04/01/1989</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1620</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5484.33</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>72.57</td>\n",
       "      <td>27.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>52</td>\n",
       "      <td>357</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>C2393</td>\n",
       "      <td>10/01/1985</td>\n",
       "      <td>28/12/2007</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2671.60</td>\n",
       "      <td>...</td>\n",
       "      <td>83.76</td>\n",
       "      <td>16.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>33</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>C3194</td>\n",
       "      <td>06/11/1949</td>\n",
       "      <td>24/10/1971</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1362.05</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.65</td>\n",
       "      <td>56.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.07</td>\n",
       "      <td>95.93</td>\n",
       "      <td>68</td>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>C1553</td>\n",
       "      <td>21/08/1945</td>\n",
       "      <td>03/08/1970</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0327</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7059.42</td>\n",
       "      <td>...</td>\n",
       "      <td>65.19</td>\n",
       "      <td>34.81</td>\n",
       "      <td>6.23</td>\n",
       "      <td>93.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>73</td>\n",
       "      <td>578</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>C1201</td>\n",
       "      <td>17/11/1970</td>\n",
       "      <td>16/05/1992</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0638</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7386.12</td>\n",
       "      <td>...</td>\n",
       "      <td>73.22</td>\n",
       "      <td>26.78</td>\n",
       "      <td>18.53</td>\n",
       "      <td>81.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>47</td>\n",
       "      <td>316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>C0596</td>\n",
       "      <td>11/04/1973</td>\n",
       "      <td>29/10/1995</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1169</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4374.25</td>\n",
       "      <td>6085.28</td>\n",
       "      <td>...</td>\n",
       "      <td>12.33</td>\n",
       "      <td>87.67</td>\n",
       "      <td>17.81</td>\n",
       "      <td>82.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.61</td>\n",
       "      <td>84.39</td>\n",
       "      <td>45</td>\n",
       "      <td>275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>C0439</td>\n",
       "      <td>13/05/1946</td>\n",
       "      <td>25/05/1967</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0806</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5678.40</td>\n",
       "      <td>...</td>\n",
       "      <td>98.05</td>\n",
       "      <td>1.95</td>\n",
       "      <td>14.67</td>\n",
       "      <td>85.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>72</td>\n",
       "      <td>616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>C3526</td>\n",
       "      <td>21/04/1971</td>\n",
       "      <td>14/10/1992</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1188</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1333.27</td>\n",
       "      <td>...</td>\n",
       "      <td>44.94</td>\n",
       "      <td>55.06</td>\n",
       "      <td>69.50</td>\n",
       "      <td>30.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.89</td>\n",
       "      <td>85.11</td>\n",
       "      <td>47</td>\n",
       "      <td>311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>C0102</td>\n",
       "      <td>16/09/1971</td>\n",
       "      <td>31/07/1992</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1479</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8181.41</td>\n",
       "      <td>...</td>\n",
       "      <td>28.30</td>\n",
       "      <td>71.70</td>\n",
       "      <td>45.07</td>\n",
       "      <td>54.93</td>\n",
       "      <td>18.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>81.78</td>\n",
       "      <td>47</td>\n",
       "      <td>314</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>C1699</td>\n",
       "      <td>10/03/1950</td>\n",
       "      <td>01/06/1974</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1540</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5356.77</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>44.36</td>\n",
       "      <td>55.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>68</td>\n",
       "      <td>532</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>C3644</td>\n",
       "      <td>08/06/1966</td>\n",
       "      <td>07/12/1988</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1589</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1875.75</td>\n",
       "      <td>...</td>\n",
       "      <td>45.06</td>\n",
       "      <td>54.94</td>\n",
       "      <td>79.62</td>\n",
       "      <td>20.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.27</td>\n",
       "      <td>58.73</td>\n",
       "      <td>52</td>\n",
       "      <td>357</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>C2707</td>\n",
       "      <td>06/05/1979</td>\n",
       "      <td>22/01/2001</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z1471</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9215.10</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.42</td>\n",
       "      <td>79.58</td>\n",
       "      <td>5.94</td>\n",
       "      <td>22.52</td>\n",
       "      <td>71.54</td>\n",
       "      <td>39</td>\n",
       "      <td>212</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>C0417</td>\n",
       "      <td>24/05/1963</td>\n",
       "      <td>12/05/1983</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0616</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2582.68</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.58</td>\n",
       "      <td>58.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>99.99</td>\n",
       "      <td>55</td>\n",
       "      <td>424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>C3130</td>\n",
       "      <td>12/01/1945</td>\n",
       "      <td>26/10/1966</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0424</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6855.77</td>\n",
       "      <td>...</td>\n",
       "      <td>69.87</td>\n",
       "      <td>30.13</td>\n",
       "      <td>45.70</td>\n",
       "      <td>54.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>73</td>\n",
       "      <td>623</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>C1554</td>\n",
       "      <td>16/09/1949</td>\n",
       "      <td>14/04/1971</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0854</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7684.82</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.53</td>\n",
       "      <td>46.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>69</td>\n",
       "      <td>569</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2844</th>\n",
       "      <td>C1123</td>\n",
       "      <td>16/11/1953</td>\n",
       "      <td>13/06/1974</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0848</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4981.78</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71.88</td>\n",
       "      <td>28.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>44.90</td>\n",
       "      <td>55.10</td>\n",
       "      <td>64</td>\n",
       "      <td>531</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>C0547</td>\n",
       "      <td>05/01/1986</td>\n",
       "      <td>15/08/2009</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z0322</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6977.63</td>\n",
       "      <td>...</td>\n",
       "      <td>42.43</td>\n",
       "      <td>57.57</td>\n",
       "      <td>46.11</td>\n",
       "      <td>53.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.79</td>\n",
       "      <td>70.21</td>\n",
       "      <td>32</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>C3732</td>\n",
       "      <td>20/11/1965</td>\n",
       "      <td>30/12/1986</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Z0112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8357.03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.04</td>\n",
       "      <td>98.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.26</td>\n",
       "      <td>98.74</td>\n",
       "      <td>52</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>C0959</td>\n",
       "      <td>13/12/1980</td>\n",
       "      <td>29/02/2004</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Z1424</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>181.56</td>\n",
       "      <td>1968.15</td>\n",
       "      <td>...</td>\n",
       "      <td>91.91</td>\n",
       "      <td>8.09</td>\n",
       "      <td>31.24</td>\n",
       "      <td>68.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>37</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1320 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID_Cliente Fecha_Nacimiento  Fecha_Alta    Sexo ID_Zona  Productos_Vida  \\\n",
       "11        C1947       10/09/1948  08/09/1973  Hombre   Z1201               1   \n",
       "34        C2877       05/11/1966  19/11/1988  Hombre   Z0789               0   \n",
       "41        C1031       04/02/1980  20/01/2001   Mujer   Z0789               0   \n",
       "60        C1093       29/10/1948  18/03/1970  Hombre   Z0664               0   \n",
       "69        C3214       03/03/1948  02/09/1970  Hombre   Z1707               0   \n",
       "84        C2780       27/07/1952  13/10/1972  Hombre   Z0738               0   \n",
       "109       C0013       29/11/1973  08/08/2000  Hombre   Z0314               0   \n",
       "138       C2650       17/04/1986  01/12/2006   Mujer   Z1112               0   \n",
       "153       C1412       06/04/1971  05/06/1991  Hombre   Z0522               0   \n",
       "157       C0799       16/12/1989  15/08/2009   Mujer   Z0522               0   \n",
       "172       C2081       27/07/1945  07/08/1966  Hombre   Z0987               1   \n",
       "216       C3512       15/10/1960  24/01/1983  Hombre   Z0517               0   \n",
       "239       C1810       15/04/1982  09/04/2002  Hombre   Z1116               0   \n",
       "263       C1226       01/12/1989  15/08/2009  Hombre   Z1171               0   \n",
       "270       C0336       28/12/1974  22/04/1997   Mujer   Z1171               0   \n",
       "293       C0310       06/09/1952  17/03/1979  Hombre   Z0220               0   \n",
       "296       C1055       11/10/1944  23/07/1965  Hombre   Z0447               0   \n",
       "298       C3117       14/10/1987  15/08/2009  Hombre   Z0447               2   \n",
       "306       C0963       17/03/1975  29/09/1996  Hombre   Z0849               0   \n",
       "379       C0085       04/09/1955  03/05/1978  Hombre   Z0679               0   \n",
       "395       C1298       02/11/1982  14/04/2005   Mujer   Z1167               0   \n",
       "425       C2124       28/01/1977  17/07/1997  Hombre   Z0444               0   \n",
       "458       C0211       21/01/1950  08/12/1972  Hombre   Z1257               1   \n",
       "490       C3672       31/10/1981  04/05/2002   Mujer   Z0290               0   \n",
       "498       C3633       22/10/1965  21/05/1986  Hombre   Z0255               0   \n",
       "500       C1296       04/06/1985  03/03/2007   Mujer   Z1210               0   \n",
       "511       C0934       05/06/1951  11/12/1972  Hombre   Z0648               0   \n",
       "513       C1673       11/02/1966  13/07/1986  Hombre   Z0723               1   \n",
       "522       C3171       06/07/1984  13/02/2006   Mujer   Z0777               0   \n",
       "601       C2304       19/05/1990  15/08/2009  Hombre   Z1612               0   \n",
       "...         ...              ...         ...     ...     ...             ...   \n",
       "2466      C3278       22/01/1961  21/11/1981   Mujer   Z1706               0   \n",
       "2469      C1100       02/06/1990  15/08/2009   Mujer   Z1301               0   \n",
       "2487      C2398       09/06/1987  08/06/2007   Mujer   Z0580               0   \n",
       "2502      C2460       25/12/1962  18/03/1984  Hombre   Z1482               0   \n",
       "2516      C2966       03/10/1948  15/10/1970  Hombre   Z1270               0   \n",
       "2531      C1282       21/07/1979  18/03/2001  Hombre   Z1722               0   \n",
       "2537      C2375       11/03/1982  27/10/2003   Mujer   Z0078               0   \n",
       "2549      C1691       27/08/1960  09/03/1983  Hombre   Z1513               0   \n",
       "2550      C2897       15/09/1989  15/08/2009   Mujer   Z1513               0   \n",
       "2556      C0076       16/12/1980  31/01/2002  Hombre   Z1120               0   \n",
       "2595      C1895       25/10/1951  12/03/1972  Hombre   Z0311               0   \n",
       "2605      C0446       22/12/1965  04/01/1989   Mujer   Z1620               0   \n",
       "2614      C2393       10/01/1985  28/12/2007  Hombre   Z0739               0   \n",
       "2653      C3194       06/11/1949  24/10/1971  Hombre   Z1615               0   \n",
       "2665      C1553       21/08/1945  03/08/1970  Hombre   Z0327               0   \n",
       "2684      C1201       17/11/1970  16/05/1992  Hombre   Z0638               0   \n",
       "2693      C0596       11/04/1973  29/10/1995  Hombre   Z1169               1   \n",
       "2730      C0439       13/05/1946  25/05/1967  Hombre   Z0806               0   \n",
       "2769      C3526       21/04/1971  14/10/1992   Mujer   Z1188               0   \n",
       "2770      C0102       16/09/1971  31/07/1992  Hombre   Z1479               0   \n",
       "2784      C1699       10/03/1950  01/06/1974   Mujer   Z1540               0   \n",
       "2803      C3644       08/06/1966  07/12/1988  Hombre   Z1589               0   \n",
       "2805      C2707       06/05/1979  22/01/2001  Hombre   Z1471               0   \n",
       "2809      C0417       24/05/1963  12/05/1983  Hombre   Z0616               0   \n",
       "2837      C3130       12/01/1945  26/10/1966  Hombre   Z0424               0   \n",
       "2842      C1554       16/09/1949  14/04/1971  Hombre   Z0854               0   \n",
       "2844      C1123       16/11/1953  13/06/1974  Hombre   Z0848               0   \n",
       "2855      C0547       05/01/1986  15/08/2009   Mujer   Z0322               0   \n",
       "2894      C3732       20/11/1965  30/12/1986  Hombre   Z0112               0   \n",
       "2925      C0959       13/12/1980  29/02/2004   Mujer   Z1424               1   \n",
       "\n",
       "      Productos_Vehiculos  Productos_Otros  Gasto_Vida  Gasto_Vehiculos  \\\n",
       "11                      0                2      492.21          3890.19   \n",
       "34                      1                3        0.00          6712.59   \n",
       "41                      1                2        0.00          6824.90   \n",
       "60                      1                2        0.00          5915.24   \n",
       "69                      1                2        0.00          6262.06   \n",
       "84                      1                2        0.00          6246.52   \n",
       "109                     1                1        0.00          7616.57   \n",
       "138                     1                0        0.00          3529.81   \n",
       "153                     3                2        0.00          8725.95   \n",
       "157                     1                1        0.00          5042.51   \n",
       "172                     1                2      167.85          4596.58   \n",
       "216                     1                0        0.00          4421.79   \n",
       "239                     1                2        0.00          4666.23   \n",
       "263                     1                2        0.00          5126.79   \n",
       "270                     1                2        0.00          4721.17   \n",
       "293                     1                1        0.00          7970.05   \n",
       "296                     3                2        0.00          5852.51   \n",
       "298                     1                2      357.70          3759.02   \n",
       "306                     2                2        0.00          5010.69   \n",
       "379                     2                1        0.00          3476.08   \n",
       "395                     1                2        0.00          5381.28   \n",
       "425                     2                1        0.00          3272.01   \n",
       "458                     0                2      355.19          4479.41   \n",
       "490                     1                2        0.00          7207.71   \n",
       "498                     0                2        0.00          3397.36   \n",
       "500                     3                2        0.00          6715.75   \n",
       "511                     0                2        0.00          2244.72   \n",
       "513                     1                2       93.77          7133.86   \n",
       "522                     0                1        0.00          3328.49   \n",
       "601                     0                0        0.00          1793.68   \n",
       "...                   ...              ...         ...              ...   \n",
       "2466                    2                2        0.00          9539.93   \n",
       "2469                    2                2        0.00          8090.46   \n",
       "2487                    1                2        0.00          6060.83   \n",
       "2502                    1                1        0.00          4494.38   \n",
       "2516                    2                2        0.00          3819.82   \n",
       "2531                    1                2        0.00          5927.76   \n",
       "2537                    0                0        0.00          3151.20   \n",
       "2549                    1                1        0.00          5703.46   \n",
       "2550                    0                0        0.00          4137.85   \n",
       "2556                    1                3        0.00          9391.84   \n",
       "2595                    0                2        0.00          1909.58   \n",
       "2605                    1                2        0.00          5484.33   \n",
       "2614                    0                2        0.00          2671.60   \n",
       "2653                    0                1        0.00          1362.05   \n",
       "2665                    1                0        0.00          7059.42   \n",
       "2684                    2                2        0.00          7386.12   \n",
       "2693                    1                1     4374.25          6085.28   \n",
       "2730                    3                0        0.00          5678.40   \n",
       "2769                    1                2        0.00          1333.27   \n",
       "2770                    1                2        0.00          8181.41   \n",
       "2784                    1                3        0.00          5356.77   \n",
       "2803                    0                2        0.00          1875.75   \n",
       "2805                    2                2        0.00          9215.10   \n",
       "2809                    0                2        0.00          2582.68   \n",
       "2837                    2                1        0.00          6855.77   \n",
       "2842                    2                2        0.00          7684.82   \n",
       "2844                    1                2        0.00          4981.78   \n",
       "2855                    1                2        0.00          6977.63   \n",
       "2894                    1                2        0.00          8357.03   \n",
       "2925                    0                2      181.56          1968.15   \n",
       "\n",
       "       ...    Vivienda_Propiedad  Vivienda_Alquiler  Medico_Seguro_Privado  \\\n",
       "11     ...                 92.04               7.96                  43.84   \n",
       "34     ...                100.00               0.00                  20.08   \n",
       "41     ...                100.00               0.00                  20.08   \n",
       "60     ...                 16.11              83.89                   1.91   \n",
       "69     ...                100.00               0.00                  70.70   \n",
       "84     ...                 79.05              20.95                   0.00   \n",
       "109    ...                  5.65              94.35                   0.00   \n",
       "138    ...                  0.00             100.00                   8.78   \n",
       "153    ...                100.00               0.00                  77.26   \n",
       "157    ...                100.00               0.00                  77.26   \n",
       "172    ...                100.00               0.00                  41.52   \n",
       "216    ...                 82.93              17.07                   5.91   \n",
       "239    ...                 83.89              16.11                  17.40   \n",
       "263    ...                 41.07              58.93                  17.98   \n",
       "270    ...                 41.07              58.93                  17.98   \n",
       "293    ...                 54.69              45.31                  32.78   \n",
       "296    ...                 26.23              73.77                  69.75   \n",
       "298    ...                 26.23              73.77                  69.75   \n",
       "306    ...                 79.87              20.13                  43.50   \n",
       "379    ...                 95.89               4.11                  12.46   \n",
       "395    ...                 46.77              53.23                   5.14   \n",
       "425    ...                100.00               0.00                  79.73   \n",
       "458    ...                 60.87              39.13                   0.00   \n",
       "490    ...                  0.00             100.00                   0.00   \n",
       "498    ...                 48.18              51.82                  14.38   \n",
       "500    ...                 97.01               2.99                  25.69   \n",
       "511    ...                 42.51              57.49                  33.80   \n",
       "513    ...                100.00               0.00                  20.26   \n",
       "522    ...                100.00               0.00                  32.87   \n",
       "601    ...                100.00               0.00                  13.21   \n",
       "...    ...                   ...                ...                    ...   \n",
       "2466   ...                100.00               0.00                  54.89   \n",
       "2469   ...                 27.62              72.38                  19.29   \n",
       "2487   ...                 11.85              88.15                   6.07   \n",
       "2502   ...                100.00               0.00                 100.00   \n",
       "2516   ...                100.00               0.00                  56.36   \n",
       "2531   ...                 85.99              14.01                  57.89   \n",
       "2537   ...                 82.59              17.41                   0.00   \n",
       "2549   ...                 84.75              15.25                  40.83   \n",
       "2550   ...                 84.75              15.25                  40.83   \n",
       "2556   ...                 15.32              84.68                  27.99   \n",
       "2595   ...                 45.34              54.66                  29.69   \n",
       "2605   ...                100.00               0.00                  72.57   \n",
       "2614   ...                 83.76              16.24                   0.00   \n",
       "2653   ...                100.00               0.00                  43.65   \n",
       "2665   ...                 65.19              34.81                   6.23   \n",
       "2684   ...                 73.22              26.78                  18.53   \n",
       "2693   ...                 12.33              87.67                  17.81   \n",
       "2730   ...                 98.05               1.95                  14.67   \n",
       "2769   ...                 44.94              55.06                  69.50   \n",
       "2770   ...                 28.30              71.70                  45.07   \n",
       "2784   ...                100.00               0.00                  44.36   \n",
       "2803   ...                 45.06              54.94                  79.62   \n",
       "2805   ...                100.00               0.00                  20.42   \n",
       "2809   ...                100.00               0.00                  41.58   \n",
       "2837   ...                 69.87              30.13                  45.70   \n",
       "2842   ...                100.00               0.00                  53.53   \n",
       "2844   ...                100.00               0.00                  71.88   \n",
       "2855   ...                 42.43              57.57                  46.11   \n",
       "2894   ...                  1.04              98.96                   0.00   \n",
       "2925   ...                 91.91               8.09                  31.24   \n",
       "\n",
       "      Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "11                      56.16                   0.00   \n",
       "34                      79.92                   0.00   \n",
       "41                      79.92                   0.00   \n",
       "60                      98.09                   0.00   \n",
       "69                      29.30                  16.02   \n",
       "84                     100.00                   0.00   \n",
       "109                    100.00                   0.00   \n",
       "138                     91.22                   0.00   \n",
       "153                     22.74                   0.00   \n",
       "157                     22.74                   0.00   \n",
       "172                     58.48                   0.00   \n",
       "216                     94.09                   0.00   \n",
       "239                     82.60                   1.59   \n",
       "263                     82.02                   0.00   \n",
       "270                     82.02                   0.00   \n",
       "293                     67.22                   0.00   \n",
       "296                     30.25                   0.00   \n",
       "298                     30.25                   0.00   \n",
       "306                     56.50                   0.00   \n",
       "379                     87.54                   0.00   \n",
       "395                     94.86                   0.00   \n",
       "425                     20.27                   0.00   \n",
       "458                    100.00                   0.00   \n",
       "490                    100.00                   0.00   \n",
       "498                     85.62                   0.00   \n",
       "500                     74.31                   0.00   \n",
       "511                     66.20                   0.00   \n",
       "513                     79.74                   0.00   \n",
       "522                     67.13                   0.00   \n",
       "601                     86.79                   0.00   \n",
       "...                       ...                    ...   \n",
       "2466                    45.11                   0.00   \n",
       "2469                    80.71                   0.00   \n",
       "2487                    93.93                   0.00   \n",
       "2502                     0.00                   0.00   \n",
       "2516                    43.64                   0.00   \n",
       "2531                    42.11                   0.00   \n",
       "2537                   100.00                   0.00   \n",
       "2549                    59.17                   0.00   \n",
       "2550                    59.17                   0.00   \n",
       "2556                    72.01                   0.00   \n",
       "2595                    70.31                   0.00   \n",
       "2605                    27.43                   0.00   \n",
       "2614                   100.00                   0.00   \n",
       "2653                    56.35                   0.00   \n",
       "2665                    93.77                   0.00   \n",
       "2684                    81.47                   0.00   \n",
       "2693                    82.19                   0.00   \n",
       "2730                    85.33                   0.00   \n",
       "2769                    30.50                   0.00   \n",
       "2770                    54.93                  18.22   \n",
       "2784                    55.64                   0.00   \n",
       "2803                    20.38                   0.00   \n",
       "2805                    79.58                   5.94   \n",
       "2809                    58.42                   0.00   \n",
       "2837                    54.30                   0.00   \n",
       "2842                    46.47                   0.00   \n",
       "2844                    28.12                   0.00   \n",
       "2855                    53.89                   0.00   \n",
       "2894                   100.00                   0.00   \n",
       "2925                    68.76                   0.00   \n",
       "\n",
       "      Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  Edad  Antiguedad  \\\n",
       "11                             3.38                 96.62    70         540   \n",
       "34                             0.00                100.00    51         358   \n",
       "41                             0.00                100.00    38         212   \n",
       "60                             0.00                100.01    69         582   \n",
       "69                            12.17                 71.81    70         577   \n",
       "84                             0.00                100.00    66         551   \n",
       "109                            6.53                 93.47    44         217   \n",
       "138                            9.59                 90.41    32         142   \n",
       "153                           21.57                 78.43    47         328   \n",
       "157                           21.57                 78.43    28         109   \n",
       "172                            7.91                 92.09    73         625   \n",
       "216                            0.00                100.01    57         428   \n",
       "239                            0.00                 98.41    36         197   \n",
       "263                            2.05                 97.95    28         109   \n",
       "270                            2.05                 97.95    43         257   \n",
       "293                            9.22                 90.77    66         474   \n",
       "296                            5.42                 94.59    73         638   \n",
       "298                            5.42                 94.59    30         109   \n",
       "306                            0.00                100.00    43         264   \n",
       "379                            0.00                100.00    63         485   \n",
       "395                            0.00                100.00    35         161   \n",
       "425                            0.00                100.00    41         254   \n",
       "458                            0.00                100.00    68         549   \n",
       "490                            0.00                100.00    36         197   \n",
       "498                            1.99                 98.01    52         388   \n",
       "500                            8.16                 91.85    33         139   \n",
       "511                            5.74                 94.26    67         549   \n",
       "513                            0.00                100.00    52         386   \n",
       "522                            0.00                100.00    34         151   \n",
       "601                            6.47                 93.53    28         109   \n",
       "...                             ...                   ...   ...         ...   \n",
       "2466                           0.00                100.00    57         442   \n",
       "2469                          13.37                 86.64    28         109   \n",
       "2487                           3.43                 96.57    31         135   \n",
       "2502                           0.00                100.00    55         414   \n",
       "2516                           0.00                100.00    70         575   \n",
       "2531                          41.03                 58.97    39         210   \n",
       "2537                           0.00                100.00    36         179   \n",
       "2549                           0.00                100.00    58         426   \n",
       "2550                           0.00                100.00    29         109   \n",
       "2556                           6.67                 93.34    37         200   \n",
       "2595                           0.00                100.00    66         558   \n",
       "2605                           0.00                100.00    52         357   \n",
       "2614                           0.00                100.00    33         129   \n",
       "2653                           4.07                 95.93    68         563   \n",
       "2665                           0.00                100.00    73         578   \n",
       "2684                           0.00                100.00    47         316   \n",
       "2693                          15.61                 84.39    45         275   \n",
       "2730                           0.00                100.00    72         616   \n",
       "2769                          14.89                 85.11    47         311   \n",
       "2770                           0.00                 81.78    47         314   \n",
       "2784                           0.00                100.00    68         532   \n",
       "2803                          41.27                 58.73    52         357   \n",
       "2805                          22.52                 71.54    39         212   \n",
       "2809                           0.00                 99.99    55         424   \n",
       "2837                           0.00                100.00    73         623   \n",
       "2842                           0.00                100.00    69         569   \n",
       "2844                          44.90                 55.10    64         531   \n",
       "2855                          29.79                 70.21    32         109   \n",
       "2894                           1.26                 98.74    52         381   \n",
       "2925                           0.00                100.00    37         175   \n",
       "\n",
       "      Sexo_2  \n",
       "11         1  \n",
       "34         1  \n",
       "41         0  \n",
       "60         1  \n",
       "69         1  \n",
       "84         1  \n",
       "109        1  \n",
       "138        0  \n",
       "153        1  \n",
       "157        0  \n",
       "172        1  \n",
       "216        1  \n",
       "239        1  \n",
       "263        1  \n",
       "270        0  \n",
       "293        1  \n",
       "296        1  \n",
       "298        1  \n",
       "306        1  \n",
       "379        1  \n",
       "395        0  \n",
       "425        1  \n",
       "458        1  \n",
       "490        0  \n",
       "498        1  \n",
       "500        0  \n",
       "511        1  \n",
       "513        1  \n",
       "522        0  \n",
       "601        1  \n",
       "...      ...  \n",
       "2466       0  \n",
       "2469       0  \n",
       "2487       0  \n",
       "2502       1  \n",
       "2516       1  \n",
       "2531       1  \n",
       "2537       0  \n",
       "2549       1  \n",
       "2550       0  \n",
       "2556       1  \n",
       "2595       1  \n",
       "2605       0  \n",
       "2614       1  \n",
       "2653       1  \n",
       "2665       1  \n",
       "2684       1  \n",
       "2693       1  \n",
       "2730       1  \n",
       "2769       0  \n",
       "2770       1  \n",
       "2784       0  \n",
       "2803       1  \n",
       "2805       1  \n",
       "2809       1  \n",
       "2837       1  \n",
       "2842       1  \n",
       "2844       1  \n",
       "2855       0  \n",
       "2894       1  \n",
       "2925       0  \n",
       "\n",
       "[1320 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_ext = datos\n",
    "\n",
    "for i in range(0,3):\n",
    "    datos_ext = datos_ext.append(datos_ext[datos_ext.Seguro_Vivienda==1])\n",
    "\n",
    "datos_ext[datos_ext.Seguro_Vivienda==1]\n",
    "#datos_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = datos[\"Seguro_Vivienda\"]\n",
    "variables = datos.drop([\"Seguro_Vivienda\", \"ID_Cliente\", \"Fecha_Nacimiento\", \"Fecha_Alta\", \"Sexo\", \"ID_Zona\"], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: Seguro_Vivienda, dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Productos_Vida</th>\n",
       "      <th>Productos_Vehiculos</th>\n",
       "      <th>Productos_Otros</th>\n",
       "      <th>Gasto_Vida</th>\n",
       "      <th>Gasto_Vehiculos</th>\n",
       "      <th>Gasto_Otros</th>\n",
       "      <th>Tipo_Familia</th>\n",
       "      <th>Tipo_Pareja</th>\n",
       "      <th>Tipo_Soltero</th>\n",
       "      <th>Educacion_Superior</th>\n",
       "      <th>...</th>\n",
       "      <th>Vivienda_Propiedad</th>\n",
       "      <th>Vivienda_Alquiler</th>\n",
       "      <th>Medico_Seguro_Privado</th>\n",
       "      <th>Medico_Seguridad_Social</th>\n",
       "      <th>Ingresos_Mas_De_40000</th>\n",
       "      <th>Ingresos_De_20000_Hasta_40000</th>\n",
       "      <th>Ingresos_Hasta_20000</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Antiguedad</th>\n",
       "      <th>Sexo_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>617.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.1</td>\n",
       "      <td>18.27</td>\n",
       "      <td>6.63</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>50</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.1</td>\n",
       "      <td>18.27</td>\n",
       "      <td>6.63</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>56</td>\n",
       "      <td>405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>197.14</td>\n",
       "      <td>75.1</td>\n",
       "      <td>18.27</td>\n",
       "      <td>6.63</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>51</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3315.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.1</td>\n",
       "      <td>18.27</td>\n",
       "      <td>6.63</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>70</td>\n",
       "      <td>588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2561.45</td>\n",
       "      <td>176.94</td>\n",
       "      <td>75.1</td>\n",
       "      <td>18.27</td>\n",
       "      <td>6.63</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>71.34</td>\n",
       "      <td>28.66</td>\n",
       "      <td>32.77</td>\n",
       "      <td>67.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.47</td>\n",
       "      <td>96.3</td>\n",
       "      <td>39</td>\n",
       "      <td>207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Productos_Vida  Productos_Vehiculos  Productos_Otros  Gasto_Vida  \\\n",
       "0               0                    1                0         0.0   \n",
       "1               0                    0                0         0.0   \n",
       "2               0                    0                1         0.0   \n",
       "3               0                    1                0         0.0   \n",
       "4               0                    1                2         0.0   \n",
       "\n",
       "   Gasto_Vehiculos  Gasto_Otros  Tipo_Familia  Tipo_Pareja  Tipo_Soltero  \\\n",
       "0           617.55         0.00          75.1        18.27          6.63   \n",
       "1             0.00         0.00          75.1        18.27          6.63   \n",
       "2             0.00       197.14          75.1        18.27          6.63   \n",
       "3          3315.54         0.00          75.1        18.27          6.63   \n",
       "4          2561.45       176.94          75.1        18.27          6.63   \n",
       "\n",
       "   Educacion_Superior   ...    Vivienda_Propiedad  Vivienda_Alquiler  \\\n",
       "0                2.75   ...                 71.34              28.66   \n",
       "1                2.75   ...                 71.34              28.66   \n",
       "2                2.75   ...                 71.34              28.66   \n",
       "3                2.75   ...                 71.34              28.66   \n",
       "4                2.75   ...                 71.34              28.66   \n",
       "\n",
       "   Medico_Seguro_Privado  Medico_Seguridad_Social  Ingresos_Mas_De_40000  \\\n",
       "0                  32.77                    67.23                   2.23   \n",
       "1                  32.77                    67.23                   2.23   \n",
       "2                  32.77                    67.23                   2.23   \n",
       "3                  32.77                    67.23                   2.23   \n",
       "4                  32.77                    67.23                   2.23   \n",
       "\n",
       "   Ingresos_De_20000_Hasta_40000  Ingresos_Hasta_20000  Edad  Antiguedad  \\\n",
       "0                           1.47                  96.3    50         354   \n",
       "1                           1.47                  96.3    56         405   \n",
       "2                           1.47                  96.3    51         377   \n",
       "3                           1.47                  96.3    70         588   \n",
       "4                           1.47                  96.3    39         207   \n",
       "\n",
       "   Sexo_2  \n",
       "0       0  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    variables, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = y_train*1\n",
    "y_test = y_test*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scale = StandardScaler()\n",
    "\n",
    "X_scale_train = scale.fit_transform(X_train)\n",
    "X_scale_test = scale.transform(X_test)\n",
    "\n",
    "#X_scale_train = X_scale_train.reshape((len(X_scale_train), np.prod(X_scale_train.shape[1:])))\n",
    "#X_scale_test = X_scale_test.reshape((len(X_scale_test), np.prod(X_scale_test.shape[1:])))\n",
    "\n",
    "#y_cat_train = y_train*1\n",
    "#y_cat_test = y_test*1\n",
    "\n",
    "#Necesitamos este paso para poder probar con dos neuronas en la última capa\n",
    "y_cat_train = to_categorical(y_train.astype(int))\n",
    "y_cat_test = to_categorical(y_test.astype(int))\n",
    "\n",
    "#y_cat_train[1:50]\n",
    "#y_train[1:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por lo que veo en las últimas versiones han quitado alguna métrica de Keras, pero en esta ruta están implementadas muchas:\n",
    "# https://github.com/GeekLiB/keras/blob/master/keras/metrics.py\n",
    "# tengo dudas si el recall es correcto, pero luego saldremos de dudas cuando veamos las curvas ROC en el siguiente notebook\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    '''Calculates the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    '''\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    '''Calculates the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    '''\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    '''Calculates the F score, the weighted harmonic mean of precision and recall.\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    '''\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "        \n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=0.01\n",
    "rate = 0.1\n",
    "x = Input(shape=(26,))\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(x)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(layer)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(layer)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(layer)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(layer)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(layer)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dense(100, activation='softsign', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l1(l))(layer)\n",
    "layer = Dropout(rate)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "y = Dense(1, activation='sigmoid',kernel_initializer='he_uniform')(layer)\n",
    "mlp = Model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "nadam = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "#callback = [EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto', baseline=None, restore_best_weights=False)]\n",
    "mlp.compile(optimizer='nadam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', recall, fbeta_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2106 samples, validate on 235 samples\n",
      "Epoch 1/1000\n",
      "2106/2106 [==============================] - 6s 3ms/step - loss: 37.3646 - acc: 0.8476 - recall: 0.0867 - fbeta_score: 0.0553 - val_loss: 8.7820 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "2106/2106 [==============================] - 1s 551us/step - loss: 4.4858 - acc: 0.9112 - recall: 0.0166 - fbeta_score: 0.0182 - val_loss: 1.7479 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "2106/2106 [==============================] - 1s 563us/step - loss: 2.2673 - acc: 0.9311 - recall: 0.0195 - fbeta_score: 0.0147 - val_loss: 4.6572 - val_acc: 0.6255 - val_recall: 0.4681 - val_fbeta_score: 0.1942\n",
      "Epoch 4/1000\n",
      "2106/2106 [==============================] - 1s 561us/step - loss: 2.0246 - acc: 0.9283 - recall: 0.0182 - fbeta_score: 0.0214 - val_loss: 1.5094 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "2106/2106 [==============================] - 1s 577us/step - loss: 1.4857 - acc: 0.9349 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 1.3975 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "2106/2106 [==============================] - 1s 561us/step - loss: 1.3319 - acc: 0.9335 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.7609 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "2106/2106 [==============================] - 1s 562us/step - loss: 1.4486 - acc: 0.9354 - recall: 0.0119 - fbeta_score: 0.0135 - val_loss: 1.2129 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "2106/2106 [==============================] - 1s 560us/step - loss: 1.5148 - acc: 0.9383 - recall: 0.0214 - fbeta_score: 0.0196 - val_loss: 1.4754 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "2106/2106 [==============================] - 1s 657us/step - loss: 1.2943 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0055 - val_loss: 0.8507 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "2106/2106 [==============================] - 1s 583us/step - loss: 1.2971 - acc: 0.9411 - recall: 0.0111 - fbeta_score: 0.0119 - val_loss: 0.9582 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "2106/2106 [==============================] - 1s 596us/step - loss: 1.0511 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.7414 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "2106/2106 [==============================] - 1s 640us/step - loss: 1.0837 - acc: 0.9425 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.7437 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 13/1000\n",
      "2106/2106 [==============================] - 1s 608us/step - loss: 1.1999 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 1.1510 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 14/1000\n",
      "2106/2106 [==============================] - 1s 596us/step - loss: 1.0015 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.5970 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 15/1000\n",
      "2106/2106 [==============================] - 1s 562us/step - loss: 0.9443 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.9315 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 16/1000\n",
      "2106/2106 [==============================] - 1s 569us/step - loss: 1.0356 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 1.0636 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 17/1000\n",
      "2106/2106 [==============================] - 1s 578us/step - loss: 0.9736 - acc: 0.9416 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 1.0987 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 18/1000\n",
      "2106/2106 [==============================] - 1s 577us/step - loss: 1.0735 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.9713 - val_acc: 0.9489 - val_recall: 0.0638 - val_fbeta_score: 0.0709\n",
      "Epoch 19/1000\n",
      "2106/2106 [==============================] - 1s 570us/step - loss: 1.1471 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 1.5969 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 20/1000\n",
      "2106/2106 [==============================] - 1s 658us/step - loss: 1.0905 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 1.0916 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 21/1000\n",
      "2106/2106 [==============================] - 1s 617us/step - loss: 1.0577 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 1.2834 - val_acc: 0.9106 - val_recall: 0.1489 - val_fbeta_score: 0.1348\n",
      "Epoch 22/1000\n",
      "2106/2106 [==============================] - 2s 725us/step - loss: 1.2618 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.9813 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 23/1000\n",
      "2106/2106 [==============================] - 1s 581us/step - loss: 1.0711 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 1.2011 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 24/1000\n",
      "2106/2106 [==============================] - 1s 710us/step - loss: 1.8221 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 5.0188 - val_acc: 0.7745 - val_recall: 0.3617 - val_fbeta_score: 0.1944\n",
      "Epoch 25/1000\n",
      "2106/2106 [==============================] - 1s 663us/step - loss: 2.8786 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 2.0425 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 26/1000\n",
      "2106/2106 [==============================] - 1s 689us/step - loss: 1.7081 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 1.2220 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 27/1000\n",
      "2106/2106 [==============================] - 2s 736us/step - loss: 1.3153 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.9733 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 28/1000\n",
      "2106/2106 [==============================] - 1s 665us/step - loss: 1.1263 - acc: 0.9411 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.8858 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 29/1000\n",
      "2106/2106 [==============================] - 2s 755us/step - loss: 1.0787 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 1.1464 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 30/1000\n",
      "2106/2106 [==============================] - 1s 603us/step - loss: 1.0190 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 1.2038 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 31/1000\n",
      "2106/2106 [==============================] - 1s 704us/step - loss: 0.9511 - acc: 0.9430 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.9147 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 32/1000\n",
      "2106/2106 [==============================] - 1s 604us/step - loss: 0.9745 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 1.0188 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 33/1000\n",
      "2106/2106 [==============================] - 1s 605us/step - loss: 1.0355 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.9305 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 34/1000\n",
      "2106/2106 [==============================] - 1s 606us/step - loss: 0.9643 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.9921 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 35/1000\n",
      "2106/2106 [==============================] - 1s 593us/step - loss: 0.9283 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.7586 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000\n",
      "2106/2106 [==============================] - 1s 547us/step - loss: 1.0527 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.8126 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 37/1000\n",
      "2106/2106 [==============================] - 1s 545us/step - loss: 1.0737 - acc: 0.9416 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.9343 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 38/1000\n",
      "2106/2106 [==============================] - 1s 614us/step - loss: 0.9122 - acc: 0.9383 - recall: 0.0040 - fbeta_score: 0.0055 - val_loss: 0.8153 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 39/1000\n",
      "2106/2106 [==============================] - 1s 627us/step - loss: 0.8641 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.7836 - val_acc: 0.9447 - val_recall: 0.0213 - val_fbeta_score: 0.0284\n",
      "Epoch 40/1000\n",
      "2106/2106 [==============================] - 2s 813us/step - loss: 0.9166 - acc: 0.9392 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.7841 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 41/1000\n",
      "2106/2106 [==============================] - 1s 582us/step - loss: 0.8012 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 1.2131 - val_acc: 0.8936 - val_recall: 0.4043 - val_fbeta_score: 0.3078\n",
      "Epoch 42/1000\n",
      "2106/2106 [==============================] - 1s 590us/step - loss: 0.9328 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.8340 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 43/1000\n",
      "2106/2106 [==============================] - 2s 753us/step - loss: 0.8141 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 1.0932 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 44/1000\n",
      "2106/2106 [==============================] - 1s 651us/step - loss: 0.8729 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 1.1918 - val_acc: 0.9404 - val_recall: 0.0213 - val_fbeta_score: 0.0284\n",
      "Epoch 45/1000\n",
      "2106/2106 [==============================] - 1s 551us/step - loss: 0.8302 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.7345 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 46/1000\n",
      "2106/2106 [==============================] - 1s 498us/step - loss: 0.8684 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.6503 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 47/1000\n",
      "2106/2106 [==============================] - 1s 557us/step - loss: 0.7636 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.7234 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 48/1000\n",
      "2106/2106 [==============================] - 1s 610us/step - loss: 0.8121 - acc: 0.9421 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.7040 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 49/1000\n",
      "2106/2106 [==============================] - 1s 582us/step - loss: 0.7980 - acc: 0.9425 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.8187 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 50/1000\n",
      "2106/2106 [==============================] - 1s 592us/step - loss: 0.7730 - acc: 0.9378 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.7119 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 51/1000\n",
      "2106/2106 [==============================] - 1s 649us/step - loss: 0.8600 - acc: 0.9406 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.6901 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 52/1000\n",
      "2106/2106 [==============================] - 1s 639us/step - loss: 0.9287 - acc: 0.9392 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.8741 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 53/1000\n",
      "2106/2106 [==============================] - 1s 596us/step - loss: 0.7455 - acc: 0.9411 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.6758 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 54/1000\n",
      "2106/2106 [==============================] - 1s 596us/step - loss: 0.6945 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5603 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 55/1000\n",
      "2106/2106 [==============================] - 1s 654us/step - loss: 0.7745 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.6338 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 56/1000\n",
      "2106/2106 [==============================] - 1s 492us/step - loss: 0.7398 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 1.0628 - val_acc: 0.9277 - val_recall: 0.3617 - val_fbeta_score: 0.3546\n",
      "Epoch 57/1000\n",
      "2106/2106 [==============================] - 1s 589us/step - loss: 0.8303 - acc: 0.9383 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.5732 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 58/1000\n",
      "2106/2106 [==============================] - 1s 525us/step - loss: 0.6807 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.6161 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 59/1000\n",
      "2106/2106 [==============================] - 1s 547us/step - loss: 0.7016 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.5978 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 60/1000\n",
      "2106/2106 [==============================] - 1s 476us/step - loss: 0.6872 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.6039 - val_acc: 0.9489 - val_recall: 0.0426 - val_fbeta_score: 0.0426\n",
      "Epoch 61/1000\n",
      "2106/2106 [==============================] - 1s 487us/step - loss: 0.6926 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.6582 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 62/1000\n",
      "2106/2106 [==============================] - 1s 495us/step - loss: 0.7083 - acc: 0.9368 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5985 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 63/1000\n",
      "2106/2106 [==============================] - 1s 557us/step - loss: 0.6381 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.6792 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 64/1000\n",
      "2106/2106 [==============================] - 1s 501us/step - loss: 0.6362 - acc: 0.9430 - recall: 0.0475 - fbeta_score: 0.0475 - val_loss: 0.5875 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 65/1000\n",
      "2106/2106 [==============================] - 1s 498us/step - loss: 0.6729 - acc: 0.9411 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.7808 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 66/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.6672 - acc: 0.9387 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.7275 - val_acc: 0.8936 - val_recall: 0.3830 - val_fbeta_score: 0.2922\n",
      "Epoch 67/1000\n",
      "2106/2106 [==============================] - 1s 576us/step - loss: 0.6222 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.6334 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 68/1000\n",
      "2106/2106 [==============================] - 1s 487us/step - loss: 0.6546 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4971 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 69/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.7026 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.6034 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 70/1000\n",
      "2106/2106 [==============================] - 1s 544us/step - loss: 0.7166 - acc: 0.9397 - recall: 0.0016 - fbeta_score: 0.0024 - val_loss: 0.7321 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 71/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 665us/step - loss: 0.7056 - acc: 0.9421 - recall: 0.0380 - fbeta_score: 0.0396 - val_loss: 0.5716 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 72/1000\n",
      "2106/2106 [==============================] - 1s 676us/step - loss: 0.6542 - acc: 0.9425 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.5669 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 73/1000\n",
      "2106/2106 [==============================] - 1s 545us/step - loss: 0.6368 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.5919 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 74/1000\n",
      "2106/2106 [==============================] - 1s 533us/step - loss: 0.6500 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.4913 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 75/1000\n",
      "2106/2106 [==============================] - 1s 494us/step - loss: 0.5470 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.7650 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 76/1000\n",
      "2106/2106 [==============================] - 1s 641us/step - loss: 0.6265 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.6105 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 77/1000\n",
      "2106/2106 [==============================] - 1s 537us/step - loss: 0.6321 - acc: 0.9397 - recall: 0.0063 - fbeta_score: 0.0071 - val_loss: 0.4795 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 78/1000\n",
      "2106/2106 [==============================] - 1s 672us/step - loss: 0.5570 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.5992 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 79/1000\n",
      "2106/2106 [==============================] - 1s 588us/step - loss: 0.5415 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 1.1166 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 80/1000\n",
      "2106/2106 [==============================] - 1s 494us/step - loss: 0.6619 - acc: 0.9416 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.5220 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 81/1000\n",
      "2106/2106 [==============================] - 1s 518us/step - loss: 0.5551 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5143 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 82/1000\n",
      "2106/2106 [==============================] - 1s 546us/step - loss: 0.6676 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.7057 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 83/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.5788 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.6961 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 84/1000\n",
      "2106/2106 [==============================] - 1s 517us/step - loss: 0.6399 - acc: 0.9421 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.6760 - val_acc: 0.9745 - val_recall: 0.3191 - val_fbeta_score: 0.3262\n",
      "Epoch 85/1000\n",
      "2106/2106 [==============================] - 1s 522us/step - loss: 0.5685 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5528 - val_acc: 0.9021 - val_recall: 0.4043 - val_fbeta_score: 0.3362\n",
      "Epoch 86/1000\n",
      "2106/2106 [==============================] - 1s 529us/step - loss: 0.5879 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.5241 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 87/1000\n",
      "2106/2106 [==============================] - 1s 555us/step - loss: 0.6330 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.7632 - val_acc: 0.9234 - val_recall: 0.4043 - val_fbeta_score: 0.3461\n",
      "Epoch 88/1000\n",
      "2106/2106 [==============================] - 1s 516us/step - loss: 0.5566 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4408 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 89/1000\n",
      "2106/2106 [==============================] - 1s 594us/step - loss: 0.6335 - acc: 0.9364 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.6192 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 90/1000\n",
      "2106/2106 [==============================] - 1s 674us/step - loss: 0.6031 - acc: 0.9402 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.5115 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 91/1000\n",
      "2106/2106 [==============================] - 1s 527us/step - loss: 0.5178 - acc: 0.9411 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.4865 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 92/1000\n",
      "2106/2106 [==============================] - 1s 513us/step - loss: 0.5105 - acc: 0.9425 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.4671 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 93/1000\n",
      "2106/2106 [==============================] - 1s 621us/step - loss: 0.5357 - acc: 0.9387 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.5157 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 94/1000\n",
      "2106/2106 [==============================] - 1s 618us/step - loss: 0.5725 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4787 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 95/1000\n",
      "2106/2106 [==============================] - 1s 555us/step - loss: 0.6020 - acc: 0.9406 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.6461 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 96/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.5023 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.5423 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 97/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.5845 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5504 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 98/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.6060 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 1.4074 - val_acc: 0.7872 - val_recall: 0.5106 - val_fbeta_score: 0.3135\n",
      "Epoch 99/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.5870 - acc: 0.9416 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.5054 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 100/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.5242 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.5194 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 101/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.6050 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4837 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 102/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.5055 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.6064 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 103/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.4874 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.4662 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 104/1000\n",
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.5629 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.5289 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 105/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.5296 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.4796 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 106/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 492us/step - loss: 0.4706 - acc: 0.9406 - recall: 0.0301 - fbeta_score: 0.0309 - val_loss: 0.5440 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 107/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.5305 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.6345 - val_acc: 0.8681 - val_recall: 0.4468 - val_fbeta_score: 0.3504\n",
      "Epoch 108/1000\n",
      "2106/2106 [==============================] - 1s 588us/step - loss: 0.5072 - acc: 0.9373 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4068 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 109/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.4850 - acc: 0.9406 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.6706 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 110/1000\n",
      "2106/2106 [==============================] - 1s 567us/step - loss: 0.5233 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.7539 - val_acc: 0.9404 - val_recall: 0.4043 - val_fbeta_score: 0.3759\n",
      "Epoch 111/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.6229 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.5175 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 112/1000\n",
      "2106/2106 [==============================] - 1s 489us/step - loss: 0.4957 - acc: 0.9402 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.4792 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 113/1000\n",
      "2106/2106 [==============================] - 1s 542us/step - loss: 0.4840 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4606 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 114/1000\n",
      "2106/2106 [==============================] - 1s 610us/step - loss: 0.4504 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4082 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 115/1000\n",
      "2106/2106 [==============================] - 1s 565us/step - loss: 0.4755 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.6684 - val_acc: 0.8766 - val_recall: 0.4043 - val_fbeta_score: 0.3291\n",
      "Epoch 116/1000\n",
      "2106/2106 [==============================] - 1s 557us/step - loss: 0.4996 - acc: 0.9406 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.4551 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 117/1000\n",
      "2106/2106 [==============================] - 1s 590us/step - loss: 0.4788 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.6426 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 118/1000\n",
      "2106/2106 [==============================] - 1s 572us/step - loss: 0.4808 - acc: 0.9378 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.7260 - val_acc: 0.9234 - val_recall: 0.4043 - val_fbeta_score: 0.3404\n",
      "Epoch 119/1000\n",
      "2106/2106 [==============================] - 1s 602us/step - loss: 0.5989 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4280 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 120/1000\n",
      "2106/2106 [==============================] - 1s 592us/step - loss: 0.4470 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3994 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 121/1000\n",
      "2106/2106 [==============================] - 1s 609us/step - loss: 0.5154 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.6455 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 122/1000\n",
      "2106/2106 [==============================] - 1s 594us/step - loss: 0.4938 - acc: 0.9421 - recall: 0.0617 - fbeta_score: 0.0649 - val_loss: 0.5312 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 123/1000\n",
      "2106/2106 [==============================] - 1s 626us/step - loss: 0.5220 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.5937 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 124/1000\n",
      "2106/2106 [==============================] - 1s 616us/step - loss: 0.5030 - acc: 0.9378 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4844 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 125/1000\n",
      "2106/2106 [==============================] - 1s 586us/step - loss: 0.5145 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4404 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 126/1000\n",
      "2106/2106 [==============================] - 1s 656us/step - loss: 0.4708 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.5026 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 127/1000\n",
      "2106/2106 [==============================] - 1s 672us/step - loss: 0.4363 - acc: 0.9387 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3972 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 128/1000\n",
      "2106/2106 [==============================] - 1s 595us/step - loss: 0.4768 - acc: 0.9411 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.4690 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 129/1000\n",
      "2106/2106 [==============================] - 1s 685us/step - loss: 0.4364 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4614 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 130/1000\n",
      "2106/2106 [==============================] - 1s 611us/step - loss: 0.4305 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4750 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 131/1000\n",
      "2106/2106 [==============================] - 1s 620us/step - loss: 0.4389 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4607 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 132/1000\n",
      "2106/2106 [==============================] - 1s 704us/step - loss: 0.4635 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4314 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 133/1000\n",
      "2106/2106 [==============================] - 1s 551us/step - loss: 0.4682 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.5096 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 134/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.4667 - acc: 0.9378 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3976 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 135/1000\n",
      "2106/2106 [==============================] - 1s 522us/step - loss: 0.4388 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4200 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 136/1000\n",
      "2106/2106 [==============================] - 1s 527us/step - loss: 0.4491 - acc: 0.9368 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.6967 - val_acc: 0.9660 - val_recall: 0.3191 - val_fbeta_score: 0.3262\n",
      "Epoch 137/1000\n",
      "2106/2106 [==============================] - 1s 516us/step - loss: 0.4703 - acc: 0.9378 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3607 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 138/1000\n",
      "2106/2106 [==============================] - 1s 595us/step - loss: 0.4848 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4713 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 139/1000\n",
      "2106/2106 [==============================] - 2s 728us/step - loss: 0.4331 - acc: 0.9411 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3668 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 140/1000\n",
      "2106/2106 [==============================] - 1s 648us/step - loss: 0.4475 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.4453 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 141/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 2s 774us/step - loss: 0.4965 - acc: 0.9364 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4629 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 142/1000\n",
      "2106/2106 [==============================] - 1s 691us/step - loss: 0.4720 - acc: 0.9387 - recall: 0.0332 - fbeta_score: 0.0348 - val_loss: 0.4383 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 143/1000\n",
      "2106/2106 [==============================] - 1s 658us/step - loss: 0.4261 - acc: 0.9430 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.4218 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 144/1000\n",
      "2106/2106 [==============================] - 2s 750us/step - loss: 0.4495 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3928 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 145/1000\n",
      "2106/2106 [==============================] - 1s 709us/step - loss: 0.4386 - acc: 0.9383 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4946 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 146/1000\n",
      "2106/2106 [==============================] - 2s 734us/step - loss: 0.4628 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4300 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 147/1000\n",
      "2106/2106 [==============================] - 1s 547us/step - loss: 0.4253 - acc: 0.9421 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4424 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 148/1000\n",
      "2106/2106 [==============================] - 1s 549us/step - loss: 0.4195 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.4555 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 149/1000\n",
      "2106/2106 [==============================] - 1s 588us/step - loss: 0.4411 - acc: 0.9416 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.4084 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 150/1000\n",
      "2106/2106 [==============================] - 1s 612us/step - loss: 0.4481 - acc: 0.9383 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4045 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 151/1000\n",
      "2106/2106 [==============================] - 1s 707us/step - loss: 0.4550 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0142 - val_loss: 0.4010 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 152/1000\n",
      "2106/2106 [==============================] - 1s 663us/step - loss: 0.4223 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3796 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 153/1000\n",
      "2106/2106 [==============================] - 1s 616us/step - loss: 0.4235 - acc: 0.9416 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.4251 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 154/1000\n",
      "2106/2106 [==============================] - 1s 624us/step - loss: 0.4466 - acc: 0.9378 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3761 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 155/1000\n",
      "2106/2106 [==============================] - 1s 638us/step - loss: 0.4679 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4629 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 156/1000\n",
      "2106/2106 [==============================] - 1s 512us/step - loss: 0.3957 - acc: 0.9411 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.4706 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 157/1000\n",
      "2106/2106 [==============================] - 1s 536us/step - loss: 0.4271 - acc: 0.9425 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.4552 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 158/1000\n",
      "2106/2106 [==============================] - 1s 582us/step - loss: 0.4083 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4915 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 159/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.4150 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3932 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 160/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.4086 - acc: 0.9402 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3685 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 161/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.3915 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3663 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 162/1000\n",
      "2106/2106 [==============================] - 1s 497us/step - loss: 0.3936 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3532 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 163/1000\n",
      "2106/2106 [==============================] - 1s 496us/step - loss: 0.4188 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4428 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 164/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.4195 - acc: 0.9416 - recall: 0.0214 - fbeta_score: 0.0237 - val_loss: 0.4216 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 165/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.4047 - acc: 0.9392 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4224 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 166/1000\n",
      "2106/2106 [==============================] - 1s 525us/step - loss: 0.4291 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4445 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 167/1000\n",
      "2106/2106 [==============================] - 1s 498us/step - loss: 0.4412 - acc: 0.9383 - recall: 0.0206 - fbeta_score: 0.0214 - val_loss: 0.5571 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 168/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.4463 - acc: 0.9378 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4138 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 169/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.4160 - acc: 0.9392 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3584 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 170/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.4025 - acc: 0.9402 - recall: 0.0206 - fbeta_score: 0.0214 - val_loss: 0.4907 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 171/1000\n",
      "2106/2106 [==============================] - 1s 501us/step - loss: 0.4268 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3738 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 172/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.3898 - acc: 0.9416 - recall: 0.0111 - fbeta_score: 0.0119 - val_loss: 0.3810 - val_acc: 0.9702 - val_recall: 0.2766 - val_fbeta_score: 0.2837\n",
      "Epoch 173/1000\n",
      "2106/2106 [==============================] - 1s 497us/step - loss: 0.4352 - acc: 0.9406 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.4784 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 174/1000\n",
      "2106/2106 [==============================] - 1s 498us/step - loss: 0.4092 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3907 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 175/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.4155 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3527 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 176/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 499us/step - loss: 0.3979 - acc: 0.9387 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3547 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 177/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.4038 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3901 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 178/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.4104 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3707 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 179/1000\n",
      "2106/2106 [==============================] - 1s 511us/step - loss: 0.4148 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4553 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 180/1000\n",
      "2106/2106 [==============================] - 1s 515us/step - loss: 0.3840 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3915 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 181/1000\n",
      "2106/2106 [==============================] - 1s 541us/step - loss: 0.4133 - acc: 0.9425 - recall: 0.0380 - fbeta_score: 0.0380 - val_loss: 0.4133 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 182/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.3992 - acc: 0.9383 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3712 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 183/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3796 - acc: 0.9397 - recall: 0.0277 - fbeta_score: 0.0293 - val_loss: 0.3416 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 184/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.4102 - acc: 0.9430 - recall: 0.0380 - fbeta_score: 0.0380 - val_loss: 0.4915 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 185/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.4098 - acc: 0.9387 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4630 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 186/1000\n",
      "2106/2106 [==============================] - 1s 501us/step - loss: 0.4091 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4337 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 187/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.4252 - acc: 0.9402 - recall: 0.0087 - fbeta_score: 0.0103 - val_loss: 0.4181 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 188/1000\n",
      "2106/2106 [==============================] - 1s 498us/step - loss: 0.3832 - acc: 0.9411 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3537 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 189/1000\n",
      "2106/2106 [==============================] - 1s 538us/step - loss: 0.3930 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3442 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 190/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3883 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3394 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 191/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.3803 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3453 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 192/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.4032 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3807 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 193/1000\n",
      "2106/2106 [==============================] - 1s 496us/step - loss: 0.3968 - acc: 0.9440 - recall: 0.0348 - fbeta_score: 0.0372 - val_loss: 0.3611 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 194/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3894 - acc: 0.9425 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4385 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 195/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.4146 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4434 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 196/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.3928 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3935 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 197/1000\n",
      "2106/2106 [==============================] - 1s 531us/step - loss: 0.3903 - acc: 0.9416 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.4242 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 198/1000\n",
      "2106/2106 [==============================] - 1s 542us/step - loss: 0.4049 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3496 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 199/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.3636 - acc: 0.9387 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3731 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 200/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3917 - acc: 0.9402 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3969 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 201/1000\n",
      "2106/2106 [==============================] - 1s 511us/step - loss: 0.3728 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3685 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 202/1000\n",
      "2106/2106 [==============================] - 1s 509us/step - loss: 0.3955 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3563 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 203/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3659 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3499 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 204/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3971 - acc: 0.9402 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3758 - val_acc: 0.9532 - val_recall: 0.0851 - val_fbeta_score: 0.0851\n",
      "Epoch 205/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3870 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3375 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 206/1000\n",
      "2106/2106 [==============================] - 1s 509us/step - loss: 0.3664 - acc: 0.9406 - recall: 0.0404 - fbeta_score: 0.0412 - val_loss: 0.4002 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 207/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3893 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3738 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 208/1000\n",
      "2106/2106 [==============================] - 1s 510us/step - loss: 0.4150 - acc: 0.9383 - recall: 0.0135 - fbeta_score: 0.0150 - val_loss: 0.4220 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 209/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3879 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4158 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 210/1000\n",
      "2106/2106 [==============================] - 1s 516us/step - loss: 0.3810 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3903 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 211/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.4078 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3970 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 212/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.3819 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3801 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 213/1000\n",
      "2106/2106 [==============================] - 1s 515us/step - loss: 0.3714 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3864 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 214/1000\n",
      "2106/2106 [==============================] - 1s 511us/step - loss: 0.3747 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3612 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 215/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.3871 - acc: 0.9378 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3684 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 216/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.4011 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3687 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 217/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.3890 - acc: 0.9425 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3740 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 218/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3603 - acc: 0.9421 - recall: 0.0214 - fbeta_score: 0.0237 - val_loss: 0.4159 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 219/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3859 - acc: 0.9373 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3594 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 220/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3813 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3973 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 221/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3649 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3310 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 222/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3721 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3384 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 223/1000\n",
      "2106/2106 [==============================] - 1s 531us/step - loss: 0.3834 - acc: 0.9378 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3617 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 224/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.3634 - acc: 0.9411 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3392 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 225/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3858 - acc: 0.9425 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3663 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 226/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3627 - acc: 0.9383 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.4167 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 227/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3746 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3572 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 228/1000\n",
      "2106/2106 [==============================] - 1s 511us/step - loss: 0.3610 - acc: 0.9421 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.4173 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 229/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3860 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3424 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 230/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.3792 - acc: 0.9378 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3782 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 231/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3708 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3853 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 232/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.3776 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3449 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 233/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3846 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3691 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 234/1000\n",
      "2106/2106 [==============================] - 1s 509us/step - loss: 0.3796 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3903 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 235/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3489 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3363 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 236/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3593 - acc: 0.9411 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3723 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 237/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3730 - acc: 0.9421 - recall: 0.0427 - fbeta_score: 0.0443 - val_loss: 0.3371 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 238/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3585 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3532 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 239/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3697 - acc: 0.9416 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3934 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 240/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3990 - acc: 0.9383 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3590 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 241/1000\n",
      "2106/2106 [==============================] - 1s 515us/step - loss: 0.3896 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3373 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 242/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3639 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3843 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 243/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3666 - acc: 0.9378 - recall: 0.0135 - fbeta_score: 0.0150 - val_loss: 0.3670 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 244/1000\n",
      "2106/2106 [==============================] - 1s 515us/step - loss: 0.4004 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3481 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 245/1000\n",
      "2106/2106 [==============================] - 1s 523us/step - loss: 0.3621 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3310 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/1000\n",
      "2106/2106 [==============================] - 1s 543us/step - loss: 0.3573 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3398 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 247/1000\n",
      "2106/2106 [==============================] - 1s 541us/step - loss: 0.3631 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3504 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 248/1000\n",
      "2106/2106 [==============================] - 1s 537us/step - loss: 0.3716 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3578 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 249/1000\n",
      "2106/2106 [==============================] - 1s 549us/step - loss: 0.3532 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4059 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 250/1000\n",
      "2106/2106 [==============================] - 1s 545us/step - loss: 0.3754 - acc: 0.9435 - recall: 0.0475 - fbeta_score: 0.0475 - val_loss: 0.3586 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 251/1000\n",
      "2106/2106 [==============================] - 1s 536us/step - loss: 0.3699 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3658 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 252/1000\n",
      "2106/2106 [==============================] - 1s 541us/step - loss: 0.3763 - acc: 0.9383 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.5384 - val_acc: 0.8638 - val_recall: 0.3830 - val_fbeta_score: 0.2652\n",
      "Epoch 253/1000\n",
      "2106/2106 [==============================] - 1s 554us/step - loss: 0.3862 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3268 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 254/1000\n",
      "2106/2106 [==============================] - 1s 566us/step - loss: 0.3659 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3439 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 255/1000\n",
      "2106/2106 [==============================] - 1s 544us/step - loss: 0.3715 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3776 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 256/1000\n",
      "2106/2106 [==============================] - 1s 547us/step - loss: 0.3759 - acc: 0.9416 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3537 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 257/1000\n",
      "2106/2106 [==============================] - 1s 602us/step - loss: 0.3738 - acc: 0.9421 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3429 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 258/1000\n",
      "2106/2106 [==============================] - 1s 602us/step - loss: 0.3652 - acc: 0.9387 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3457 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 259/1000\n",
      "2106/2106 [==============================] - 1s 628us/step - loss: 0.3658 - acc: 0.9383 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3492 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 260/1000\n",
      "2106/2106 [==============================] - 1s 644us/step - loss: 0.3691 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3795 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 261/1000\n",
      "2106/2106 [==============================] - 1s 640us/step - loss: 0.3701 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3376 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 262/1000\n",
      "2106/2106 [==============================] - 1s 635us/step - loss: 0.3791 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3774 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 263/1000\n",
      "2106/2106 [==============================] - 1s 613us/step - loss: 0.3659 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3333 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 264/1000\n",
      "2106/2106 [==============================] - 2s 717us/step - loss: 0.3477 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4023 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 265/1000\n",
      "2106/2106 [==============================] - 1s 590us/step - loss: 0.3878 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3113 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 266/1000\n",
      "2106/2106 [==============================] - 2s 752us/step - loss: 0.3531 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3565 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 267/1000\n",
      "2106/2106 [==============================] - 1s 609us/step - loss: 0.3705 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3734 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 268/1000\n",
      "2106/2106 [==============================] - 1s 691us/step - loss: 0.3687 - acc: 0.9421 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3291 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 269/1000\n",
      "2106/2106 [==============================] - 1s 623us/step - loss: 0.3634 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3512 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 270/1000\n",
      "2106/2106 [==============================] - 1s 602us/step - loss: 0.3500 - acc: 0.9425 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3361 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 271/1000\n",
      "2106/2106 [==============================] - 1s 600us/step - loss: 0.3589 - acc: 0.9397 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3316 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 272/1000\n",
      "2106/2106 [==============================] - 1s 649us/step - loss: 0.3613 - acc: 0.9378 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3464 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 273/1000\n",
      "2106/2106 [==============================] - 1s 633us/step - loss: 0.3816 - acc: 0.9421 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3347 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 274/1000\n",
      "2106/2106 [==============================] - 1s 602us/step - loss: 0.3681 - acc: 0.9421 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3279 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 275/1000\n",
      "2106/2106 [==============================] - 1s 600us/step - loss: 0.3649 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3262 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 276/1000\n",
      "2106/2106 [==============================] - 1s 590us/step - loss: 0.3676 - acc: 0.9392 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3539 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 277/1000\n",
      "2106/2106 [==============================] - 1s 637us/step - loss: 0.3647 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3366 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 278/1000\n",
      "2106/2106 [==============================] - 1s 662us/step - loss: 0.3642 - acc: 0.9378 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3486 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 279/1000\n",
      "2106/2106 [==============================] - 1s 645us/step - loss: 0.3488 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3853 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 280/1000\n",
      "2106/2106 [==============================] - 1s 625us/step - loss: 0.3795 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4174 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/1000\n",
      "2106/2106 [==============================] - 1s 558us/step - loss: 0.3515 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3652 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 282/1000\n",
      "2106/2106 [==============================] - 1s 632us/step - loss: 0.3780 - acc: 0.9421 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4034 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 283/1000\n",
      "2106/2106 [==============================] - 1s 594us/step - loss: 0.3701 - acc: 0.9383 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3711 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 284/1000\n",
      "2106/2106 [==============================] - 1s 570us/step - loss: 0.3704 - acc: 0.9387 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3353 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 285/1000\n",
      "2106/2106 [==============================] - 1s 561us/step - loss: 0.3539 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3077 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 286/1000\n",
      "2106/2106 [==============================] - 1s 563us/step - loss: 0.3631 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3378 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 287/1000\n",
      "2106/2106 [==============================] - 1s 587us/step - loss: 0.3624 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3812 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 288/1000\n",
      "2106/2106 [==============================] - 1s 615us/step - loss: 0.3737 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3604 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 289/1000\n",
      "2106/2106 [==============================] - 1s 569us/step - loss: 0.3731 - acc: 0.9406 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3410 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 290/1000\n",
      "2106/2106 [==============================] - 1s 570us/step - loss: 0.3533 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3765 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 291/1000\n",
      "2106/2106 [==============================] - 1s 569us/step - loss: 0.3624 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3255 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 292/1000\n",
      "2106/2106 [==============================] - 1s 579us/step - loss: 0.3503 - acc: 0.9411 - recall: 0.0063 - fbeta_score: 0.0071 - val_loss: 0.3794 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 293/1000\n",
      "2106/2106 [==============================] - 1s 568us/step - loss: 0.3490 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4156 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 294/1000\n",
      "2106/2106 [==============================] - 1s 569us/step - loss: 0.3753 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3256 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 295/1000\n",
      "2106/2106 [==============================] - 1s 570us/step - loss: 0.3566 - acc: 0.9440 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3752 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 296/1000\n",
      "2106/2106 [==============================] - 1s 562us/step - loss: 0.3554 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3209 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 297/1000\n",
      "2106/2106 [==============================] - 1s 587us/step - loss: 0.3530 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3727 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 298/1000\n",
      "2106/2106 [==============================] - 2s 743us/step - loss: 0.3559 - acc: 0.9383 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3328 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 299/1000\n",
      "2106/2106 [==============================] - 1s 605us/step - loss: 0.3507 - acc: 0.9383 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3043 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 300/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3427 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3347 - val_acc: 0.9745 - val_recall: 0.2766 - val_fbeta_score: 0.2837\n",
      "Epoch 301/1000\n",
      "2106/2106 [==============================] - 1s 687us/step - loss: 0.3560 - acc: 0.9425 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3598 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 302/1000\n",
      "2106/2106 [==============================] - 2s 769us/step - loss: 0.3687 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3379 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 303/1000\n",
      "2106/2106 [==============================] - 2s 801us/step - loss: 0.3535 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3351 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 304/1000\n",
      "2106/2106 [==============================] - 2s 743us/step - loss: 0.3544 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3279 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 305/1000\n",
      "2106/2106 [==============================] - 2s 818us/step - loss: 0.3700 - acc: 0.9416 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3264 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 306/1000\n",
      "2106/2106 [==============================] - 2s 745us/step - loss: 0.3528 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3349 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 307/1000\n",
      "2106/2106 [==============================] - 2s 726us/step - loss: 0.3553 - acc: 0.9397 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3234 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 308/1000\n",
      "2106/2106 [==============================] - 2s 716us/step - loss: 0.3441 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3373 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 309/1000\n",
      "2106/2106 [==============================] - 2s 761us/step - loss: 0.3546 - acc: 0.9421 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3267 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 310/1000\n",
      "2106/2106 [==============================] - 2s 741us/step - loss: 0.3349 - acc: 0.9402 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3391 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 311/1000\n",
      "2106/2106 [==============================] - 2s 727us/step - loss: 0.3659 - acc: 0.9402 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3765 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 312/1000\n",
      "2106/2106 [==============================] - 2s 832us/step - loss: 0.3596 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3431 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 313/1000\n",
      "2106/2106 [==============================] - 1s 675us/step - loss: 0.3614 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3790 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 314/1000\n",
      "2106/2106 [==============================] - 2s 740us/step - loss: 0.3677 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3419 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 315/1000\n",
      "2106/2106 [==============================] - 1s 660us/step - loss: 0.3591 - acc: 0.9364 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3888 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 316/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 624us/step - loss: 0.3543 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3616 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 317/1000\n",
      "2106/2106 [==============================] - 1s 630us/step - loss: 0.3713 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3306 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 318/1000\n",
      "2106/2106 [==============================] - 2s 826us/step - loss: 0.3630 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3321 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 319/1000\n",
      "2106/2106 [==============================] - 2s 763us/step - loss: 0.3613 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3484 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 320/1000\n",
      "2106/2106 [==============================] - 2s 803us/step - loss: 0.3641 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3483 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 321/1000\n",
      "2106/2106 [==============================] - 1s 701us/step - loss: 0.3495 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3363 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 322/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3665 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3346 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 323/1000\n",
      "2106/2106 [==============================] - 1s 533us/step - loss: 0.3682 - acc: 0.9421 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3634 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 324/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3554 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3294 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 325/1000\n",
      "2106/2106 [==============================] - 1s 501us/step - loss: 0.3349 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0174 - val_loss: 0.3269 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 326/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3523 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3176 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 327/1000\n",
      "2106/2106 [==============================] - 1s 552us/step - loss: 0.3603 - acc: 0.9421 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3604 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 328/1000\n",
      "2106/2106 [==============================] - 1s 573us/step - loss: 0.3515 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3707 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 329/1000\n",
      "2106/2106 [==============================] - 1s 512us/step - loss: 0.3519 - acc: 0.9368 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3284 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 330/1000\n",
      "2106/2106 [==============================] - 1s 565us/step - loss: 0.3404 - acc: 0.9397 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3281 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 331/1000\n",
      "2106/2106 [==============================] - 1s 610us/step - loss: 0.3362 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3480 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 332/1000\n",
      "2106/2106 [==============================] - 1s 669us/step - loss: 0.3682 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.4560 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 333/1000\n",
      "2106/2106 [==============================] - 1s 517us/step - loss: 0.3696 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4032 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 334/1000\n",
      "2106/2106 [==============================] - 1s 555us/step - loss: 0.3768 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3163 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 335/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.3431 - acc: 0.9425 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3622 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 336/1000\n",
      "2106/2106 [==============================] - 1s 495us/step - loss: 0.3769 - acc: 0.9383 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3087 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 337/1000\n",
      "2106/2106 [==============================] - 1s 548us/step - loss: 0.3528 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3670 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 338/1000\n",
      "2106/2106 [==============================] - 1s 494us/step - loss: 0.3748 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3420 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 339/1000\n",
      "2106/2106 [==============================] - 2s 781us/step - loss: 0.3567 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3703 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 340/1000\n",
      "2106/2106 [==============================] - 2s 749us/step - loss: 0.3756 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3426 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 341/1000\n",
      "2106/2106 [==============================] - 1s 551us/step - loss: 0.3677 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3382 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 342/1000\n",
      "2106/2106 [==============================] - 1s 536us/step - loss: 0.3442 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3322 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 343/1000\n",
      "2106/2106 [==============================] - 2s 736us/step - loss: 0.3571 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3235 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 344/1000\n",
      "2106/2106 [==============================] - 1s 692us/step - loss: 0.3427 - acc: 0.9397 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3772 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 345/1000\n",
      "2106/2106 [==============================] - 1s 542us/step - loss: 0.3599 - acc: 0.9416 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3243 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 346/1000\n",
      "2106/2106 [==============================] - 1s 557us/step - loss: 0.3522 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3245 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 347/1000\n",
      "2106/2106 [==============================] - 1s 522us/step - loss: 0.3624 - acc: 0.9373 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3492 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 348/1000\n",
      "2106/2106 [==============================] - 1s 619us/step - loss: 0.3558 - acc: 0.9430 - recall: 0.0332 - fbeta_score: 0.0348 - val_loss: 0.3324 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 349/1000\n",
      "2106/2106 [==============================] - 1s 565us/step - loss: 0.3483 - acc: 0.9397 - recall: 0.0016 - fbeta_score: 0.0024 - val_loss: 0.4413 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 350/1000\n",
      "2106/2106 [==============================] - 1s 575us/step - loss: 0.3474 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3528 - val_acc: 0.9574 - val_recall: 0.3191 - val_fbeta_score: 0.3262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351/1000\n",
      "2106/2106 [==============================] - 1s 597us/step - loss: 0.3758 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4111 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 352/1000\n",
      "2106/2106 [==============================] - 1s 623us/step - loss: 0.3721 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3570 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 353/1000\n",
      "2106/2106 [==============================] - 1s 651us/step - loss: 0.3498 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3733 - val_acc: 0.9489 - val_recall: 0.0638 - val_fbeta_score: 0.0709\n",
      "Epoch 354/1000\n",
      "2106/2106 [==============================] - 1s 578us/step - loss: 0.3555 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3208 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 355/1000\n",
      "2106/2106 [==============================] - 1s 647us/step - loss: 0.3468 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3910 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 356/1000\n",
      "2106/2106 [==============================] - 1s 494us/step - loss: 0.3727 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3478 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 357/1000\n",
      "2106/2106 [==============================] - 1s 492us/step - loss: 0.3449 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3709 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 358/1000\n",
      "2106/2106 [==============================] - 1s 594us/step - loss: 0.3718 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3277 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 359/1000\n",
      "2106/2106 [==============================] - 1s 495us/step - loss: 0.3430 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3291 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 360/1000\n",
      "2106/2106 [==============================] - 1s 494us/step - loss: 0.3530 - acc: 0.9387 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3403 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 361/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3607 - acc: 0.9406 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4529 - val_acc: 0.8766 - val_recall: 0.4468 - val_fbeta_score: 0.3404\n",
      "Epoch 362/1000\n",
      "2106/2106 [==============================] - 1s 496us/step - loss: 0.3431 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3306 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 363/1000\n",
      "2106/2106 [==============================] - 1s 498us/step - loss: 0.3484 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3542 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 364/1000\n",
      "2106/2106 [==============================] - 1s 497us/step - loss: 0.3605 - acc: 0.9392 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3563 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 365/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.3459 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3480 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 366/1000\n",
      "2106/2106 [==============================] - 1s 489us/step - loss: 0.3458 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3851 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 367/1000\n",
      "2106/2106 [==============================] - 1s 487us/step - loss: 0.3608 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3684 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 368/1000\n",
      "2106/2106 [==============================] - 1s 497us/step - loss: 0.3498 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3298 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 369/1000\n",
      "2106/2106 [==============================] - 1s 487us/step - loss: 0.3478 - acc: 0.9430 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3112 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 370/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3497 - acc: 0.9392 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3724 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 371/1000\n",
      "2106/2106 [==============================] - 1s 497us/step - loss: 0.3494 - acc: 0.9383 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3241 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 372/1000\n",
      "2106/2106 [==============================] - 1s 494us/step - loss: 0.3632 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4124 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 373/1000\n",
      "2106/2106 [==============================] - 1s 494us/step - loss: 0.3614 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3478 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 374/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3367 - acc: 0.9402 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3603 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 375/1000\n",
      "2106/2106 [==============================] - 1s 489us/step - loss: 0.3499 - acc: 0.9402 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3771 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 376/1000\n",
      "2106/2106 [==============================] - 1s 487us/step - loss: 0.3666 - acc: 0.9416 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3608 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 377/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.3464 - acc: 0.9416 - recall: 0.0071 - fbeta_score: 0.0095 - val_loss: 0.3476 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 378/1000\n",
      "2106/2106 [==============================] - 1s 494us/step - loss: 0.3639 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3488 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 379/1000\n",
      "2106/2106 [==============================] - 1s 487us/step - loss: 0.3485 - acc: 0.9392 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3767 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 380/1000\n",
      "2106/2106 [==============================] - 1s 495us/step - loss: 0.3589 - acc: 0.9430 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3181 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 381/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.3794 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3340 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 382/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3367 - acc: 0.9416 - recall: 0.0332 - fbeta_score: 0.0348 - val_loss: 0.3425 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 383/1000\n",
      "2106/2106 [==============================] - 1s 494us/step - loss: 0.3619 - acc: 0.9392 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3397 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 384/1000\n",
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.3515 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3497 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 385/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3490 - acc: 0.9425 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3791 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386/1000\n",
      "2106/2106 [==============================] - 1s 476us/step - loss: 0.3445 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3228 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 387/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.3448 - acc: 0.9421 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3206 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 388/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3370 - acc: 0.9421 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.3344 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 389/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.3687 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3654 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 390/1000\n",
      "2106/2106 [==============================] - 1s 516us/step - loss: 0.3451 - acc: 0.9402 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3874 - val_acc: 0.9319 - val_recall: 0.4043 - val_fbeta_score: 0.3887\n",
      "Epoch 391/1000\n",
      "2106/2106 [==============================] - 1s 498us/step - loss: 0.3612 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3610 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 392/1000\n",
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.3435 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3749 - val_acc: 0.9660 - val_recall: 0.1915 - val_fbeta_score: 0.1986\n",
      "Epoch 393/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3577 - acc: 0.9416 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3512 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 394/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3680 - acc: 0.9425 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3570 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 395/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3393 - acc: 0.9411 - recall: 0.0135 - fbeta_score: 0.0150 - val_loss: 0.3034 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 396/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3454 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3613 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 397/1000\n",
      "2106/2106 [==============================] - 1s 489us/step - loss: 0.3629 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.4393 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 398/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.3522 - acc: 0.9421 - recall: 0.0166 - fbeta_score: 0.0190 - val_loss: 0.3434 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 399/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3443 - acc: 0.9397 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3744 - val_acc: 0.9489 - val_recall: 0.0851 - val_fbeta_score: 0.0851\n",
      "Epoch 400/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3619 - acc: 0.9392 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3472 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 401/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3514 - acc: 0.9411 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3781 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 402/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3484 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3175 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 403/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.3565 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4235 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 404/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3434 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3605 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 405/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3429 - acc: 0.9425 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3380 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 406/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3471 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3612 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 407/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3610 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3522 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 408/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.3483 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.3412 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 409/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3680 - acc: 0.9421 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3705 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 410/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3436 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3313 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 411/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3635 - acc: 0.9402 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3701 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 412/1000\n",
      "2106/2106 [==============================] - 1s 477us/step - loss: 0.3610 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4216 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 413/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.3670 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3721 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 414/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.3510 - acc: 0.9430 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.4085 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 415/1000\n",
      "2106/2106 [==============================] - 1s 470us/step - loss: 0.3689 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3559 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 416/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.3409 - acc: 0.9392 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3461 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 417/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3552 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3048 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 418/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3556 - acc: 0.9402 - recall: 0.0206 - fbeta_score: 0.0214 - val_loss: 0.3159 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 419/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3446 - acc: 0.9421 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.3186 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 420/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.3704 - acc: 0.9416 - recall: 0.0154 - fbeta_score: 0.0161 - val_loss: 0.3334 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 421/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 518us/step - loss: 0.3394 - acc: 0.9402 - recall: 0.0087 - fbeta_score: 0.0103 - val_loss: 0.3472 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 422/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3443 - acc: 0.9383 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3705 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 423/1000\n",
      "2106/2106 [==============================] - 1s 487us/step - loss: 0.3540 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3506 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 424/1000\n",
      "2106/2106 [==============================] - 1s 491us/step - loss: 0.3574 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3516 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 425/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.3451 - acc: 0.9368 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3142 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 426/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.3389 - acc: 0.9402 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3009 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 427/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3402 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3447 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 428/1000\n",
      "2106/2106 [==============================] - 1s 477us/step - loss: 0.3631 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3142 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 429/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.3515 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3896 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 430/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3555 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3313 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 431/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3360 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3006 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 432/1000\n",
      "2106/2106 [==============================] - 1s 477us/step - loss: 0.3396 - acc: 0.9378 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4157 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 433/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3713 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3726 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 434/1000\n",
      "2106/2106 [==============================] - 1s 489us/step - loss: 0.3453 - acc: 0.9392 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3108 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 435/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3420 - acc: 0.9416 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3143 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 436/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3255 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3175 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 437/1000\n",
      "2106/2106 [==============================] - 1s 477us/step - loss: 0.3619 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3121 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 438/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3454 - acc: 0.9402 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3495 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 439/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.3398 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3291 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 440/1000\n",
      "2106/2106 [==============================] - 1s 497us/step - loss: 0.3562 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3231 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 441/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3382 - acc: 0.9383 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3330 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 442/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3483 - acc: 0.9383 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3235 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 443/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3455 - acc: 0.9402 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3175 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 444/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.3359 - acc: 0.9397 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3309 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 445/1000\n",
      "2106/2106 [==============================] - 1s 510us/step - loss: 0.3469 - acc: 0.9421 - recall: 0.0427 - fbeta_score: 0.0427 - val_loss: 0.3301 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 446/1000\n",
      "2106/2106 [==============================] - 1s 501us/step - loss: 0.3449 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3348 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 447/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3451 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3213 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 448/1000\n",
      "2106/2106 [==============================] - 1s 523us/step - loss: 0.3476 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3670 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 449/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3491 - acc: 0.9406 - recall: 0.0135 - fbeta_score: 0.0150 - val_loss: 0.3910 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 450/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3417 - acc: 0.9416 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3268 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 451/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3318 - acc: 0.9411 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3612 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 452/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3418 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3305 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 453/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3481 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3321 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 454/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.3669 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.2993 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 455/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3502 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.4254 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.3530 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3247 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 457/1000\n",
      "2106/2106 [==============================] - 1s 487us/step - loss: 0.3379 - acc: 0.9406 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3441 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 458/1000\n",
      "2106/2106 [==============================] - 1s 476us/step - loss: 0.3395 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3645 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 459/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3513 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3434 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 460/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3386 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3505 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 461/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3625 - acc: 0.9392 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3602 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 462/1000\n",
      "2106/2106 [==============================] - 1s 497us/step - loss: 0.3554 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3289 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 463/1000\n",
      "2106/2106 [==============================] - 1s 539us/step - loss: 0.3524 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3338 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 464/1000\n",
      "2106/2106 [==============================] - 1s 577us/step - loss: 0.3457 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3687 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 465/1000\n",
      "2106/2106 [==============================] - 1s 575us/step - loss: 0.3485 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3215 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 466/1000\n",
      "2106/2106 [==============================] - 1s 573us/step - loss: 0.3313 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3496 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 467/1000\n",
      "2106/2106 [==============================] - 1s 575us/step - loss: 0.3397 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3074 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 468/1000\n",
      "2106/2106 [==============================] - 1s 565us/step - loss: 0.3400 - acc: 0.9416 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3298 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 469/1000\n",
      "2106/2106 [==============================] - 1s 588us/step - loss: 0.3499 - acc: 0.9416 - recall: 0.0111 - fbeta_score: 0.0119 - val_loss: 0.3308 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 470/1000\n",
      "2106/2106 [==============================] - 1s 572us/step - loss: 0.3397 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3360 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 471/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.3384 - acc: 0.9383 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3696 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 472/1000\n",
      "2106/2106 [==============================] - 1s 582us/step - loss: 0.3899 - acc: 0.9416 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3234 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 473/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3429 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3231 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 474/1000\n",
      "2106/2106 [==============================] - 1s 496us/step - loss: 0.3381 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3528 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 475/1000\n",
      "2106/2106 [==============================] - 1s 539us/step - loss: 0.3495 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3566 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 476/1000\n",
      "2106/2106 [==============================] - 1s 650us/step - loss: 0.3491 - acc: 0.9383 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3276 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 477/1000\n",
      "2106/2106 [==============================] - 1s 625us/step - loss: 0.3278 - acc: 0.9416 - recall: 0.0214 - fbeta_score: 0.0237 - val_loss: 0.3136 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 478/1000\n",
      "2106/2106 [==============================] - 1s 593us/step - loss: 0.3425 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3384 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 479/1000\n",
      "2106/2106 [==============================] - 1s 673us/step - loss: 0.3437 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3514 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 480/1000\n",
      "2106/2106 [==============================] - 1s 675us/step - loss: 0.3468 - acc: 0.9406 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.3682 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 481/1000\n",
      "2106/2106 [==============================] - 1s 701us/step - loss: 0.3566 - acc: 0.9406 - recall: 0.0040 - fbeta_score: 0.0055 - val_loss: 0.3665 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 482/1000\n",
      "2106/2106 [==============================] - 1s 612us/step - loss: 0.3571 - acc: 0.9421 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3563 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 483/1000\n",
      "2106/2106 [==============================] - 1s 620us/step - loss: 0.3340 - acc: 0.9402 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3349 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 484/1000\n",
      "2106/2106 [==============================] - 1s 649us/step - loss: 0.3528 - acc: 0.9378 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3236 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 485/1000\n",
      "2106/2106 [==============================] - 1s 612us/step - loss: 0.3344 - acc: 0.9416 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3004 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 486/1000\n",
      "2106/2106 [==============================] - 1s 680us/step - loss: 0.3455 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3169 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 487/1000\n",
      "2106/2106 [==============================] - 1s 594us/step - loss: 0.3487 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3311 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 488/1000\n",
      "2106/2106 [==============================] - 1s 617us/step - loss: 0.3452 - acc: 0.9416 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3207 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 489/1000\n",
      "2106/2106 [==============================] - 1s 584us/step - loss: 0.3566 - acc: 0.9411 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.5577 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 490/1000\n",
      "2106/2106 [==============================] - 1s 647us/step - loss: 0.3769 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3589 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491/1000\n",
      "2106/2106 [==============================] - 1s 594us/step - loss: 0.3576 - acc: 0.9406 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.3156 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 492/1000\n",
      "2106/2106 [==============================] - 1s 651us/step - loss: 0.3446 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3450 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 493/1000\n",
      "2106/2106 [==============================] - 1s 647us/step - loss: 0.3305 - acc: 0.9373 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3304 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 494/1000\n",
      "2106/2106 [==============================] - 1s 617us/step - loss: 0.3773 - acc: 0.9416 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4135 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 495/1000\n",
      "2106/2106 [==============================] - 1s 640us/step - loss: 0.3541 - acc: 0.9402 - recall: 0.0063 - fbeta_score: 0.0071 - val_loss: 0.3224 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 496/1000\n",
      "2106/2106 [==============================] - 1s 618us/step - loss: 0.3498 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3864 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 497/1000\n",
      "2106/2106 [==============================] - 1s 654us/step - loss: 0.3467 - acc: 0.9416 - recall: 0.0206 - fbeta_score: 0.0214 - val_loss: 0.3351 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 498/1000\n",
      "2106/2106 [==============================] - 1s 609us/step - loss: 0.3473 - acc: 0.9421 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3466 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 499/1000\n",
      "2106/2106 [==============================] - 1s 495us/step - loss: 0.3447 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3048 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 500/1000\n",
      "2106/2106 [==============================] - 1s 491us/step - loss: 0.3364 - acc: 0.9392 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3259 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 501/1000\n",
      "2106/2106 [==============================] - 1s 492us/step - loss: 0.3430 - acc: 0.9421 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3212 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 502/1000\n",
      "2106/2106 [==============================] - 1s 702us/step - loss: 0.3545 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3243 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 503/1000\n",
      "2106/2106 [==============================] - 2s 757us/step - loss: 0.3509 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3096 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 504/1000\n",
      "2106/2106 [==============================] - 1s 664us/step - loss: 0.3366 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3272 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 505/1000\n",
      "2106/2106 [==============================] - 1s 707us/step - loss: 0.3503 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3354 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 506/1000\n",
      "2106/2106 [==============================] - 2s 727us/step - loss: 0.3420 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3409 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 507/1000\n",
      "2106/2106 [==============================] - 1s 695us/step - loss: 0.3369 - acc: 0.9406 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.2865 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 508/1000\n",
      "2106/2106 [==============================] - 1s 604us/step - loss: 0.3302 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3476 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 509/1000\n",
      "2106/2106 [==============================] - 1s 594us/step - loss: 0.3435 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3218 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 510/1000\n",
      "2106/2106 [==============================] - 1s 581us/step - loss: 0.3402 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4094 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 511/1000\n",
      "2106/2106 [==============================] - 1s 530us/step - loss: 0.3451 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3081 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 512/1000\n",
      "2106/2106 [==============================] - 1s 621us/step - loss: 0.3277 - acc: 0.9378 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3292 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 513/1000\n",
      "2106/2106 [==============================] - 1s 618us/step - loss: 0.3441 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.3120 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 514/1000\n",
      "2106/2106 [==============================] - 1s 675us/step - loss: 0.3474 - acc: 0.9411 - recall: 0.0111 - fbeta_score: 0.0119 - val_loss: 0.3328 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 515/1000\n",
      "2106/2106 [==============================] - 1s 557us/step - loss: 0.3373 - acc: 0.9425 - recall: 0.0332 - fbeta_score: 0.0348 - val_loss: 0.3282 - val_acc: 0.9447 - val_recall: 0.0213 - val_fbeta_score: 0.0284\n",
      "Epoch 516/1000\n",
      "2106/2106 [==============================] - 1s 549us/step - loss: 0.3562 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3653 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 517/1000\n",
      "2106/2106 [==============================] - 1s 568us/step - loss: 0.3568 - acc: 0.9392 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3639 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 518/1000\n",
      "2106/2106 [==============================] - 1s 530us/step - loss: 0.3295 - acc: 0.9397 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4626 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 519/1000\n",
      "2106/2106 [==============================] - 1s 519us/step - loss: 0.3733 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3311 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 520/1000\n",
      "2106/2106 [==============================] - 1s 527us/step - loss: 0.3454 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3101 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 521/1000\n",
      "2106/2106 [==============================] - 1s 513us/step - loss: 0.3400 - acc: 0.9430 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3213 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 522/1000\n",
      "2106/2106 [==============================] - 1s 517us/step - loss: 0.3303 - acc: 0.9406 - recall: 0.0348 - fbeta_score: 0.0372 - val_loss: 0.3034 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 523/1000\n",
      "2106/2106 [==============================] - 1s 538us/step - loss: 0.3318 - acc: 0.9397 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3056 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 524/1000\n",
      "2106/2106 [==============================] - 1s 528us/step - loss: 0.3360 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3487 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 525/1000\n",
      "2106/2106 [==============================] - 1s 520us/step - loss: 0.3354 - acc: 0.9421 - recall: 0.0206 - fbeta_score: 0.0214 - val_loss: 0.4007 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 526/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 524us/step - loss: 0.3520 - acc: 0.9435 - recall: 0.0332 - fbeta_score: 0.0348 - val_loss: 0.3498 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 527/1000\n",
      "2106/2106 [==============================] - 1s 541us/step - loss: 0.3446 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3218 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 528/1000\n",
      "2106/2106 [==============================] - 1s 529us/step - loss: 0.3650 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.4196 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 529/1000\n",
      "2106/2106 [==============================] - 1s 525us/step - loss: 0.3609 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0063 - val_loss: 0.3598 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 530/1000\n",
      "2106/2106 [==============================] - 1s 518us/step - loss: 0.3386 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3409 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 531/1000\n",
      "2106/2106 [==============================] - 1s 597us/step - loss: 0.3505 - acc: 0.9421 - recall: 0.0404 - fbeta_score: 0.0412 - val_loss: 0.3135 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 532/1000\n",
      "2106/2106 [==============================] - 2s 784us/step - loss: 0.3458 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3281 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 533/1000\n",
      "2106/2106 [==============================] - 1s 625us/step - loss: 0.3425 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3834 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 534/1000\n",
      "2106/2106 [==============================] - 1s 693us/step - loss: 0.3423 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3884 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 535/1000\n",
      "2106/2106 [==============================] - 1s 587us/step - loss: 0.3494 - acc: 0.9406 - recall: 0.0135 - fbeta_score: 0.0150 - val_loss: 0.3228 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 536/1000\n",
      "2106/2106 [==============================] - 1s 586us/step - loss: 0.3424 - acc: 0.9411 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3039 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 537/1000\n",
      "2106/2106 [==============================] - 1s 612us/step - loss: 0.3389 - acc: 0.9425 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3338 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 538/1000\n",
      "2106/2106 [==============================] - 1s 597us/step - loss: 0.3348 - acc: 0.9383 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3275 - val_acc: 0.9447 - val_recall: 0.0213 - val_fbeta_score: 0.0284\n",
      "Epoch 539/1000\n",
      "2106/2106 [==============================] - 1s 648us/step - loss: 0.3459 - acc: 0.9402 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3231 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 540/1000\n",
      "2106/2106 [==============================] - 1s 628us/step - loss: 0.3373 - acc: 0.9387 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3137 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 541/1000\n",
      "2106/2106 [==============================] - 2s 714us/step - loss: 0.3488 - acc: 0.9425 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3083 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 542/1000\n",
      "2106/2106 [==============================] - 1s 697us/step - loss: 0.3554 - acc: 0.9392 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.4331 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 543/1000\n",
      "2106/2106 [==============================] - 1s 618us/step - loss: 0.3485 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3431 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 544/1000\n",
      "2106/2106 [==============================] - 1s 545us/step - loss: 0.3529 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3470 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 545/1000\n",
      "2106/2106 [==============================] - 1s 543us/step - loss: 0.3517 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3561 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 546/1000\n",
      "2106/2106 [==============================] - 1s 533us/step - loss: 0.3461 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3395 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 547/1000\n",
      "2106/2106 [==============================] - 1s 537us/step - loss: 0.3395 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3199 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 548/1000\n",
      "2106/2106 [==============================] - 1s 525us/step - loss: 0.3282 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3260 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 549/1000\n",
      "2106/2106 [==============================] - 1s 529us/step - loss: 0.3619 - acc: 0.9402 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3322 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 550/1000\n",
      "2106/2106 [==============================] - 1s 559us/step - loss: 0.3332 - acc: 0.9402 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3155 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 551/1000\n",
      "2106/2106 [==============================] - 1s 541us/step - loss: 0.3300 - acc: 0.9397 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3261 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 552/1000\n",
      "2106/2106 [==============================] - 1s 531us/step - loss: 0.3343 - acc: 0.9364 - recall: 0.0166 - fbeta_score: 0.0190 - val_loss: 0.3163 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 553/1000\n",
      "2106/2106 [==============================] - 1s 525us/step - loss: 0.3413 - acc: 0.9397 - recall: 0.0063 - fbeta_score: 0.0071 - val_loss: 0.3004 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 554/1000\n",
      "2106/2106 [==============================] - 1s 529us/step - loss: 0.3283 - acc: 0.9416 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3460 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 555/1000\n",
      "2106/2106 [==============================] - 1s 564us/step - loss: 0.3427 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.4012 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 556/1000\n",
      "2106/2106 [==============================] - 1s 540us/step - loss: 0.3361 - acc: 0.9425 - recall: 0.0427 - fbeta_score: 0.0427 - val_loss: 0.3578 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 557/1000\n",
      "2106/2106 [==============================] - 1s 528us/step - loss: 0.3570 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3264 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 558/1000\n",
      "2106/2106 [==============================] - 1s 525us/step - loss: 0.3333 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3213 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 559/1000\n",
      "2106/2106 [==============================] - 1s 583us/step - loss: 0.3426 - acc: 0.9416 - recall: 0.0380 - fbeta_score: 0.0380 - val_loss: 0.3862 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 560/1000\n",
      "2106/2106 [==============================] - 1s 608us/step - loss: 0.3486 - acc: 0.9402 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3137 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 561/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 630us/step - loss: 0.3257 - acc: 0.9430 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.3427 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 562/1000\n",
      "2106/2106 [==============================] - 1s 630us/step - loss: 0.3494 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3594 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 563/1000\n",
      "2106/2106 [==============================] - 1s 623us/step - loss: 0.3612 - acc: 0.9406 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3756 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 564/1000\n",
      "2106/2106 [==============================] - 1s 621us/step - loss: 0.3518 - acc: 0.9392 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3371 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 565/1000\n",
      "2106/2106 [==============================] - 1s 607us/step - loss: 0.3292 - acc: 0.9416 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3127 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 566/1000\n",
      "2106/2106 [==============================] - 2s 724us/step - loss: 0.3338 - acc: 0.9425 - recall: 0.0522 - fbeta_score: 0.0522 - val_loss: 0.3696 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 567/1000\n",
      "2106/2106 [==============================] - 1s 557us/step - loss: 0.3476 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3584 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 568/1000\n",
      "2106/2106 [==============================] - 1s 535us/step - loss: 0.3426 - acc: 0.9397 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3406 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 569/1000\n",
      "2106/2106 [==============================] - 1s 640us/step - loss: 0.3824 - acc: 0.9378 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4435 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 570/1000\n",
      "2106/2106 [==============================] - 1s 538us/step - loss: 0.3501 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3230 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 571/1000\n",
      "2106/2106 [==============================] - 1s 545us/step - loss: 0.3463 - acc: 0.9373 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3554 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 572/1000\n",
      "2106/2106 [==============================] - 1s 530us/step - loss: 0.3566 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3424 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 573/1000\n",
      "2106/2106 [==============================] - 1s 532us/step - loss: 0.3456 - acc: 0.9416 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3458 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 574/1000\n",
      "2106/2106 [==============================] - 1s 534us/step - loss: 0.3572 - acc: 0.9373 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.3442 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 575/1000\n",
      "2106/2106 [==============================] - 1s 539us/step - loss: 0.3574 - acc: 0.9421 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3335 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 576/1000\n",
      "2106/2106 [==============================] - 1s 534us/step - loss: 0.3393 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3214 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 577/1000\n",
      "2106/2106 [==============================] - 2s 755us/step - loss: 0.3464 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4058 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 578/1000\n",
      "2106/2106 [==============================] - 1s 622us/step - loss: 0.3547 - acc: 0.9421 - recall: 0.0174 - fbeta_score: 0.0190 - val_loss: 0.3496 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 579/1000\n",
      "2106/2106 [==============================] - 1s 517us/step - loss: 0.3656 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3128 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 580/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3651 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3391 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 581/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3408 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3639 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 582/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3325 - acc: 0.9383 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3670 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 583/1000\n",
      "2106/2106 [==============================] - 1s 551us/step - loss: 0.3458 - acc: 0.9378 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.3098 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 584/1000\n",
      "2106/2106 [==============================] - 1s 551us/step - loss: 0.3610 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3452 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 585/1000\n",
      "2106/2106 [==============================] - 1s 546us/step - loss: 0.3565 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3302 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 586/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3554 - acc: 0.9378 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3943 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 587/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3700 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3720 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 588/1000\n",
      "2106/2106 [==============================] - ETA: 0s - loss: 0.3705 - acc: 0.9423 - recall: 0.0124 - fbeta_score: 0.0133 - ETA: 0s - loss: 0.3761 - acc: 0.9432 - recall: 0.0114 - fbe - 1s 550us/step - loss: 0.3722 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3238 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 589/1000\n",
      "2106/2106 [==============================] - 1s 513us/step - loss: 0.3478 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3356 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 590/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3526 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3664 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 591/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3454 - acc: 0.9392 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3302 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 592/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3482 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3233 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 593/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3488 - acc: 0.9411 - recall: 0.0214 - fbeta_score: 0.0253 - val_loss: 0.3507 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 594/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3548 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3558 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 595/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3557 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3646 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 596/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3778 - acc: 0.9397 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.3634 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 597/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3546 - acc: 0.9435 - recall: 0.0427 - fbeta_score: 0.0427 - val_loss: 0.3635 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 598/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3478 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3157 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 599/1000\n",
      "2106/2106 [==============================] - 1s 528us/step - loss: 0.3482 - acc: 0.9406 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.4101 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 600/1000\n",
      "2106/2106 [==============================] - 1s 531us/step - loss: 0.3535 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3066 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 601/1000\n",
      "2106/2106 [==============================] - 1s 575us/step - loss: 0.3490 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3388 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 602/1000\n",
      "2106/2106 [==============================] - 1s 544us/step - loss: 0.3661 - acc: 0.9421 - recall: 0.0372 - fbeta_score: 0.0404 - val_loss: 0.3282 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 603/1000\n",
      "2106/2106 [==============================] - 1s 562us/step - loss: 0.3509 - acc: 0.9368 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3307 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 604/1000\n",
      "2106/2106 [==============================] - 1s 499us/step - loss: 0.3532 - acc: 0.9387 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3249 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 605/1000\n",
      "2106/2106 [==============================] - 1s 494us/step - loss: 0.3642 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3561 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 606/1000\n",
      "2106/2106 [==============================] - 1s 594us/step - loss: 0.3576 - acc: 0.9406 - recall: 0.0040 - fbeta_score: 0.0055 - val_loss: 0.3524 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 607/1000\n",
      "2106/2106 [==============================] - 1s 634us/step - loss: 0.3594 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3789 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 608/1000\n",
      "2106/2106 [==============================] - 1s 614us/step - loss: 0.3623 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3372 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 609/1000\n",
      "2106/2106 [==============================] - 1s 606us/step - loss: 0.3528 - acc: 0.9411 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.3481 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 610/1000\n",
      "2106/2106 [==============================] - 1s 602us/step - loss: 0.3294 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3271 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 611/1000\n",
      "2106/2106 [==============================] - 1s 607us/step - loss: 0.3488 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4368 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 612/1000\n",
      "2106/2106 [==============================] - 1s 704us/step - loss: 0.3776 - acc: 0.9387 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3683 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 613/1000\n",
      "2106/2106 [==============================] - 1s 621us/step - loss: 0.3425 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3492 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 614/1000\n",
      "2106/2106 [==============================] - 1s 538us/step - loss: 0.3591 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3949 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 615/1000\n",
      "2106/2106 [==============================] - 1s 646us/step - loss: 0.3570 - acc: 0.9387 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3365 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 616/1000\n",
      "2106/2106 [==============================] - 1s 633us/step - loss: 0.3474 - acc: 0.9387 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3090 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 617/1000\n",
      "2106/2106 [==============================] - 1s 655us/step - loss: 0.3343 - acc: 0.9425 - recall: 0.0380 - fbeta_score: 0.0380 - val_loss: 0.3337 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 618/1000\n",
      "2106/2106 [==============================] - 1s 664us/step - loss: 0.3625 - acc: 0.9402 - recall: 0.0040 - fbeta_score: 0.0055 - val_loss: 0.3132 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 619/1000\n",
      "2106/2106 [==============================] - 1s 592us/step - loss: 0.3464 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.3746 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 620/1000\n",
      "2106/2106 [==============================] - 1s 596us/step - loss: 0.3512 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3679 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 621/1000\n",
      "2106/2106 [==============================] - 1s 617us/step - loss: 0.3452 - acc: 0.9397 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3457 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 622/1000\n",
      "2106/2106 [==============================] - 1s 589us/step - loss: 0.3566 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.4235 - val_acc: 0.8936 - val_recall: 0.3191 - val_fbeta_score: 0.2766\n",
      "Epoch 623/1000\n",
      "2106/2106 [==============================] - 1s 598us/step - loss: 0.3538 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4078 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 624/1000\n",
      "2106/2106 [==============================] - 1s 626us/step - loss: 0.3541 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3483 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 625/1000\n",
      "2106/2106 [==============================] - 1s 577us/step - loss: 0.3477 - acc: 0.9421 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3481 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 626/1000\n",
      "2106/2106 [==============================] - 1s 626us/step - loss: 0.3381 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3888 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 627/1000\n",
      "2106/2106 [==============================] - 1s 604us/step - loss: 0.3580 - acc: 0.9406 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3722 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 628/1000\n",
      "2106/2106 [==============================] - 1s 651us/step - loss: 0.3489 - acc: 0.9378 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3096 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 629/1000\n",
      "2106/2106 [==============================] - 1s 579us/step - loss: 0.3507 - acc: 0.9425 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3400 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 630/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 545us/step - loss: 0.3581 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3518 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 631/1000\n",
      "2106/2106 [==============================] - 1s 547us/step - loss: 0.3408 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3269 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 632/1000\n",
      "2106/2106 [==============================] - 1s 548us/step - loss: 0.3394 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3526 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 633/1000\n",
      "2106/2106 [==============================] - 1s 541us/step - loss: 0.3605 - acc: 0.9397 - recall: 0.0111 - fbeta_score: 0.0119 - val_loss: 0.3659 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 634/1000\n",
      "2106/2106 [==============================] - 1s 548us/step - loss: 0.3631 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3331 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 635/1000\n",
      "2106/2106 [==============================] - 1s 546us/step - loss: 0.3509 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3412 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 636/1000\n",
      "2106/2106 [==============================] - 1s 547us/step - loss: 0.3467 - acc: 0.9411 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3075 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 637/1000\n",
      "2106/2106 [==============================] - 1s 545us/step - loss: 0.3384 - acc: 0.9397 - recall: 0.0131 - fbeta_score: 0.0146 - val_loss: 0.3435 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 638/1000\n",
      "2106/2106 [==============================] - 1s 563us/step - loss: 0.3543 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3690 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 639/1000\n",
      "2106/2106 [==============================] - 1s 554us/step - loss: 0.3476 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3296 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 640/1000\n",
      "2106/2106 [==============================] - 1s 558us/step - loss: 0.3513 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3161 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 641/1000\n",
      "2106/2106 [==============================] - 1s 549us/step - loss: 0.3475 - acc: 0.9383 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3187 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 642/1000\n",
      "2106/2106 [==============================] - 1s 577us/step - loss: 0.3492 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3607 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 643/1000\n",
      "2106/2106 [==============================] - 1s 621us/step - loss: 0.3445 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3631 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 644/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.3387 - acc: 0.9392 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3197 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 645/1000\n",
      "2106/2106 [==============================] - 1s 476us/step - loss: 0.3483 - acc: 0.9397 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.3862 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 646/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3435 - acc: 0.9368 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3506 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 647/1000\n",
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.3543 - acc: 0.9402 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.3355 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 648/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3575 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3572 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 649/1000\n",
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.3596 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.4155 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 650/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3564 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3361 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 651/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.3350 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3598 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 652/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3575 - acc: 0.9402 - recall: 0.0111 - fbeta_score: 0.0150 - val_loss: 0.3194 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 653/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3420 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3380 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 654/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.3599 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3542 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 655/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3539 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3611 - val_acc: 0.9532 - val_recall: 0.1277 - val_fbeta_score: 0.1277\n",
      "Epoch 656/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3539 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3505 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 657/1000\n",
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.3569 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3911 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 658/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3429 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3433 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 659/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3372 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3716 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 660/1000\n",
      "2106/2106 [==============================] - 1s 495us/step - loss: 0.3595 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3239 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 661/1000\n",
      "2106/2106 [==============================] - 1s 495us/step - loss: 0.3511 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3422 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 662/1000\n",
      "2106/2106 [==============================] - 1s 523us/step - loss: 0.3486 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3175 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 663/1000\n",
      "2106/2106 [==============================] - 1s 578us/step - loss: 0.3401 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3464 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 664/1000\n",
      "2106/2106 [==============================] - 1s 584us/step - loss: 0.3690 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3400 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 665/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 579us/step - loss: 0.3488 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3289 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 666/1000\n",
      "2106/2106 [==============================] - 1s 579us/step - loss: 0.3434 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.3477 - val_acc: 0.9532 - val_recall: 0.1915 - val_fbeta_score: 0.1844\n",
      "Epoch 667/1000\n",
      "2106/2106 [==============================] - 1s 581us/step - loss: 0.3499 - acc: 0.9430 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3150 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 668/1000\n",
      "2106/2106 [==============================] - 1s 574us/step - loss: 0.3327 - acc: 0.9411 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3763 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 669/1000\n",
      "2106/2106 [==============================] - 1s 630us/step - loss: 0.3537 - acc: 0.9383 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3290 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 670/1000\n",
      "2106/2106 [==============================] - 1s 522us/step - loss: 0.3554 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3013 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 671/1000\n",
      "2106/2106 [==============================] - 1s 585us/step - loss: 0.3364 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3562 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 672/1000\n",
      "2106/2106 [==============================] - 1s 619us/step - loss: 0.3467 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3379 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 673/1000\n",
      "2106/2106 [==============================] - 1s 555us/step - loss: 0.3788 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4206 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 674/1000\n",
      "2106/2106 [==============================] - 1s 544us/step - loss: 0.3726 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3587 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 675/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3463 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3447 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 676/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3537 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3334 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 677/1000\n",
      "2106/2106 [==============================] - 1s 510us/step - loss: 0.3598 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3502 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 678/1000\n",
      "2106/2106 [==============================] - 1s 516us/step - loss: 0.3468 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3252 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 679/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3450 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.3402 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 680/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3382 - acc: 0.9421 - recall: 0.0427 - fbeta_score: 0.0427 - val_loss: 0.3093 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 681/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3481 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3388 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 682/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3404 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3375 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 683/1000\n",
      "2106/2106 [==============================] - 1s 526us/step - loss: 0.3492 - acc: 0.9411 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3697 - val_acc: 0.9191 - val_recall: 0.3617 - val_fbeta_score: 0.3433\n",
      "Epoch 684/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3550 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3713 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 685/1000\n",
      "2106/2106 [==============================] - 1s 509us/step - loss: 0.3493 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3811 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 686/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3737 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3198 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 687/1000\n",
      "2106/2106 [==============================] - 1s 511us/step - loss: 0.3502 - acc: 0.9383 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3118 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 688/1000\n",
      "2106/2106 [==============================] - 1s 554us/step - loss: 0.3524 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3344 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 689/1000\n",
      "2106/2106 [==============================] - 1s 517us/step - loss: 0.3338 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3129 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 690/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3408 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3566 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 691/1000\n",
      "2106/2106 [==============================] - 1s 499us/step - loss: 0.3444 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3263 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 692/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3466 - acc: 0.9373 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3234 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 693/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.3407 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3456 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 694/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3583 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3717 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 695/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3633 - acc: 0.9392 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3194 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 696/1000\n",
      "2106/2106 [==============================] - 1s 512us/step - loss: 0.3368 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3699 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 697/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3761 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3276 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 698/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.3538 - acc: 0.9421 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3044 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 699/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3311 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3032 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 700/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3460 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3379 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 701/1000\n",
      "2106/2106 [==============================] - 1s 514us/step - loss: 0.3439 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3350 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 702/1000\n",
      "2106/2106 [==============================] - 1s 509us/step - loss: 0.3398 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3443 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 703/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.3373 - acc: 0.9406 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3167 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 704/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3622 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3412 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 705/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3367 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3065 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 706/1000\n",
      "2106/2106 [==============================] - 1s 548us/step - loss: 0.3457 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3117 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 707/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3649 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3485 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 708/1000\n",
      "2106/2106 [==============================] - 1s 523us/step - loss: 0.3624 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3757 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 709/1000\n",
      "2106/2106 [==============================] - 1s 525us/step - loss: 0.3518 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3446 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 710/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3630 - acc: 0.9411 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3436 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 711/1000\n",
      "2106/2106 [==============================] - 1s 542us/step - loss: 0.3565 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3162 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 712/1000\n",
      "2106/2106 [==============================] - 1s 510us/step - loss: 0.3602 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3374 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 713/1000\n",
      "2106/2106 [==============================] - 1s 529us/step - loss: 0.3417 - acc: 0.9411 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3596 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 714/1000\n",
      "2106/2106 [==============================] - 1s 512us/step - loss: 0.3592 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3400 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 715/1000\n",
      "2106/2106 [==============================] - 1s 524us/step - loss: 0.3631 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3263 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 716/1000\n",
      "2106/2106 [==============================] - 1s 525us/step - loss: 0.3595 - acc: 0.9397 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3592 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 717/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3385 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3808 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 718/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3455 - acc: 0.9425 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3394 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 719/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.3685 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3394 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 720/1000\n",
      "2106/2106 [==============================] - 1s 518us/step - loss: 0.3645 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3562 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 721/1000\n",
      "2106/2106 [==============================] - 1s 509us/step - loss: 0.3544 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3109 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 722/1000\n",
      "2106/2106 [==============================] - 1s 542us/step - loss: 0.3545 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3402 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 723/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.3563 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3551 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 724/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3515 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3905 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 725/1000\n",
      "2106/2106 [==============================] - 1s 521us/step - loss: 0.3703 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3501 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 726/1000\n",
      "2106/2106 [==============================] - 1s 524us/step - loss: 0.3401 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3233 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 727/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3500 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3979 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 728/1000\n",
      "2106/2106 [==============================] - 1s 521us/step - loss: 0.3506 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3435 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 729/1000\n",
      "2106/2106 [==============================] - 1s 528us/step - loss: 0.3381 - acc: 0.9387 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3108 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 730/1000\n",
      "2106/2106 [==============================] - 1s 543us/step - loss: 0.3360 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3227 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 731/1000\n",
      "2106/2106 [==============================] - 1s 512us/step - loss: 0.3385 - acc: 0.9449 - recall: 0.0546 - fbeta_score: 0.0554 - val_loss: 0.3552 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 732/1000\n",
      "2106/2106 [==============================] - 1s 525us/step - loss: 0.3477 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.2997 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 733/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3360 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3562 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 734/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3496 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3203 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 735/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.3545 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3434 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 736/1000\n",
      "2106/2106 [==============================] - 1s 506us/step - loss: 0.3504 - acc: 0.9368 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3416 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 737/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.3409 - acc: 0.9425 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3353 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 738/1000\n",
      "2106/2106 [==============================] - 1s 539us/step - loss: 0.3441 - acc: 0.9416 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3291 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 739/1000\n",
      "2106/2106 [==============================] - 1s 547us/step - loss: 0.3414 - acc: 0.9378 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3016 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 740/1000\n",
      "2106/2106 [==============================] - 1s 509us/step - loss: 0.3539 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3912 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 741/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.3661 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3420 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 742/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.3401 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3790 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 743/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.3379 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3660 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 744/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3599 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3318 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 745/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3574 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3739 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 746/1000\n",
      "2106/2106 [==============================] - 1s 556us/step - loss: 0.3376 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3113 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 747/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3511 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3132 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 748/1000\n",
      "2106/2106 [==============================] - 1s 514us/step - loss: 0.3424 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3825 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 749/1000\n",
      "2106/2106 [==============================] - 1s 514us/step - loss: 0.3556 - acc: 0.9406 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.4063 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 750/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3379 - acc: 0.9411 - recall: 0.0427 - fbeta_score: 0.0427 - val_loss: 0.3174 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 751/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3424 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3877 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 752/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3497 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3853 - val_acc: 0.9191 - val_recall: 0.4468 - val_fbeta_score: 0.3645\n",
      "Epoch 753/1000\n",
      "2106/2106 [==============================] - 1s 507us/step - loss: 0.3608 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3223 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 754/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.3383 - acc: 0.9430 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3262 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 755/1000\n",
      "2106/2106 [==============================] - 1s 499us/step - loss: 0.3483 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3674 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 756/1000\n",
      "2106/2106 [==============================] - 1s 502us/step - loss: 0.3605 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3269 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 757/1000\n",
      "2106/2106 [==============================] - 1s 509us/step - loss: 0.3536 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3233 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 758/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.3573 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3120 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 759/1000\n",
      "2106/2106 [==============================] - 1s 498us/step - loss: 0.3509 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3147 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 760/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3460 - acc: 0.9402 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3288 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 761/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.3459 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3442 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 762/1000\n",
      "2106/2106 [==============================] - 1s 499us/step - loss: 0.3574 - acc: 0.9416 - recall: 0.0206 - fbeta_score: 0.0214 - val_loss: 0.3449 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 763/1000\n",
      "2106/2106 [==============================] - 1s 505us/step - loss: 0.3562 - acc: 0.9421 - recall: 0.0087 - fbeta_score: 0.0103 - val_loss: 0.3610 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 764/1000\n",
      "2106/2106 [==============================] - 1s 509us/step - loss: 0.3635 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3538 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 765/1000\n",
      "2106/2106 [==============================] - 1s 503us/step - loss: 0.3513 - acc: 0.9402 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3357 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 766/1000\n",
      "2106/2106 [==============================] - 1s 501us/step - loss: 0.3447 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3297 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 767/1000\n",
      "2106/2106 [==============================] - 1s 501us/step - loss: 0.3390 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3414 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 768/1000\n",
      "2106/2106 [==============================] - 1s 537us/step - loss: 0.3525 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3421 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 769/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.3606 - acc: 0.9392 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3566 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 770/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3713 - acc: 0.9397 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3800 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 771/1000\n",
      "2106/2106 [==============================] - 1s 518us/step - loss: 0.3650 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3569 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 772/1000\n",
      "2106/2106 [==============================] - 1s 564us/step - loss: 0.3696 - acc: 0.9411 - recall: 0.0451 - fbeta_score: 0.0459 - val_loss: 0.3778 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 773/1000\n",
      "2106/2106 [==============================] - 1s 601us/step - loss: 0.3592 - acc: 0.9383 - recall: 0.0182 - fbeta_score: 0.0214 - val_loss: 0.3651 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 774/1000\n",
      "2106/2106 [==============================] - 1s 605us/step - loss: 0.3624 - acc: 0.9425 - recall: 0.0356 - fbeta_score: 0.0364 - val_loss: 0.3410 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 775/1000\n",
      "2106/2106 [==============================] - 1s 617us/step - loss: 0.3666 - acc: 0.9402 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.4327 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 776/1000\n",
      "2106/2106 [==============================] - 1s 626us/step - loss: 0.3877 - acc: 0.9387 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4121 - val_acc: 0.9660 - val_recall: 0.3617 - val_fbeta_score: 0.3546\n",
      "Epoch 777/1000\n",
      "2106/2106 [==============================] - 1s 591us/step - loss: 0.3459 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3351 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 778/1000\n",
      "2106/2106 [==============================] - 2s 742us/step - loss: 0.3521 - acc: 0.9359 - recall: 0.0214 - fbeta_score: 0.0237 - val_loss: 0.3342 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 779/1000\n",
      "2106/2106 [==============================] - 1s 635us/step - loss: 0.3433 - acc: 0.9373 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3175 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 780/1000\n",
      "2106/2106 [==============================] - 1s 554us/step - loss: 0.3443 - acc: 0.9392 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3212 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 781/1000\n",
      "2106/2106 [==============================] - 1s 663us/step - loss: 0.3516 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3126 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 782/1000\n",
      "2106/2106 [==============================] - 1s 612us/step - loss: 0.3558 - acc: 0.9430 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3492 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 783/1000\n",
      "2106/2106 [==============================] - 1s 586us/step - loss: 0.3602 - acc: 0.9373 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3609 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 784/1000\n",
      "2106/2106 [==============================] - 1s 630us/step - loss: 0.3699 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3481 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 785/1000\n",
      "2106/2106 [==============================] - 1s 656us/step - loss: 0.3449 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3369 - val_acc: 0.9574 - val_recall: 0.3617 - val_fbeta_score: 0.3333\n",
      "Epoch 786/1000\n",
      "2106/2106 [==============================] - 1s 611us/step - loss: 0.3441 - acc: 0.9397 - recall: 0.0277 - fbeta_score: 0.0309 - val_loss: 0.3460 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 787/1000\n",
      "2106/2106 [==============================] - 1s 616us/step - loss: 0.3540 - acc: 0.9402 - recall: 0.0317 - fbeta_score: 0.0332 - val_loss: 0.3597 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 788/1000\n",
      "2106/2106 [==============================] - 1s 619us/step - loss: 0.3753 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3279 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 789/1000\n",
      "2106/2106 [==============================] - 1s 592us/step - loss: 0.3349 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3263 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 790/1000\n",
      "2106/2106 [==============================] - 1s 673us/step - loss: 0.3582 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3231 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 791/1000\n",
      "2106/2106 [==============================] - 1s 608us/step - loss: 0.3648 - acc: 0.9421 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3215 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 792/1000\n",
      "2106/2106 [==============================] - 1s 585us/step - loss: 0.3427 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3806 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 793/1000\n",
      "2106/2106 [==============================] - 1s 593us/step - loss: 0.3547 - acc: 0.9392 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3118 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 794/1000\n",
      "2106/2106 [==============================] - 1s 593us/step - loss: 0.3478 - acc: 0.9416 - recall: 0.0135 - fbeta_score: 0.0150 - val_loss: 0.3218 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 795/1000\n",
      "2106/2106 [==============================] - 1s 618us/step - loss: 0.3385 - acc: 0.9421 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3397 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 796/1000\n",
      "2106/2106 [==============================] - 1s 626us/step - loss: 0.3435 - acc: 0.9373 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3703 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 797/1000\n",
      "2106/2106 [==============================] - 1s 647us/step - loss: 0.3580 - acc: 0.9416 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3050 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 798/1000\n",
      "2106/2106 [==============================] - 1s 632us/step - loss: 0.3479 - acc: 0.9392 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3388 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 799/1000\n",
      "2106/2106 [==============================] - 1s 603us/step - loss: 0.3447 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3823 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 800/1000\n",
      "2106/2106 [==============================] - 1s 695us/step - loss: 0.3593 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3459 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 801/1000\n",
      "2106/2106 [==============================] - 1s 555us/step - loss: 0.3491 - acc: 0.9421 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3406 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 802/1000\n",
      "2106/2106 [==============================] - 1s 497us/step - loss: 0.3557 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3612 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 803/1000\n",
      "2106/2106 [==============================] - 1s 519us/step - loss: 0.3666 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3629 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 804/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3503 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3370 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 805/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3524 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3276 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 806/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.3967 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3188 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 807/1000\n",
      "2106/2106 [==============================] - 1s 494us/step - loss: 0.3389 - acc: 0.9430 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3624 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 808/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.3618 - acc: 0.9425 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.2961 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 809/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3338 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3799 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 810/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.3751 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3544 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 811/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3622 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3698 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 812/1000\n",
      "2106/2106 [==============================] - 1s 489us/step - loss: 0.3507 - acc: 0.9402 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3633 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 813/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3670 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3176 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 814/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3263 - acc: 0.9425 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3092 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 815/1000\n",
      "2106/2106 [==============================] - 1s 495us/step - loss: 0.3544 - acc: 0.9364 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3513 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 816/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.3592 - acc: 0.9402 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.4230 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 817/1000\n",
      "2106/2106 [==============================] - 1s 491us/step - loss: 0.3565 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3454 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 818/1000\n",
      "2106/2106 [==============================] - 1s 478us/step - loss: 0.3523 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3161 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 819/1000\n",
      "2106/2106 [==============================] - 1s 474us/step - loss: 0.3455 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3414 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 820/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3448 - acc: 0.9392 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3225 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 821/1000\n",
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.3561 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3582 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 822/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3806 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3323 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 823/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.3416 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3433 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 824/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3465 - acc: 0.9425 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3552 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 825/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3593 - acc: 0.9421 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3571 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 826/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3462 - acc: 0.9392 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3251 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 827/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.3376 - acc: 0.9416 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3218 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 828/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3418 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3797 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 829/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.3785 - acc: 0.9387 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3358 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 830/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3488 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3105 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 831/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3246 - acc: 0.9402 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3715 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 832/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.3580 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3558 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 833/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3555 - acc: 0.9402 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3472 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 834/1000\n",
      "2106/2106 [==============================] - 1s 495us/step - loss: 0.3581 - acc: 0.9397 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3635 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 835/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.3522 - acc: 0.9416 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3541 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 836/1000\n",
      "2106/2106 [==============================] - 1s 496us/step - loss: 0.3453 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3630 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 837/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3511 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0253 - val_loss: 0.3211 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 838/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3528 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.4723 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 839/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3585 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3920 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 840/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.3591 - acc: 0.9383 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3597 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 841/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3434 - acc: 0.9421 - recall: 0.0380 - fbeta_score: 0.0380 - val_loss: 0.3169 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 842/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3615 - acc: 0.9416 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4255 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 843/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3640 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.4324 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 844/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3508 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3326 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 845/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3563 - acc: 0.9406 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3337 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 846/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.3435 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3315 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 847/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3406 - acc: 0.9397 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3210 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 848/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3241 - acc: 0.9402 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3231 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 849/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.3820 - acc: 0.9421 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3310 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 850/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3321 - acc: 0.9416 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3131 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 851/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3373 - acc: 0.9440 - recall: 0.0277 - fbeta_score: 0.0293 - val_loss: 0.3264 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 852/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3555 - acc: 0.9402 - recall: 0.0063 - fbeta_score: 0.0071 - val_loss: 0.3399 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 853/1000\n",
      "2106/2106 [==============================] - 1s 489us/step - loss: 0.3503 - acc: 0.9397 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3717 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 854/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.3646 - acc: 0.9383 - recall: 0.0095 - fbeta_score: 0.0111 - val_loss: 0.3457 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 855/1000\n",
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.3313 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4018 - val_acc: 0.9489 - val_recall: 0.0638 - val_fbeta_score: 0.0709\n",
      "Epoch 856/1000\n",
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.3619 - acc: 0.9383 - recall: 0.0063 - fbeta_score: 0.0071 - val_loss: 0.3361 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 857/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3488 - acc: 0.9421 - recall: 0.0158 - fbeta_score: 0.0166 - val_loss: 0.3687 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 858/1000\n",
      "2106/2106 [==============================] - 1s 492us/step - loss: 0.3386 - acc: 0.9416 - recall: 0.0182 - fbeta_score: 0.0198 - val_loss: 0.4117 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 859/1000\n",
      "2106/2106 [==============================] - 1s 487us/step - loss: 0.3602 - acc: 0.9354 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3220 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 860/1000\n",
      "2106/2106 [==============================] - ETA: 0s - loss: 0.3233 - acc: 0.9411 - recall: 0.0266 - fbeta_score: 0.02 - 1s 486us/step - loss: 0.3236 - acc: 0.9402 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3184 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 861/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3683 - acc: 0.9387 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3436 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 862/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3268 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3366 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 863/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3432 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3295 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 864/1000\n",
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.3461 - acc: 0.9425 - recall: 0.0404 - fbeta_score: 0.0412 - val_loss: 0.3582 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 865/1000\n",
      "2106/2106 [==============================] - 1s 490us/step - loss: 0.3455 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3030 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 866/1000\n",
      "2106/2106 [==============================] - 1s 485us/step - loss: 0.3315 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.2972 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 867/1000\n",
      "2106/2106 [==============================] - 1s 489us/step - loss: 0.3524 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3146 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 868/1000\n",
      "2106/2106 [==============================] - 1s 489us/step - loss: 0.3421 - acc: 0.9406 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3144 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 869/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3339 - acc: 0.9435 - recall: 0.0467 - fbeta_score: 0.0483 - val_loss: 0.3351 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 870/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3597 - acc: 0.9406 - recall: 0.0404 - fbeta_score: 0.0412 - val_loss: 0.4043 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 871/1000\n",
      "2106/2106 [==============================] - 1s 484us/step - loss: 0.3421 - acc: 0.9421 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3369 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 872/1000\n",
      "2106/2106 [==============================] - 1s 482us/step - loss: 0.3362 - acc: 0.9383 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3186 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 873/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3548 - acc: 0.9411 - recall: 0.0332 - fbeta_score: 0.0348 - val_loss: 0.3291 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 874/1000\n",
      "2106/2106 [==============================] - 1s 492us/step - loss: 0.3302 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3307 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 875/1000\n",
      "2106/2106 [==============================] - 1s 486us/step - loss: 0.3420 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3473 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 876/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.3322 - acc: 0.9378 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3182 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 877/1000\n",
      "2106/2106 [==============================] - 1s 479us/step - loss: 0.3493 - acc: 0.9383 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3127 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 878/1000\n",
      "2106/2106 [==============================] - 1s 493us/step - loss: 0.3586 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0063 - val_loss: 0.3349 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 879/1000\n",
      "2106/2106 [==============================] - 1s 488us/step - loss: 0.3493 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3461 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 880/1000\n",
      "2106/2106 [==============================] - 1s 606us/step - loss: 0.3375 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3218 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 881/1000\n",
      "2106/2106 [==============================] - 1s 708us/step - loss: 0.3439 - acc: 0.9402 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3347 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 882/1000\n",
      "2106/2106 [==============================] - 1s 662us/step - loss: 0.3514 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3344 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 883/1000\n",
      "2106/2106 [==============================] - 2s 713us/step - loss: 0.3482 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3340 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 884/1000\n",
      "2106/2106 [==============================] - 1s 691us/step - loss: 0.3342 - acc: 0.9421 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3402 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 885/1000\n",
      "2106/2106 [==============================] - 1s 664us/step - loss: 0.3411 - acc: 0.9402 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3374 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 886/1000\n",
      "2106/2106 [==============================] - 1s 647us/step - loss: 0.3377 - acc: 0.9406 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.4154 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 887/1000\n",
      "2106/2106 [==============================] - 2s 755us/step - loss: 0.3584 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3502 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 888/1000\n",
      "2106/2106 [==============================] - 1s 584us/step - loss: 0.3576 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3148 - val_acc: 0.9617 - val_recall: 0.1702 - val_fbeta_score: 0.1702\n",
      "Epoch 889/1000\n",
      "2106/2106 [==============================] - 1s 586us/step - loss: 0.3559 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3453 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 890/1000\n",
      "2106/2106 [==============================] - 1s 695us/step - loss: 0.3288 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3944 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 891/1000\n",
      "2106/2106 [==============================] - 1s 586us/step - loss: 0.3350 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3031 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 892/1000\n",
      "2106/2106 [==============================] - 1s 586us/step - loss: 0.3450 - acc: 0.9411 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3122 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 893/1000\n",
      "2106/2106 [==============================] - 1s 581us/step - loss: 0.3443 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3861 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 894/1000\n",
      "2106/2106 [==============================] - 1s 598us/step - loss: 0.3394 - acc: 0.9392 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3060 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 895/1000\n",
      "2106/2106 [==============================] - 1s 594us/step - loss: 0.3245 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3295 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 896/1000\n",
      "2106/2106 [==============================] - 1s 577us/step - loss: 0.3381 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3087 - val_acc: 0.9532 - val_recall: 0.0851 - val_fbeta_score: 0.0851\n",
      "Epoch 897/1000\n",
      "2106/2106 [==============================] - 1s 602us/step - loss: 0.3309 - acc: 0.9387 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3072 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 898/1000\n",
      "2106/2106 [==============================] - 1s 586us/step - loss: 0.3309 - acc: 0.9402 - recall: 0.0024 - fbeta_score: 0.0032 - val_loss: 0.3337 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 899/1000\n",
      "2106/2106 [==============================] - 1s 601us/step - loss: 0.3637 - acc: 0.9416 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3328 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 900/1000\n",
      "2106/2106 [==============================] - 1s 634us/step - loss: 0.3486 - acc: 0.9425 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3457 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 901/1000\n",
      "2106/2106 [==============================] - 1s 607us/step - loss: 0.3415 - acc: 0.9402 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3554 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 902/1000\n",
      "2106/2106 [==============================] - 1s 608us/step - loss: 0.3522 - acc: 0.9397 - recall: 0.0285 - fbeta_score: 0.0301 - val_loss: 0.3169 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 903/1000\n",
      "2106/2106 [==============================] - 1s 594us/step - loss: 0.3467 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3384 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 904/1000\n",
      "2106/2106 [==============================] - 1s 595us/step - loss: 0.3450 - acc: 0.9397 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3257 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 905/1000\n",
      "2106/2106 [==============================] - 1s 608us/step - loss: 0.3528 - acc: 0.9416 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3258 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 906/1000\n",
      "2106/2106 [==============================] - 1s 594us/step - loss: 0.3274 - acc: 0.9387 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3226 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 907/1000\n",
      "2106/2106 [==============================] - 1s 601us/step - loss: 0.3378 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3108 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 908/1000\n",
      "2106/2106 [==============================] - 1s 607us/step - loss: 0.3386 - acc: 0.9425 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3541 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 909/1000\n",
      "2106/2106 [==============================] - 1s 606us/step - loss: 0.3614 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3268 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 910/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 550us/step - loss: 0.3487 - acc: 0.9402 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3084 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 911/1000\n",
      "2106/2106 [==============================] - 1s 553us/step - loss: 0.3519 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3336 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 912/1000\n",
      "2106/2106 [==============================] - 1s 538us/step - loss: 0.3607 - acc: 0.9406 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3518 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 913/1000\n",
      "2106/2106 [==============================] - 1s 546us/step - loss: 0.3459 - acc: 0.9373 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3293 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 914/1000\n",
      "2106/2106 [==============================] - 1s 548us/step - loss: 0.3471 - acc: 0.9387 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3343 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 915/1000\n",
      "2106/2106 [==============================] - 1s 559us/step - loss: 0.3407 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3617 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 916/1000\n",
      "2106/2106 [==============================] - 1s 547us/step - loss: 0.3555 - acc: 0.9373 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3560 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 917/1000\n",
      "2106/2106 [==============================] - 1s 562us/step - loss: 0.3361 - acc: 0.9387 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3313 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 918/1000\n",
      "2106/2106 [==============================] - 1s 554us/step - loss: 0.3565 - acc: 0.9392 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3991 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 919/1000\n",
      "2106/2106 [==============================] - 1s 545us/step - loss: 0.3456 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3805 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 920/1000\n",
      "2106/2106 [==============================] - 1s 551us/step - loss: 0.3369 - acc: 0.9392 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3277 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 921/1000\n",
      "2106/2106 [==============================] - 1s 549us/step - loss: 0.3542 - acc: 0.9364 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3490 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 922/1000\n",
      "2106/2106 [==============================] - 1s 553us/step - loss: 0.3519 - acc: 0.9411 - recall: 0.0380 - fbeta_score: 0.0396 - val_loss: 0.3177 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 923/1000\n",
      "2106/2106 [==============================] - 1s 555us/step - loss: 0.3526 - acc: 0.9368 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3380 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 924/1000\n",
      "2106/2106 [==============================] - 1s 570us/step - loss: 0.3780 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3419 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 925/1000\n",
      "2106/2106 [==============================] - 1s 561us/step - loss: 0.3463 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3548 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 926/1000\n",
      "2106/2106 [==============================] - 1s 570us/step - loss: 0.3479 - acc: 0.9397 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3481 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 927/1000\n",
      "2106/2106 [==============================] - 1s 561us/step - loss: 0.3309 - acc: 0.9411 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3041 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 928/1000\n",
      "2106/2106 [==============================] - 1s 606us/step - loss: 0.3469 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3599 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 929/1000\n",
      "2106/2106 [==============================] - 2s 722us/step - loss: 0.3470 - acc: 0.9425 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3517 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 930/1000\n",
      "2106/2106 [==============================] - 1s 626us/step - loss: 0.3464 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3258 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 931/1000\n",
      "2106/2106 [==============================] - 1s 609us/step - loss: 0.3472 - acc: 0.9430 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3378 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 932/1000\n",
      "2106/2106 [==============================] - 1s 594us/step - loss: 0.3364 - acc: 0.9421 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3589 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 933/1000\n",
      "2106/2106 [==============================] - 1s 592us/step - loss: 0.3454 - acc: 0.9454 - recall: 0.0633 - fbeta_score: 0.0657 - val_loss: 0.3375 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 934/1000\n",
      "2106/2106 [==============================] - 1s 652us/step - loss: 0.3519 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3544 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 935/1000\n",
      "2106/2106 [==============================] - 1s 653us/step - loss: 0.3699 - acc: 0.9411 - recall: 0.0206 - fbeta_score: 0.0214 - val_loss: 0.3520 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 936/1000\n",
      "2106/2106 [==============================] - 1s 513us/step - loss: 0.3475 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3307 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 937/1000\n",
      "2106/2106 [==============================] - 1s 508us/step - loss: 0.3358 - acc: 0.9430 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3407 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 938/1000\n",
      "2106/2106 [==============================] - 1s 583us/step - loss: 0.3575 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3242 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 939/1000\n",
      "2106/2106 [==============================] - 1s 525us/step - loss: 0.3596 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3484 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 940/1000\n",
      "2106/2106 [==============================] - 1s 592us/step - loss: 0.3325 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3815 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 941/1000\n",
      "2106/2106 [==============================] - 2s 876us/step - loss: 0.3377 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3004 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 942/1000\n",
      "2106/2106 [==============================] - 1s 703us/step - loss: 0.3549 - acc: 0.9421 - recall: 0.0214 - fbeta_score: 0.0237 - val_loss: 0.3589 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 943/1000\n",
      "2106/2106 [==============================] - 1s 573us/step - loss: 0.3544 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3152 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 944/1000\n",
      "2106/2106 [==============================] - 2s 816us/step - loss: 0.3495 - acc: 0.9416 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3617 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 639us/step - loss: 0.3478 - acc: 0.9411 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3590 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 946/1000\n",
      "2106/2106 [==============================] - 1s 563us/step - loss: 0.3562 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3396 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 947/1000\n",
      "2106/2106 [==============================] - 1s 670us/step - loss: 0.3496 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3471 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 948/1000\n",
      "2106/2106 [==============================] - 1s 531us/step - loss: 0.3464 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3209 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 949/1000\n",
      "2106/2106 [==============================] - 1s 517us/step - loss: 0.3448 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3246 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 950/1000\n",
      "2106/2106 [==============================] - 1s 567us/step - loss: 0.3318 - acc: 0.9430 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3591 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 951/1000\n",
      "2106/2106 [==============================] - 1s 644us/step - loss: 0.3331 - acc: 0.9421 - recall: 0.0404 - fbeta_score: 0.0412 - val_loss: 0.3229 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 952/1000\n",
      "2106/2106 [==============================] - 1s 492us/step - loss: 0.3533 - acc: 0.9397 - recall: 0.0206 - fbeta_score: 0.0214 - val_loss: 0.3373 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 953/1000\n",
      "2106/2106 [==============================] - 1s 564us/step - loss: 0.3490 - acc: 0.9387 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3334 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 954/1000\n",
      "2106/2106 [==============================] - 1s 510us/step - loss: 0.3549 - acc: 0.9392 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3319 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 955/1000\n",
      "2106/2106 [==============================] - 1s 579us/step - loss: 0.3413 - acc: 0.9397 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3407 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 956/1000\n",
      "2106/2106 [==============================] - 2s 836us/step - loss: 0.3698 - acc: 0.9421 - recall: 0.0059 - fbeta_score: 0.0066 - val_loss: 0.3302 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 957/1000\n",
      "2106/2106 [==============================] - 2s 793us/step - loss: 0.3460 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3406 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 958/1000\n",
      "2106/2106 [==============================] - 2s 770us/step - loss: 0.3390 - acc: 0.9383 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3339 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 959/1000\n",
      "2106/2106 [==============================] - 1s 597us/step - loss: 0.3562 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3302 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 960/1000\n",
      "2106/2106 [==============================] - 1s 481us/step - loss: 0.3268 - acc: 0.9421 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.3563 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 961/1000\n",
      "2106/2106 [==============================] - 1s 707us/step - loss: 0.3422 - acc: 0.9411 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3227 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 962/1000\n",
      "2106/2106 [==============================] - 1s 595us/step - loss: 0.3425 - acc: 0.9364 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3997 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 963/1000\n",
      "2106/2106 [==============================] - 1s 574us/step - loss: 0.3525 - acc: 0.9421 - recall: 0.0332 - fbeta_score: 0.0332 - val_loss: 0.3465 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 964/1000\n",
      "2106/2106 [==============================] - 1s 586us/step - loss: 0.3379 - acc: 0.9397 - recall: 0.0237 - fbeta_score: 0.0237 - val_loss: 0.2783 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 965/1000\n",
      "2106/2106 [==============================] - 1s 536us/step - loss: 0.3330 - acc: 0.9416 - recall: 0.0285 - fbeta_score: 0.0285 - val_loss: 0.3263 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 966/1000\n",
      "2106/2106 [==============================] - 1s 533us/step - loss: 0.3461 - acc: 0.9402 - recall: 0.0059 - fbeta_score: 0.0066 - val_loss: 0.3066 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 967/1000\n",
      "2106/2106 [==============================] - 1s 511us/step - loss: 0.3437 - acc: 0.9392 - recall: 0.0071 - fbeta_score: 0.0095 - val_loss: 0.3269 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 968/1000\n",
      "2106/2106 [==============================] - 1s 532us/step - loss: 0.3475 - acc: 0.9406 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3480 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 969/1000\n",
      "2106/2106 [==============================] - 1s 501us/step - loss: 0.3383 - acc: 0.9397 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3118 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 970/1000\n",
      "2106/2106 [==============================] - 1s 511us/step - loss: 0.3296 - acc: 0.9397 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3367 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 971/1000\n",
      "2106/2106 [==============================] - 1s 520us/step - loss: 0.3513 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3091 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 972/1000\n",
      "2106/2106 [==============================] - 1s 518us/step - loss: 0.3440 - acc: 0.9387 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3428 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 973/1000\n",
      "2106/2106 [==============================] - 1s 504us/step - loss: 0.3253 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3671 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 974/1000\n",
      "2106/2106 [==============================] - 1s 514us/step - loss: 0.3551 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3622 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 975/1000\n",
      "2106/2106 [==============================] - 1s 524us/step - loss: 0.3392 - acc: 0.9411 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3219 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 976/1000\n",
      "2106/2106 [==============================] - 1s 552us/step - loss: 0.3467 - acc: 0.9392 - recall: 0.0190 - fbeta_score: 0.0190 - val_loss: 0.3163 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 977/1000\n",
      "2106/2106 [==============================] - 1s 626us/step - loss: 0.3359 - acc: 0.9392 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3315 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 978/1000\n",
      "2106/2106 [==============================] - 1s 593us/step - loss: 0.3392 - acc: 0.9406 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3414 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 979/1000\n",
      "2106/2106 [==============================] - 1s 587us/step - loss: 0.3376 - acc: 0.9406 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3182 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 980/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106/2106 [==============================] - 1s 585us/step - loss: 0.3219 - acc: 0.9402 - recall: 0.0182 - fbeta_score: 0.0198 - val_loss: 0.2945 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 981/1000\n",
      "2106/2106 [==============================] - 1s 597us/step - loss: 0.3427 - acc: 0.9406 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3034 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 982/1000\n",
      "2106/2106 [==============================] - 1s 607us/step - loss: 0.3482 - acc: 0.9387 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3218 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 983/1000\n",
      "2106/2106 [==============================] - 1s 610us/step - loss: 0.3720 - acc: 0.9383 - recall: 0.0071 - fbeta_score: 0.0079 - val_loss: 0.3239 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 984/1000\n",
      "2106/2106 [==============================] - 1s 535us/step - loss: 0.3305 - acc: 0.9402 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3149 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 985/1000\n",
      "2106/2106 [==============================] - 1s 638us/step - loss: 0.3409 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3552 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 986/1000\n",
      "2106/2106 [==============================] - 1s 627us/step - loss: 0.3577 - acc: 0.9421 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3420 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 987/1000\n",
      "2106/2106 [==============================] - 1s 638us/step - loss: 0.3597 - acc: 0.9411 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3301 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 988/1000\n",
      "2106/2106 [==============================] - 1s 678us/step - loss: 0.3420 - acc: 0.9430 - recall: 0.0261 - fbeta_score: 0.0269 - val_loss: 0.3353 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 989/1000\n",
      "2106/2106 [==============================] - 1s 562us/step - loss: 0.3580 - acc: 0.9383 - recall: 0.0000e+00 - fbeta_score: 0.0000e+00 - val_loss: 0.3395 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 990/1000\n",
      "2106/2106 [==============================] - 1s 570us/step - loss: 0.3364 - acc: 0.9406 - recall: 0.0142 - fbeta_score: 0.0158 - val_loss: 0.3174 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 991/1000\n",
      "2106/2106 [==============================] - 1s 532us/step - loss: 0.3371 - acc: 0.9397 - recall: 0.0095 - fbeta_score: 0.0095 - val_loss: 0.3932 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 992/1000\n",
      "2106/2106 [==============================] - 1s 514us/step - loss: 0.3565 - acc: 0.9397 - recall: 0.0190 - fbeta_score: 0.0206 - val_loss: 0.3987 - val_acc: 0.9447 - val_recall: 0.3617 - val_fbeta_score: 0.3262\n",
      "Epoch 993/1000\n",
      "2106/2106 [==============================] - 2s 750us/step - loss: 0.3834 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3579 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 994/1000\n",
      "2106/2106 [==============================] - 1s 679us/step - loss: 0.3300 - acc: 0.9411 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3280 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 995/1000\n",
      "2106/2106 [==============================] - 1s 589us/step - loss: 0.3556 - acc: 0.9402 - recall: 0.0214 - fbeta_score: 0.0222 - val_loss: 0.3104 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 996/1000\n",
      "2106/2106 [==============================] - 1s 571us/step - loss: 0.3297 - acc: 0.9402 - recall: 0.0047 - fbeta_score: 0.0047 - val_loss: 0.3063 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 997/1000\n",
      "2106/2106 [==============================] - 1s 558us/step - loss: 0.3377 - acc: 0.9373 - recall: 0.0119 - fbeta_score: 0.0127 - val_loss: 0.3031 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 998/1000\n",
      "2106/2106 [==============================] - 1s 500us/step - loss: 0.3420 - acc: 0.9425 - recall: 0.0142 - fbeta_score: 0.0142 - val_loss: 0.3391 - val_acc: 0.9489 - val_recall: 0.0638 - val_fbeta_score: 0.0709\n",
      "Epoch 999/1000\n",
      "2106/2106 [==============================] - 1s 483us/step - loss: 0.3491 - acc: 0.9397 - recall: 0.0166 - fbeta_score: 0.0174 - val_loss: 0.3110 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "Epoch 1000/1000\n",
      "2106/2106 [==============================] - 1s 480us/step - loss: 0.3222 - acc: 0.9411 - recall: 0.0309 - fbeta_score: 0.0317 - val_loss: 0.3129 - val_acc: 0.9404 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211595fb6a0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_scale_train, y_train,\n",
    "                epochs=1000,\n",
    "                batch_size=10,\n",
    "                shuffle=True,\n",
    "#                callbacks = callback,\n",
    "                 validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 100)               2700      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 66,201\n",
      "Trainable params: 64,801\n",
      "Non-trainable params: 1,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586/586 [==============================] - 0s 84us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3067373134475519, 0.9539249146757679, 0.0, 0.0]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.evaluate(X_scale_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9338103756708407"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score = mlp.predict(X_scale_test)\n",
    "#y_score =y_score[:,1]>y_score[:,0]\n",
    "y_score[0:100]\n",
    "#y_test[0:100]\n",
    "\n",
    "roc_auc_score(y_test, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "\n",
    "\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bastante similar al modelo de una capa, esto me hace pensar que la opción \"class_weight='balanced'\" funciona bastante bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
